"""
Agent Execution Pipeline

7ë‹¨ê³„: Agentic LLMì˜ ììœ¨ì  í†µê³„ ê²€ì • ë° ë™ì  ì¡°ì •
Agentê°€ í†µê³„ ê²€ì • ì „ ê³¼ì •ì„ ììœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ë©°, í•„ìš”ì‹œ ë™ì ìœ¼ë¡œ ë¶„ì„ ë°©ë²•ì„ ì¡°ì •í•©ë‹ˆë‹¤.
"""

import logging
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
import time
import re

from .base_pipeline_step import BasePipelineStep, PipelineStepRegistry
from core.rag.rag_manager import RAGManager
from services.llm.llm_client import LLMClient
from services.llm.prompt_engine import PromptEngine
from services.code_executor.safe_code_runner import SafeCodeRunner
from services.visualization.plot_generator import PlotGenerator
from services.statistics.stats_executor import StatsExecutor


class AgentExecutionStep(BasePipelineStep):
    """7ë‹¨ê³„: Agentic LLMì˜ í†µê³„ ë¶„ì„ ì‹¤í–‰ ë° ê²°ê³¼ í•´ì„"""
    
    def __init__(self):
        """AgentExecutionStep ì´ˆê¸°í™”"""
        super().__init__("Agentic LLMì˜ í†µê³„ ë¶„ì„ ì‹¤í–‰ ë° ê²°ê³¼ í•´ì„", 7)
        self.rag_manager = RAGManager()
        self.llm_client = LLMClient()
        self.prompt_engine = PromptEngine()
        self.code_runner = SafeCodeRunner()
        self.plot_generator = PlotGenerator()
        self.stats_executor = StatsExecutor()
        
    def validate_input(self, input_data: Dict[str, Any]) -> bool:
        """
        ì…ë ¥ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
        
        Args:
            input_data: 6ë‹¨ê³„ì—ì„œ ì „ë‹¬ë°›ì€ ë°ì´í„°
            
        Returns:
            bool: ìœ íš¨ì„± ê²€ì¦ ê²°ê³¼
        """
        required_fields = [
            'analysis_code', 'execution_plan', 'data_requirements',
            'statistical_design', 'visualization_plan', 'documentation'
        ]
        return all(field in input_data for field in required_fields)
    
    def get_expected_output_schema(self) -> Dict[str, Any]:
        """
        ì˜ˆìƒ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: ì¶œë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ
        """
        return {
            'execution_results': {
                'status': str,
                'statistics': dict,
                'metrics': dict,
                'validation_results': list
            },
            'analysis_insights': {
                'key_findings': list,
                'statistical_significance': dict,
                'limitations': list
            },
            'visualizations': {
                'plots': list,
                'interactive_elements': list,
                'plot_descriptions': dict
            },
            'interpretation': {
                'summary': str,
                'detailed_analysis': dict,
                'recommendations': list
            },
            'quality_metrics': {
                'reliability_scores': dict,
                'validation_metrics': dict,
                'confidence_intervals': dict
            },
            'execution_metadata': {
                'runtime_stats': dict,
                'resource_usage': dict,
                'error_logs': list
            }
        }
    
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Agentic LLMì˜ í†µê³„ ë¶„ì„ ì‹¤í–‰ ë° ê²°ê³¼ í•´ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            input_data: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸
                - analysis_code: ë¶„ì„ ì½”ë“œ
                - execution_plan: ì‹¤í–‰ ê³„íš
                - data_requirements: ë°ì´í„° ìš”êµ¬ì‚¬í•­
                - statistical_design: í†µê³„ì  ì„¤ê³„
                - visualization_plan: ì‹œê°í™” ê³„íš
                - documentation: ë¬¸ì„œí™”
            
        Returns:
            Dict: ì‹¤í–‰ ê²°ê³¼
        """
        self.logger.info("7ë‹¨ê³„: Agentic LLMì˜ í†µê³„ ë¶„ì„ ì‹¤í–‰ ë° ê²°ê³¼ í•´ì„ ì‹œì‘")
        
        try:
            # 1. í†µê³„ ë¶„ì„ ì‹¤í–‰
            execution_results = self._execute_statistical_analysis(input_data)
            
            # 2. RAG ê¸°ë°˜ ê²°ê³¼ í•´ì„
            analysis_insights = self._interpret_results_with_rag(
                execution_results, input_data
            )
            
            # 3. ì‹œê°í™” ìƒì„±
            visualizations = self._generate_visualizations(
                execution_results, input_data['visualization_plan']
            )
            
            # 4. ê²°ê³¼ í•´ì„ ë° ìš”ì•½
            interpretation = self._create_interpretation(
                execution_results, analysis_insights, visualizations
            )
            
            # 5. í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            quality_metrics = self._calculate_quality_metrics(
                execution_results, input_data['statistical_design']
            )
            
            # 6. ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            execution_metadata = self._collect_execution_metadata()
            
            self.logger.info("í†µê³„ ë¶„ì„ ì‹¤í–‰ ë° ê²°ê³¼ í•´ì„ ì™„ë£Œ")
            
            return {
                'execution_results': execution_results,
                'analysis_insights': analysis_insights,
                'visualizations': visualizations,
                'interpretation': interpretation,
                'quality_metrics': quality_metrics,
                'execution_metadata': execution_metadata,
                'success_message': "ğŸ“Š í†µê³„ ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            }
                
        except Exception as e:
            self.logger.error(f"í†µê³„ ë¶„ì„ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': str(e),
                'error_type': 'execution_error'
            }
    
    def _execute_statistical_analysis(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """í†µê³„ ë¶„ì„ ì‹¤í–‰"""
        try:
            # 1. ì½”ë“œ ì‹¤í–‰ ì¤€ë¹„
            execution_context = self._prepare_execution_context(input_data)
            
            # 2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰
            preprocessed_data = self._execute_preprocessing(
                input_data['data_requirements']
            )
            
            # 3. í†µê³„ ë¶„ì„ ì‹¤í–‰
            statistics = self.stats_executor.execute_analysis(
                code=input_data['analysis_code']['main_script'],
                data=preprocessed_data,
                parameters=input_data['statistical_design']['parameters']
            )
            
            # 4. ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = self._calculate_analysis_metrics(statistics)
            
            # 5. ê²€ì¦ ì‹¤í–‰
            validation_results = self._execute_validations(
                statistics, input_data['execution_plan']['validation_checks']
            )
            
            return {
                'status': 'success',
                'statistics': statistics,
                'metrics': metrics,
                'validation_results': validation_results
            }
            
        except Exception as e:
            self.logger.error(f"í†µê³„ ë¶„ì„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return {
                'status': 'error',
                'error_message': str(e)
            }
    
    def _interpret_results_with_rag(self, execution_results: Dict[str, Any],
                                  input_data: Dict[str, Any]) -> Dict[str, Any]:
        """RAG ê¸°ë°˜ ê²°ê³¼ í•´ì„"""
        try:
            # 1. ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰
            interpretation_knowledge = self.rag_manager.search(
                query=self._build_interpretation_query(execution_results)
            )
            
            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•
            interpretation_context = self.rag_manager.build_context(
                query=self._build_interpretation_query(execution_results)
            )
            
            # 3. LLMì„ í†µí•œ í•´ì„ ìƒì„±
            prompt = self.prompt_engine.create_interpretation_prompt(
                context=interpretation_context
            )
            
            llm_response = self.llm_client.generate(prompt)
            
            # 4. í•´ì„ ê²°ê³¼ êµ¬ì¡°í™”
            interpretation = self._parse_interpretation_response(llm_response)
            
            return {
                'key_findings': interpretation.get('key_findings', []),
                'statistical_significance': interpretation.get('significance', {}),
                'limitations': interpretation.get('limitations', [])
            }
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ í•´ì„ ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': str(e)
            }
    
    def _generate_visualizations(self, execution_results: Dict[str, Any],
                               visualization_plan: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹œê°í™” ìƒì„±"""
        try:
            # 1. ê¸°ë³¸ í”Œë¡¯ ìƒì„±
            plots = self.plot_generator.generate_plots(
                data=execution_results['statistics'],
                plot_specs=visualization_plan['plots']
            )
            
            # 2. ì¸í„°ë™í‹°ë¸Œ ìš”ì†Œ ì¶”ê°€
            interactive_elements = self.plot_generator.add_interactivity(
                plots=plots,
                interactive_specs=visualization_plan['interactive_elements']
            )
            
            # 3. í”Œë¡¯ ì„¤ëª… ìƒì„±
            plot_descriptions = self._generate_plot_descriptions(
                plots, execution_results
            )
            
            return {
                'plots': plots,
                'interactive_elements': interactive_elements,
                'plot_descriptions': plot_descriptions
            }
            
        except Exception as e:
            self.logger.error(f"ì‹œê°í™” ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': str(e)
            }
    
    def _create_interpretation(self, execution_results: Dict[str, Any],
                             analysis_insights: Dict[str, Any],
                             visualizations: Dict[str, Any]) -> Dict[str, Any]:
        """ê²°ê³¼ í•´ì„ ë° ìš”ì•½"""
        try:
            # 1. ìš”ì•½ ìƒì„±
            summary = self._generate_analysis_summary(
                execution_results, analysis_insights
            )
            
            # 2. ìƒì„¸ ë¶„ì„
            detailed_analysis = self._generate_detailed_analysis(
                execution_results, analysis_insights, visualizations
            )
            
            # 3. ì¶”ì²œì‚¬í•­ ë„ì¶œ
            recommendations = self._generate_recommendations(
                detailed_analysis, analysis_insights
            )
            
            return {
                'summary': summary,
                'detailed_analysis': detailed_analysis,
                'recommendations': recommendations
            }
            
        except Exception as e:
            self.logger.error(f"í•´ì„ ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': str(e)
            }
    
    def _calculate_quality_metrics(self, execution_results: Dict[str, Any],
                                 statistical_design: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            # 1. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            reliability_scores = self._calculate_reliability_scores(
                execution_results
            )
            
            # 2. ê²€ì¦ ë©”íŠ¸ë¦­ ê³„ì‚°
            validation_metrics = self._calculate_validation_metrics(
                execution_results, statistical_design
            )
            
            # 3. ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
            confidence_intervals = self._calculate_confidence_intervals(
                execution_results, statistical_design
            )
            
            return {
                'reliability_scores': reliability_scores,
                'validation_metrics': validation_metrics,
                'confidence_intervals': confidence_intervals
            }
            
        except Exception as e:
            self.logger.error(f"í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': str(e)
            }
    
    def _collect_execution_metadata(self) -> Dict[str, Any]:
        """ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # 1. ëŸ°íƒ€ì„ í†µê³„ ìˆ˜ì§‘
            runtime_stats = self.stats_executor.get_runtime_statistics()
            
            # 2. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìˆ˜ì§‘
            resource_usage = self.stats_executor.get_resource_usage()
            
            # 3. ì˜¤ë¥˜ ë¡œê·¸ ìˆ˜ì§‘
            error_logs = self.stats_executor.get_error_logs()
            
            return {
                'runtime_stats': runtime_stats,
                'resource_usage': resource_usage,
                'error_logs': error_logs
            }
            
        except Exception as e:
            self.logger.error(f"ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': str(e)
            }
    
    def _prepare_execution_context(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„"""
        try:
            execution_context = input_data.get('execution_context', {})
            analysis_code = input_data.get('analysis_code', {})
            data_requirements = input_data.get('data_requirements', {})
            
            # ì‹¤í–‰ í™˜ê²½ ì„¤ì •
            context = {
                'workspace_path': '/tmp/statistical_analysis',
                'data_path': data_requirements.get('data_path', ''),
                'output_path': '/tmp/analysis_output',
                'temp_path': '/tmp/analysis_temp'
            }
            
            # íŒŒë¼ë¯¸í„° ì„¤ì •
            context['parameters'] = execution_context.get('parameters', {})
            context['constraints'] = execution_context.get('constraints', {})
            context['special_instructions'] = execution_context.get('special_instructions', [])
            
            # ì½”ë“œ ì‹¤í–‰ ì„¤ì •
            context['code_settings'] = {
                'timeout': context['constraints'].get('max_execution_time', 300),
                'memory_limit': context['constraints'].get('max_memory_usage', 1024),
                'safe_mode': context['constraints'].get('safe_mode', True),
                'allowed_imports': ['pandas', 'numpy', 'scipy', 'matplotlib', 'seaborn', 'statsmodels'],
                'restricted_imports': context['constraints'].get('restricted_imports', ['os', 'subprocess', 'sys'])
            }
            
            # ë°ì´í„° ì„¤ì •
            context['data_settings'] = {
                'encoding': data_requirements.get('encoding', 'utf-8'),
                'separator': data_requirements.get('separator', ','),
                'missing_values': data_requirements.get('missing_values', ['', 'NA', 'NULL']),
                'data_types': data_requirements.get('data_types', {})
            }
            
            # ì¶œë ¥ ì„¤ì •
            context['output_settings'] = {
                'save_plots': True,
                'plot_format': ['png', 'html'],
                'save_data': True,
                'save_results': True,
                'generate_report': True
            }
            
            return context
            
        except Exception as e:
            self.logger.error(f"ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ì˜¤ë¥˜: {e}")
            return {
                'workspace_path': '/tmp/statistical_analysis',
                'parameters': {},
                'constraints': {'max_execution_time': 300, 'safe_mode': True}
            }
    
    def _execute_preprocessing(self, data_requirements: Dict[str, Any]) -> Any:
        """ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰"""
        try:
            from services.statistics.data_preprocessor import DataPreprocessor
            
            preprocessor = DataPreprocessor()
            
            # ë°ì´í„° ë¡œë“œ
            data_path = data_requirements.get('data_path', '')
            if not data_path:
                raise ValueError("ë°ì´í„° ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # ì „ì²˜ë¦¬ ì˜µì…˜ ì„¤ì •
            preprocessing_options = {
                'handle_missing': data_requirements.get('missing_value_handling', 'drop'),
                'outlier_treatment': data_requirements.get('outlier_handling', 'identify'),
                'normalization': data_requirements.get('normalization', None),
                'encoding': data_requirements.get('categorical_encoding', 'label')
            }
            
            # ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰
            preprocessed_data = preprocessor.preprocess_data(
                data_path=data_path,
                target_columns=data_requirements.get('target_columns', []),
                options=preprocessing_options
            )
            
            # ì „ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦
            if preprocessed_data is None:
                raise ValueError("ë°ì´í„° ì „ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            self.logger.info(f"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {preprocessed_data.shape}")
            
            return preprocessed_data
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return None
    
    def _calculate_analysis_metrics(self, statistics: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            metrics = {}
            
            # ê¸°ë³¸ í†µê³„ëŸ‰ ë©”íŠ¸ë¦­
            if 'descriptive_stats' in statistics:
                desc_stats = statistics['descriptive_stats']
                metrics['data_quality'] = {
                    'completeness': desc_stats.get('completeness_ratio', 0.0),
                    'sample_size': desc_stats.get('count', 0),
                    'missing_percentage': desc_stats.get('missing_percentage', 0.0)
                }
            
            # í†µê³„ ê²€ì • ë©”íŠ¸ë¦­
            if 'test_results' in statistics:
                test_results = statistics['test_results']
                metrics['statistical_significance'] = {
                    'p_value': test_results.get('p_value', 1.0),
                    'test_statistic': test_results.get('statistic', 0.0),
                    'degrees_of_freedom': test_results.get('df', 0),
                    'is_significant': test_results.get('p_value', 1.0) < 0.05
                }
                
                # íš¨ê³¼í¬ê¸° ê³„ì‚°
                if 'effect_size' in test_results:
                    metrics['effect_size'] = {
                        'value': test_results['effect_size'],
                        'interpretation': self._interpret_effect_size(
                            test_results['effect_size'], 
                            test_results.get('test_type', '')
                        )
                    }
            
            # ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ (íšŒê·€ë¶„ì„ì¸ ê²½ìš°)
            if 'model_summary' in statistics:
                model_summary = statistics['model_summary']
                metrics['model_performance'] = {
                    'r_squared': model_summary.get('r_squared', 0.0),
                    'adjusted_r_squared': model_summary.get('adj_r_squared', 0.0),
                    'f_statistic': model_summary.get('f_statistic', 0.0),
                    'aic': model_summary.get('aic', float('inf')),
                    'bic': model_summary.get('bic', float('inf'))
                }
            
            # ê°€ì • ê²€ì¦ ë©”íŠ¸ë¦­
            if 'assumption_tests' in statistics:
                assumption_tests = statistics['assumption_tests']
                metrics['assumption_validity'] = {}
                
                for test_name, test_result in assumption_tests.items():
                    metrics['assumption_validity'][test_name] = {
                        'passed': test_result.get('p_value', 0.0) > 0.05,
                        'p_value': test_result.get('p_value', 1.0),
                        'test_statistic': test_result.get('statistic', 0.0)
                    }
            
            # ì‹ ë¢°ë„ ë©”íŠ¸ë¦­
            metrics['reliability'] = {
                'sample_adequacy': self._assess_sample_adequacy(statistics),
                'assumption_score': self._calculate_assumption_score(metrics.get('assumption_validity', {})),
                'overall_confidence': self._calculate_overall_confidence(metrics)
            }
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {
                'data_quality': {'completeness': 0.0, 'sample_size': 0},
                'reliability': {'overall_confidence': 0.0}
            }
    
    def _execute_validations(self, statistics: Dict[str, Any],
                           validation_checks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê²€ì¦ ì‹¤í–‰"""
        try:
            validation_results = []
            
            for check in validation_checks:
                check_type = check.get('type', '')
                check_name = check.get('name', check_type)
                
                result = {
                    'check_name': check_name,
                    'check_type': check_type,
                    'passed': False,
                    'message': '',
                    'details': {}
                }
                
                try:
                    if check_type == 'data_completeness_check':
                        result.update(self._validate_data_completeness(statistics, check))
                    elif check_type == 'outlier_detection':
                        result.update(self._validate_outliers(statistics, check))
                    elif check_type == 'normality_assumption':
                        result.update(self._validate_normality(statistics, check))
                    elif check_type == 'independence_assumption':
                        result.update(self._validate_independence(statistics, check))
                    elif check_type == 'homoscedasticity_assumption':
                        result.update(self._validate_homoscedasticity(statistics, check))
                    elif check_type == 'linearity_assumption':
                        result.update(self._validate_linearity(statistics, check))
                    elif check_type == 'multicollinearity_check':
                        result.update(self._validate_multicollinearity(statistics, check))
                    elif check_type == 'sample_size_adequacy':
                        result.update(self._validate_sample_size(statistics, check))
                    else:
                        result['message'] = f"ì•Œ ìˆ˜ ì—†ëŠ” ê²€ì¦ ìœ í˜•: {check_type}"
                        
                except Exception as check_error:
                    result['message'] = f"ê²€ì¦ ì‹¤í–‰ ì˜¤ë¥˜: {str(check_error)}"
                    result['details']['error'] = str(check_error)
                
                validation_results.append(result)
            
            return validation_results
            
        except Exception as e:
            self.logger.error(f"ê²€ì¦ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return [{
                'check_name': 'error',
                'check_type': 'error',
                'passed': False,
                'message': f"ê²€ì¦ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }]
    
    def _build_interpretation_query(self, execution_results: Dict[str, Any]) -> str:
        """í•´ì„ ì¿¼ë¦¬ ìƒì„±"""
        try:
            statistics = execution_results.get('statistics', {})
            test_results = statistics.get('test_results', {})
            
            # ê¸°ë³¸ ì •ë³´
            sample_size = statistics.get('descriptive_stats', {}).get('sample_size', 'N/A')
            num_tests = len(test_results)
            
            # ìœ ì˜í•œ ê²°ê³¼ ì§‘ê³„
            significant_tests = []
            for test_name, result in test_results.items():
                if result.get('p_value', 1.0) < 0.05:
                    significant_tests.append(test_name)
            
            # ìš”ì•½ ë¬¸ì„œ êµ¬ì„±
            query = f"""
ë‹¤ìŒ í†µê³„ ë¶„ì„ ê²°ê³¼ë¥¼ í•´ì„í•´ì£¼ì„¸ìš”:

### ê¸°ë³¸ ì •ë³´
- í‘œë³¸ í¬ê¸°: {sample_size}
- ì‹¤í–‰ëœ ê²€ì • ìˆ˜: {num_tests}
- ìœ ì˜í•œ ê²°ê³¼: {len(significant_tests)}ê°œ

### ì£¼ìš” ë°œê²¬ì‚¬í•­
"""
            
            # ìœ ì˜í•œ ê²°ê³¼ë“¤ ìš”ì•½
            if significant_tests:
                query += "**í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê²°ê³¼:**\n"
                for test_name in significant_tests[:5]:  # ìµœëŒ€ 5ê°œ
                    result = test_results[test_name]
                    p_value = result.get('p_value', 1.0)
                    effect_size = result.get('effect_size', 'N/A')
                    query += f"- {test_name}: p = {p_value:.4f}"
                    if effect_size != 'N/A':
                        query += f", íš¨ê³¼í¬ê¸° = {effect_size}"
                    query += "\n"
            else:
                query += "**í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê²°ê³¼ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.**\n"
            
            # í•´ì„ ì¸ì‚¬ì´íŠ¸ í¬í•¨
            if 'key_insights' in execution_results:
                insights = execution_results['key_insights']
                if insights:
                    query += "\n### í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n"
                    for insight in insights[:3]:  # ìµœëŒ€ 3ê°œ
                        query += f"- {insight}\n"
            
            # ê²°ë¡ 
            if 'conclusion' in execution_results:
                query += f"\n### ê²°ë¡ \n{execution_results['conclusion']}\n"
            
            # ê¶Œê³ ì‚¬í•­
            if 'follow_up_suggestions' in execution_results:
                suggestions = execution_results['follow_up_suggestions']
                if suggestions:
                    query += "\n### ê¶Œê³ ì‚¬í•­\n"
                    for suggestion in suggestions[:3]:  # ìµœëŒ€ 3ê°œ
                        query += f"- {suggestion}\n"
            
            query += "\n### ì£¼ì˜ì‚¬í•­\n"
            query += "- ì´ ë¶„ì„ ê²°ê³¼ëŠ” ì œê³µëœ ë°ì´í„°ì™€ ê°€ì •ì— ê¸°ë°˜í•©ë‹ˆë‹¤.\n"
            query += "- ì‹¤ë¬´ì  ì˜ì‚¬ê²°ì •ì‹œ ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ì˜ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n"
            query += "- ì¶”ê°€ì ì¸ ë°ì´í„° ìˆ˜ì§‘ì´ë‚˜ ë¶„ì„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
            
            return query
            
        except Exception as e:
            self.logger.error(f"í•´ì„ ì¿¼ë¦¬ ìƒì„± ì˜¤ë¥˜: {e}")
            return "í†µê³„ ë¶„ì„ ê²°ê³¼ë¥¼ ì¼ë°˜ì ì¸ ê´€ì ì—ì„œ í•´ì„í•´ì£¼ì„¸ìš”."
    
    def _parse_interpretation_response(self, llm_response: str) -> Dict[str, Any]:
        """í•´ì„ ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            import re
            
            interpretation = {
                'statistical_significance': '',
                'practical_significance': '',
                'reliability_assessment': '',
                'limitations': [],
                'follow_up_suggestions': [],
                'key_insights': [],
                'conclusion': ''
            }
            
            # JSON í˜•íƒœ ì‘ë‹µ ì‹œë„
            json_match = re.search(r'```json\s*(.*?)\s*```', llm_response, re.DOTALL)
            if json_match:
                try:
                    parsed = json.loads(json_match.group(1))
                    return parsed
                except json.JSONDecodeError:
                    # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ êµ¬ì¡°ë¡œ í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„
                    self.logger.warning("JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ íŒŒì‹±ìœ¼ë¡œ ì „í™˜")
                    return self._fallback_interpretation_parsing(llm_response)
            
            # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ íŒŒì‹±
            response_lower = llm_response.lower()
            
            # 1. í†µê³„ì  ìœ ì˜ì„± ì„¹ì…˜ ì¶”ì¶œ
            stat_sig_pattern = r'í†µê³„ì \s*ìœ ì˜ì„±[:\s]*(.*?)(?=ì‹¤ë¬´ì |ì‹¤ìš©ì |ì‹ ë¢°ì„±|í•œê³„|ê²°ë¡ |$)'
            stat_sig_match = re.search(stat_sig_pattern, llm_response, re.IGNORECASE | re.DOTALL)
            if stat_sig_match:
                interpretation['statistical_significance'] = stat_sig_match.group(1).strip()
            
            # 2. ì‹¤ë¬´ì  ìœ ì˜ì„± ì„¹ì…˜ ì¶”ì¶œ
            practical_sig_pattern = r'(?:ì‹¤ë¬´ì |ì‹¤ìš©ì )\s*ìœ ì˜ì„±[:\s]*(.*?)(?=ì‹ ë¢°ì„±|í•œê³„|ê²°ë¡ |$)'
            practical_sig_match = re.search(practical_sig_pattern, llm_response, re.IGNORECASE | re.DOTALL)
            if practical_sig_match:
                interpretation['practical_significance'] = practical_sig_match.group(1).strip()
            
            # 3. ì‹ ë¢°ì„± í‰ê°€ ì„¹ì…˜ ì¶”ì¶œ
            reliability_pattern = r'ì‹ ë¢°ì„±[:\s]*(.*?)(?=í•œê³„|ê²°ë¡ |ì œì•ˆ|$)'
            reliability_match = re.search(reliability_pattern, llm_response, re.IGNORECASE | re.DOTALL)
            if reliability_match:
                interpretation['reliability_assessment'] = reliability_match.group(1).strip()
            
            # 4. í•œê³„ì  ì¶”ì¶œ
            limitation_pattern = r'í•œê³„[ì ]?[:\s]*(.*?)(?=ì œì•ˆ|ê¶Œê³ |ê²°ë¡ |$)'
            limitation_match = re.search(limitation_pattern, llm_response, re.IGNORECASE | re.DOTALL)
            if limitation_match:
                limitation_text = limitation_match.group(1)
                limitations = re.findall(r'[-â€¢]\s*([^-â€¢\n]+)', limitation_text)
                interpretation['limitations'] = [lim.strip() for lim in limitations if lim.strip()]
            
            # 5. í›„ì† ì œì•ˆ ì¶”ì¶œ
            follow_up_pattern = r'(?:í›„ì†|ì¶”ê°€|ì œì•ˆ|ê¶Œê³ )[:\s]*(.*?)(?=ê²°ë¡ |$)'
            follow_up_match = re.search(follow_up_pattern, llm_response, re.IGNORECASE | re.DOTALL)
            if follow_up_match:
                follow_up_text = follow_up_match.group(1)
                suggestions = re.findall(r'[-â€¢]\s*([^-â€¢\n]+)', follow_up_text)
                interpretation['follow_up_suggestions'] = [sug.strip() for sug in suggestions if sug.strip()]
            
            # 6. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ (ì£¼ìš” í¬ì¸íŠ¸ë“¤)
            insight_keywords = ['ì¤‘ìš”í•œ', 'í•µì‹¬', 'ì£¼ëª©í• ', 'íŠ¹íˆ', 'ê²°ê³¼ì ìœ¼ë¡œ']
            insights = []
            sentences = re.split(r'[.!?]', llm_response)
            for sentence in sentences:
                if any(keyword in sentence for keyword in insight_keywords):
                    clean_sentence = sentence.strip()
                    if len(clean_sentence) > 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                        insights.append(clean_sentence)
            interpretation['key_insights'] = insights[:5]  # ìµœëŒ€ 5ê°œ
            
            # 7. ê²°ë¡  ì¶”ì¶œ
            conclusion_pattern = r'ê²°ë¡ [:\s]*(.*?)$'
            conclusion_match = re.search(conclusion_pattern, llm_response, re.IGNORECASE | re.DOTALL)
            if conclusion_match:
                interpretation['conclusion'] = conclusion_match.group(1).strip()
            
            # ë¹ˆ í•„ë“œë“¤ì— ëŒ€í•œ ê¸°ë³¸ê°’ ì„¤ì •
            if not interpretation['statistical_significance']:
                interpretation['statistical_significance'] = "í†µê³„ ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ í•´ì„ì„ ì°¸ì¡°í•˜ì„¸ìš”."
            
            if not interpretation['conclusion']:
                # ë§ˆì§€ë§‰ ë¬¸ë‹¨ì„ ê²°ë¡ ìœ¼ë¡œ ì‚¬ìš©
                paragraphs = llm_response.split('\n\n')
                if paragraphs:
                    interpretation['conclusion'] = paragraphs[-1].strip()
            
            return interpretation
            
        except Exception as e:
            self.logger.error(f"í•´ì„ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {
                'statistical_significance': 'íŒŒì‹± ì˜¤ë¥˜ë¡œ ì¸í•´ í•´ì„ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'practical_significance': '',
                'reliability_assessment': '',
                'limitations': ['ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ'],
                'follow_up_suggestions': ['ì „ë¬¸ê°€ ìƒë‹´ ê¶Œì¥'],
                'key_insights': [],
                'conclusion': 'ê²°ê³¼ í•´ì„ì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.',
                'error': str(e)
            }
    
    def _generate_plot_descriptions(self, plots: List[Dict[str, Any]],
                                  execution_results: Dict[str, Any]) -> Dict[str, Any]:
        """í”Œë¡¯ ì„¤ëª… ìƒì„±"""
        try:
            plot_descriptions = {
                'individual_plots': {},
                'plot_relationships': [],
                'visual_insights': [],
                'interpretation_guide': {}
            }
            
            statistics = execution_results.get('statistics', {})
            test_results = statistics.get('test_results', {})
            
            # ê°œë³„ í”Œë¡¯ ì„¤ëª… ìƒì„±
            for plot in plots:
                plot_id = plot.get('id', f"plot_{len(plot_descriptions['individual_plots']) + 1}")
                plot_type = plot.get('type', 'unknown')
                plot_title = plot.get('title', 'Untitled Plot')
                
                description = self._generate_single_plot_description(
                    plot, test_results
                )
                
                plot_descriptions['individual_plots'][plot_id] = {
                    'title': plot_title,
                    'type': plot_type,
                    'description': description,
                    'key_features': self._identify_plot_key_features(plot, test_results),
                    'interpretation_tips': self._generate_plot_interpretation_tips(plot_type)
                }
            
            # í”Œë¡¯ ê°„ ê´€ê³„ ë¶„ì„
            plot_descriptions['plot_relationships'] = self._analyze_plot_relationships(plots)
            
            # ì‹œê°ì  ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
            plot_descriptions['visual_insights'] = self._derive_visual_insights(plots, test_results)
            
            # í•´ì„ ê°€ì´ë“œ ìƒì„±
            plot_descriptions['interpretation_guide'] = self._create_interpretation_guide(plots)
            
            return plot_descriptions
            
        except Exception as e:
            self.logger.error(f"í”Œë¡¯ ì„¤ëª… ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                'individual_plots': {},
                'plot_relationships': [],
                'visual_insights': ['í”Œë¡¯ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'],
                'interpretation_guide': {'error': str(e)}
            }
    
    def _generate_single_plot_description(self, plot: Dict[str, Any], 
                                        test_results: Dict[str, Any]) -> str:
        """ë‹¨ì¼ í”Œë¡¯ ì„¤ëª… ìƒì„±"""
        plot_type = plot.get('type', 'unknown')
        
        if plot_type == 'histogram':
            return self._describe_histogram(plot, test_results)
        elif plot_type == 'boxplot':
            return self._describe_boxplot(plot, test_results)
        elif plot_type == 'scatter':
            return self._describe_scatterplot(plot, test_results)
        elif plot_type == 'bar':
            return self._describe_barplot(plot, test_results)
        elif plot_type == 'line':
            return self._describe_lineplot(plot, test_results)
        elif 'correlation' in plot_type:
            return self._describe_correlation_plot(plot, test_results)
        elif 'residual' in plot_type:
            return self._describe_residual_plot(plot, test_results)
        else:
            return f"ì´ {plot_type} í”Œë¡¯ì€ ë°ì´í„°ì˜ íŒ¨í„´ê³¼ ê´€ê³„ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤."
    
    def _describe_histogram(self, plot: Dict[str, Any], test_results: Dict[str, Any]) -> str:
        """íˆìŠ¤í† ê·¸ë¨ ì„¤ëª…"""
        variable = plot.get('variables', ['ë³€ìˆ˜'])[0] if plot.get('variables') else 'ë³€ìˆ˜'
        
        description = f"{variable}ì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì£¼ëŠ” íˆìŠ¤í† ê·¸ë¨ì…ë‹ˆë‹¤. "
        description += "ë§‰ëŒ€ì˜ ë†’ì´ëŠ” í•´ë‹¹ êµ¬ê°„ì— ì†í•˜ëŠ” ë°ì´í„° í¬ì¸íŠ¸ì˜ ë¹ˆë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. "
        
        # ì •ê·œì„± ê²€ì • ê²°ê³¼ê°€ ìˆë‹¤ë©´ í¬í•¨
        normality_tests = ['shapiro_wilk', 'kolmogorov_smirnov', 'anderson_darling']
        for test_name in normality_tests:
            if test_name in test_results:
                result = test_results[test_name]
                p_value = result.get('p_value', 1.0)
                if p_value < 0.05:
                    description += f"ì •ê·œì„± ê²€ì • ê²°ê³¼ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤ (p = {p_value:.3f}). "
                else:
                    description += f"ì •ê·œì„± ê²€ì • ê²°ê³¼ ì •ê·œë¶„í¬ ê°€ì •ì„ ë§Œì¡±í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤ (p = {p_value:.3f}). "
                break
        
        description += "ë¶„í¬ì˜ ëª¨ì–‘, ì¤‘ì‹¬ ê²½í–¥ì„±, ê·¸ë¦¬ê³  ì´ìƒê°’ ì—¬ë¶€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        return description
    
    def _describe_boxplot(self, plot: Dict[str, Any], test_results: Dict[str, Any]) -> str:
        """ë°•ìŠ¤í”Œë¡¯ ì„¤ëª…"""
        variables = plot.get('variables', ['ë³€ìˆ˜'])
        
        description = f"{', '.join(variables)}ì˜ ë¶„í¬ íŠ¹ì„±ì„ ìš”ì•½í•œ ë°•ìŠ¤í”Œë¡¯ì…ë‹ˆë‹¤. "
        description += "ë°•ìŠ¤ëŠ” 1ì‚¬ë¶„ìœ„ìˆ˜(Q1)ë¶€í„° 3ì‚¬ë¶„ìœ„ìˆ˜(Q3)ê¹Œì§€ì˜ êµ¬ê°„ì„ ë‚˜íƒ€ë‚´ë©°, "
        description += "ë°•ìŠ¤ ë‚´ë¶€ì˜ ì„ ì€ ì¤‘ì•™ê°’(median)ì„ í‘œì‹œí•©ë‹ˆë‹¤. "
        description += "ë°•ìŠ¤ì—ì„œ ì—°ì¥ëœ ì„ (whiskers)ì€ ë°ì´í„°ì˜ ë²”ìœ„ë¥¼ ë³´ì—¬ì£¼ê³ , "
        description += "ê°œë³„ ì ë“¤ì€ ì´ìƒê°’(outliers)ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤."
        
        return description
    
    def _describe_scatterplot(self, plot: Dict[str, Any], test_results: Dict[str, Any]) -> str:
        """ì‚°ì ë„ ì„¤ëª…"""
        variables = plot.get('variables', ['Xë³€ìˆ˜', 'Yë³€ìˆ˜'])
        x_var = variables[0] if len(variables) > 0 else 'Xë³€ìˆ˜'
        y_var = variables[1] if len(variables) > 1 else 'Yë³€ìˆ˜'
        
        description = f"{x_var}ì™€ {y_var} ê°„ì˜ ê´€ê³„ë¥¼ ë³´ì—¬ì£¼ëŠ” ì‚°ì ë„ì…ë‹ˆë‹¤. "
        
        # ìƒê´€ë¶„ì„ ê²°ê³¼ê°€ ìˆë‹¤ë©´ í¬í•¨
        correlation_tests = ['pearson_correlation', 'spearman_correlation']
        for test_name in correlation_tests:
            if test_name in test_results:
                result = test_results[test_name]
                correlation = result.get('correlation', 0)
                p_value = result.get('p_value', 1.0)
                
                strength = "ì•½í•œ" if abs(correlation) < 0.3 else "ë³´í†µ" if abs(correlation) < 0.7 else "ê°•í•œ"
                direction = "ì–‘ì˜" if correlation > 0 else "ìŒì˜"
                
                if p_value < 0.05:
                    description += f"ë‘ ë³€ìˆ˜ ê°„ì—ëŠ” {strength} {direction} ìƒê´€ê´€ê³„ê°€ ìˆìŠµë‹ˆë‹¤ "
                    description += f"(r = {correlation:.3f}, p = {p_value:.3f}). "
                else:
                    description += f"ë‘ ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ "
                    description += f"(r = {correlation:.3f}, p = {p_value:.3f}). "
                break
        
        description += "ì ë“¤ì˜ íŒ¨í„´ì„ í†µí•´ ì„ í˜•ì„±, ì´ìƒê°’, ê·¸ë¦¬ê³  ê´€ê³„ì˜ ê°•ë„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        return description
    
    def _describe_barplot(self, plot: Dict[str, Any], test_results: Dict[str, Any]) -> str:
        """ë§‰ëŒ€ê·¸ë˜í”„ ì„¤ëª…"""
        variables = plot.get('variables', ['ë²”ì£¼ë³€ìˆ˜'])
        variable = variables[0] if variables else 'ë²”ì£¼ë³€ìˆ˜'
        
        description = f"{variable}ì˜ ê° ë²”ì£¼ë³„ ë¹ˆë„ ë˜ëŠ” í‰ê· ê°’ì„ ë³´ì—¬ì£¼ëŠ” ë§‰ëŒ€ê·¸ë˜í”„ì…ë‹ˆë‹¤. "
        description += "ë§‰ëŒ€ì˜ ë†’ì´ëŠ” í•´ë‹¹ ë²”ì£¼ì˜ ê°’ì˜ í¬ê¸°ë¥¼ ë‚˜íƒ€ë‚´ë©°, "
        description += "ë²”ì£¼ ê°„ ì°¨ì´ë¥¼ ì§ê´€ì ìœ¼ë¡œ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        # ì¹´ì´ì œê³± ê²€ì •ì´ë‚˜ ANOVA ê²°ê³¼ê°€ ìˆë‹¤ë©´ í¬í•¨
        group_tests = ['chi_square', 'one_way_anova', 'kruskal_wallis']
        for test_name in group_tests:
            if test_name in test_results:
                result = test_results[test_name]
                p_value = result.get('p_value', 1.0)
                
                if p_value < 0.05:
                    description += f" ë²”ì£¼ ê°„ ì°¨ì´ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•©ë‹ˆë‹¤ (p = {p_value:.3f}). "
                else:
                    description += f" ë²”ì£¼ ê°„ ì°¨ì´ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (p = {p_value:.3f}). "
                break
        
        return description
    
    def _describe_lineplot(self, plot: Dict[str, Any], test_results: Dict[str, Any]) -> str:
        """ì„ ê·¸ë˜í”„ ì„¤ëª…"""
        description = "ì‹œê°„ì— ë”°ë¥¸ ë³€í™”ë‚˜ ìˆœì„œí˜• ë³€ìˆ˜ì˜ íŒ¨í„´ì„ ë³´ì—¬ì£¼ëŠ” ì„ ê·¸ë˜í”„ì…ë‹ˆë‹¤. "
        description += "ì„ ì˜ ê¸°ìš¸ê¸°ì™€ ë³€í™” íŒ¨í„´ì„ í†µí•´ íŠ¸ë Œë“œì™€ ì£¼ê¸°ì  ë³€ë™ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        return description
    
    def _describe_correlation_plot(self, plot: Dict[str, Any], test_results: Dict[str, Any]) -> str:
        """ìƒê´€ê´€ê³„ í”Œë¡¯ ì„¤ëª…"""
        description = "ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•œ í”Œë¡¯ì…ë‹ˆë‹¤. "
        description += "ìƒ‰ìƒì˜ ê°•ë„ì™€ ë°©í–¥ì€ ìƒê´€ê³„ìˆ˜ì˜ í¬ê¸°ì™€ ë°©í–¥ì„ ë‚˜íƒ€ë‚´ë©°, "
        description += "ë³€ìˆ˜ë“¤ ê°„ì˜ ë³µì¡í•œ ê´€ê³„ íŒ¨í„´ì„ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        return description
    
    def _describe_residual_plot(self, plot: Dict[str, Any], test_results: Dict[str, Any]) -> str:
        """ì”ì°¨ í”Œë¡¯ ì„¤ëª…"""
        description = "íšŒê·€ ëª¨ë¸ì˜ ì”ì°¨(residuals)ë¥¼ ì‹œê°í™”í•œ í”Œë¡¯ì…ë‹ˆë‹¤. "
        description += "ì”ì°¨ì˜ íŒ¨í„´ì„ í†µí•´ ëª¨ë¸ì˜ ê°€ì • ìœ„ë°° ì—¬ë¶€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©°, "
        description += "ë“±ë¶„ì‚°ì„±, ì„ í˜•ì„±, ë…ë¦½ì„± ë“±ì˜ íšŒê·€ ê°€ì •ì„ ê²€í† í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤."
        
        return description
    
    def _identify_plot_key_features(self, plot: Dict[str, Any], 
                                  test_results: Dict[str, Any]) -> List[str]:
        """í”Œë¡¯ì˜ ì£¼ìš” íŠ¹ì§• ì‹ë³„"""
        features = []
        plot_type = plot.get('type', 'unknown')
        
        if plot_type == 'histogram':
            features.extend(['ë¶„í¬ì˜ ëª¨ì–‘', 'ì¤‘ì‹¬ ê²½í–¥ì„±', 'ì‚°í¬ë„', 'ì´ìƒê°’'])
        elif plot_type == 'boxplot':
            features.extend(['ì¤‘ì•™ê°’', 'ì‚¬ë¶„ìœ„ìˆ˜', 'ì´ìƒê°’', 'ë¶„í¬ì˜ ëŒ€ì¹­ì„±'])
        elif plot_type == 'scatter':
            features.extend(['ì„ í˜• ê´€ê³„', 'ìƒê´€ ê°•ë„', 'ì´ìƒê°’', 'íŒ¨í„´'])
        elif plot_type == 'bar':
            features.extend(['ë²”ì£¼ë³„ ë¹ˆë„', 'ê·¸ë£¹ ê°„ ì°¨ì´', 'ìƒëŒ€ì  í¬ê¸°'])
        elif 'correlation' in plot_type:
            features.extend(['ìƒê´€ ê°•ë„', 'ìƒê´€ ë°©í–¥', 'ë³€ìˆ˜ ê°„ ê´€ê³„'])
        elif 'residual' in plot_type:
            features.extend(['ì”ì°¨ íŒ¨í„´', 'ë“±ë¶„ì‚°ì„±', 'ì„ í˜•ì„±', 'ê°€ì • ìœ„ë°°'])
        
        return features
    
    def _generate_plot_interpretation_tips(self, plot_type: str) -> List[str]:
        """í”Œë¡¯ í•´ì„ íŒ ìƒì„±"""
        tips = []
        
        if plot_type == 'histogram':
            tips.extend([
                "ì •ê·œë¶„í¬ì— ê°€ê¹Œìš´ ì¢… ëª¨ì–‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”",
                "ì¹˜ìš°ì¹¨(skewness)ì´ ìˆëŠ”ì§€ ê´€ì°°í•˜ì„¸ìš”",
                "ì´ìƒê°’ì´ë‚˜ ë‹¤ì¤‘ ëª¨ë“œê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”"
            ])
        elif plot_type == 'boxplot':
            tips.extend([
                "ë°•ìŠ¤ì˜ ìœ„ì¹˜ì™€ í¬ê¸°ë¡œ ë¶„í¬ íŠ¹ì„±ì„ íŒŒì•…í•˜ì„¸ìš”",
                "ì´ìƒê°’ë“¤ì´ ì˜ë¯¸ê°€ ìˆëŠ”ì§€ ê²€í† í•˜ì„¸ìš”",
                "ê·¸ë£¹ ê°„ ë°•ìŠ¤ì˜ ì°¨ì´ë¥¼ ë¹„êµí•˜ì„¸ìš”"
            ])
        elif plot_type == 'scatter':
            tips.extend([
                "ì ë“¤ì´ ì§ì„  íŒ¨í„´ì„ ë³´ì´ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”",
                "ì´ìƒê°’ì´ ìƒê´€ê´€ê³„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê³ ë ¤í•˜ì„¸ìš”",
                "ë¹„ì„ í˜• ê´€ê³„ì˜ ê°€ëŠ¥ì„±ì„ ê²€í† í•˜ì„¸ìš”"
            ])
        elif plot_type == 'bar':
            tips.extend([
                "ë²”ì£¼ ê°„ ì°¨ì´ì˜ í¬ê¸°ì™€ ë°©í–¥ì„ í™•ì¸í•˜ì„¸ìš”",
                "í‘œë³¸ í¬ê¸°ì˜ ì°¨ì´ë¥¼ ê³ ë ¤í•˜ì„¸ìš”",
                "ì‹¤ì§ˆì  ìœ ì˜ì„±ì„ í•¨ê»˜ í‰ê°€í•˜ì„¸ìš”"
            ])
        
        return tips
    
    def _analyze_plot_relationships(self, plots: List[Dict[str, Any]]) -> List[str]:
        """í”Œë¡¯ ê°„ ê´€ê³„ ë¶„ì„"""
        relationships = []
        
        if len(plots) < 2:
            return relationships
        
        # ìƒë³´ì  ê´€ê³„ ì‹ë³„
        plot_types = [plot.get('type', '') for plot in plots]
        
        if 'histogram' in plot_types and 'boxplot' in plot_types:
            relationships.append("íˆìŠ¤í† ê·¸ë¨ê³¼ ë°•ìŠ¤í”Œë¡¯ì´ í•¨ê»˜ ë¶„í¬ì˜ ì „ì²´ íŠ¹ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤")
        
        if 'scatter' in plot_types and any('correlation' in ptype for ptype in plot_types):
            relationships.append("ì‚°ì ë„ì™€ ìƒê´€ë¶„ì„ ê²°ê³¼ê°€ ë³€ìˆ˜ ê°„ ê´€ê³„ë¥¼ ë‹¤ê°ë„ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤")
        
        if any('residual' in ptype for ptype in plot_types) and 'scatter' in plot_types:
            relationships.append("ì”ì°¨ í”Œë¡¯ì´ íšŒê·€ ëª¨ë¸ì˜ ì í•©ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤")
        
        return relationships
    
    def _derive_visual_insights(self, plots: List[Dict[str, Any]], 
                              test_results: Dict[str, Any]) -> List[str]:
        """ì‹œê°ì  ì¸ì‚¬ì´íŠ¸ ë„ì¶œ"""
        insights = []
        
        # í”Œë¡¯ ìœ í˜•ë³„ ì¸ì‚¬ì´íŠ¸
        for plot in plots:
            plot_type = plot.get('type', '')
            
            if 'distribution' in plot.get('insights', {}):
                insights.append("ë°ì´í„° ë¶„í¬ì˜ íŠ¹ì„±ì´ ë¶„ì„ ë°©ë²• ì„ íƒì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤")
            
            if 'outliers' in plot.get('insights', {}):
                insights.append("ì´ìƒê°’ì˜ ì¡´ì¬ê°€ ë¶„ì„ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹ ì¤‘íˆ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤")
            
            if 'pattern' in plot.get('insights', {}):
                insights.append("ë°ì´í„°ì˜ íŒ¨í„´ì´ ì˜ˆìƒê³¼ ë‹¤ë¥¼ ê²½ìš° ì¶”ê°€ íƒìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ì¼ë°˜ì  ì¸ì‚¬ì´íŠ¸
        if len(plots) > 2:
            insights.append("ë‹¤ì–‘í•œ ì‹œê°í™”ë¥¼ í†µí•´ ë°ì´í„°ì˜ ì„œë¡œ ë‹¤ë¥¸ ì¸¡ë©´ì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        return insights[:5]  # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
    
    def _create_interpretation_guide(self, plots: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í•´ì„ ê°€ì´ë“œ ìƒì„±"""
        guide = {
            'reading_order': [],
            'key_questions': [],
            'common_mistakes': [],
            'best_practices': []
        }
        
        # ì½ê¸° ìˆœì„œ ì œì•ˆ
        plot_priorities = {
            'histogram': 1,
            'boxplot': 2,
            'scatter': 3,
            'correlation': 4,
            'residual': 5,
            'bar': 3
        }
        
        sorted_plots = sorted(plots, key=lambda p: plot_priorities.get(p.get('type', ''), 99))
        guide['reading_order'] = [f"{i+1}. {plot.get('title', f'Plot {i+1}')}" 
                                 for i, plot in enumerate(sorted_plots)]
        
        # í•µì‹¬ ì§ˆë¬¸ë“¤
        guide['key_questions'] = [
            "ì´ ì‹œê°í™”ê°€ ë³´ì—¬ì£¼ëŠ” ì£¼ìš” íŒ¨í„´ì€ ë¬´ì—‡ì¸ê°€?",
            "ë°ì´í„°ì˜ ê°€ì •ë“¤ì´ ë§Œì¡±ë˜ê³  ìˆëŠ”ê°€?",
            "ì´ìƒê°’ì´ë‚˜ íŠ¹ì´ì ë“¤ì´ ìˆëŠ”ê°€?",
            "ì‹œê°ì  íŒ¨í„´ì´ í†µê³„ ê²€ì • ê²°ê³¼ì™€ ì¼ì¹˜í•˜ëŠ”ê°€?"
        ]
        
        # í”í•œ ì‹¤ìˆ˜ë“¤
        guide['common_mistakes'] = [
            "ì‹œê°ì  íŒ¨í„´ë§Œìœ¼ë¡œ ì¸ê³¼ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ëŠ” ê²ƒ",
            "ì´ìƒê°’ì„ ë¬´ì¡°ê±´ ì œê±°í•˜ë ¤ëŠ” ì‹œë„",
            "ì²™ë„ì˜ ì°¨ì´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì€ ë¹„êµ",
            "í‘œë³¸ í¬ê¸°ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì€ í•´ì„"
        ]
        
        # ëª¨ë²” ê´€í–‰ë“¤
        guide['best_practices'] = [
            "ì—¬ëŸ¬ ì‹œê°í™”ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•˜ê¸°",
            "í†µê³„ ê²€ì • ê²°ê³¼ì™€ ì‹œê°í™”ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ê¸°",
            "ë„ë©”ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ í•´ì„í•˜ê¸°",
            "ê²°ê³¼ì˜ í•œê³„ì ì„ ëª…í™•íˆ ì¸ì‹í•˜ê¸°"
        ]
        
        return guide
    
    def _generate_analysis_summary(self, execution_results: Dict[str, Any],
                                 analysis_insights: Dict[str, Any]) -> str:
        """ë¶„ì„ ìš”ì•½ ìƒì„±"""
        try:
            statistics = execution_results.get('statistics', {})
            test_results = statistics.get('test_results', {})
            
            # ê¸°ë³¸ ì •ë³´
            sample_size = statistics.get('descriptive_stats', {}).get('sample_size', 'N/A')
            num_tests = len(test_results)
            
            # ìœ ì˜í•œ ê²°ê³¼ ì§‘ê³„
            significant_tests = []
            for test_name, result in test_results.items():
                if result.get('p_value', 1.0) < 0.05:
                    significant_tests.append(test_name)
            
            # ìš”ì•½ ë¬¸ì„œ êµ¬ì„±
            summary = f"""
## í†µê³„ ë¶„ì„ ìš”ì•½ ë³´ê³ ì„œ

### ê¸°ë³¸ ì •ë³´
- í‘œë³¸ í¬ê¸°: {sample_size}
- ì‹¤í–‰ëœ ê²€ì • ìˆ˜: {num_tests}
- ìœ ì˜í•œ ê²°ê³¼: {len(significant_tests)}ê°œ

### ì£¼ìš” ë°œê²¬ì‚¬í•­
"""
            
            # ìœ ì˜í•œ ê²°ê³¼ë“¤ ìš”ì•½
            if significant_tests:
                summary += "**í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê²°ê³¼:**\n"
                for test_name in significant_tests[:5]:  # ìµœëŒ€ 5ê°œ
                    result = test_results[test_name]
                    p_value = result.get('p_value', 1.0)
                    effect_size = result.get('effect_size', 'N/A')
                    summary += f"- {test_name}: p = {p_value:.4f}"
                    if effect_size != 'N/A':
                        summary += f", íš¨ê³¼í¬ê¸° = {effect_size}"
                    summary += "\n"
            else:
                summary += "**í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê²°ê³¼ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.**\n"
            
            # í•´ì„ ì¸ì‚¬ì´íŠ¸ í¬í•¨
            if 'key_insights' in analysis_insights:
                insights = analysis_insights['key_insights']
                if insights:
                    summary += "\n### í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n"
                    for insight in insights[:3]:  # ìµœëŒ€ 3ê°œ
                        summary += f"- {insight}\n"
            
            # ê²°ë¡ 
            if 'conclusion' in analysis_insights:
                summary += f"\n### ê²°ë¡ \n{analysis_insights['conclusion']}\n"
            
            # ê¶Œê³ ì‚¬í•­
            if 'follow_up_suggestions' in analysis_insights:
                suggestions = analysis_insights['follow_up_suggestions']
                if suggestions:
                    summary += "\n### ê¶Œê³ ì‚¬í•­\n"
                    for suggestion in suggestions[:3]:  # ìµœëŒ€ 3ê°œ
                        summary += f"- {suggestion}\n"
            
            summary += "\n### ì£¼ì˜ì‚¬í•­\n"
            summary += "- ì´ ë¶„ì„ ê²°ê³¼ëŠ” ì œê³µëœ ë°ì´í„°ì™€ ê°€ì •ì— ê¸°ë°˜í•©ë‹ˆë‹¤.\n"
            summary += "- ì‹¤ë¬´ì  ì˜ì‚¬ê²°ì •ì‹œ ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ì˜ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n"
            summary += "- ì¶”ê°€ì ì¸ ë°ì´í„° ìˆ˜ì§‘ì´ë‚˜ ë¶„ì„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
            
            return summary
            
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"""
## ë¶„ì„ ìš”ì•½ ìƒì„± ì˜¤ë¥˜

ë¶„ì„ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}

ê¸°ë³¸ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ê²€í† í•˜ì‹œê³ , í•„ìš”ì‹œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
"""
    
    def _generate_detailed_analysis(self, execution_results: Dict[str, Any],
                                  analysis_insights: Dict[str, Any],
                                  visualizations: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒì„¸ ë¶„ì„ ìƒì„±"""
        try:
            detailed_analysis = {
                'statistical_analysis': {},
                'data_characteristics': {},
                'assumption_analysis': {},
                'effect_analysis': {},
                'visualization_analysis': {},
                'methodological_notes': {}
            }
            
            statistics = execution_results.get('statistics', {})
            
            # 1. í†µê³„ ë¶„ì„ ì„¸ë¶€ì‚¬í•­
            detailed_analysis['statistical_analysis'] = self._analyze_statistical_details(statistics)
            
            # 2. ë°ì´í„° íŠ¹ì„± ë¶„ì„
            detailed_analysis['data_characteristics'] = self._analyze_data_characteristics(statistics)
            
            # 3. ê°€ì • ë¶„ì„
            detailed_analysis['assumption_analysis'] = self._analyze_assumptions(statistics)
            
            # 4. íš¨ê³¼ ë¶„ì„
            detailed_analysis['effect_analysis'] = self._analyze_effects(statistics)
            
            # 5. ì‹œê°í™” ë¶„ì„
            detailed_analysis['visualization_analysis'] = self._analyze_visualizations(visualizations)
            
            # 6. ë°©ë²•ë¡ ì  ë…¸íŠ¸
            detailed_analysis['methodological_notes'] = self._generate_methodological_notes(
                execution_results, analysis_insights
            )
            
            return detailed_analysis
            
        except Exception as e:
            self.logger.error(f"ìƒì„¸ ë¶„ì„ ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': str(e),
                'statistical_analysis': {},
                'data_characteristics': {},
                'assumption_analysis': {},
                'effect_analysis': {},
                'visualization_analysis': {},
                'methodological_notes': {}
            }
    
    def _generate_recommendations(self, detailed_analysis: Dict[str, Any],
                                analysis_insights: Dict[str, Any]) -> List[str]:
        """ì¶”ì²œì‚¬í•­ ë„ì¶œ"""
        try:
            recommendations = []
            
            # 1. í†µê³„ì  ê¶Œê³ ì‚¬í•­
            statistical_recs = self._generate_statistical_recommendations(detailed_analysis)
            recommendations.extend(statistical_recs)
            
            # 2. ë°©ë²•ë¡ ì  ê¶Œê³ ì‚¬í•­
            methodological_recs = self._generate_methodological_recommendations(detailed_analysis)
            recommendations.extend(methodological_recs)
            
            # 3. ë°ì´í„° í’ˆì§ˆ ê¶Œê³ ì‚¬í•­
            data_quality_recs = self._generate_data_quality_recommendations(detailed_analysis)
            recommendations.extend(data_quality_recs)
            
            # 4. í›„ì† ë¶„ì„ ê¶Œê³ ì‚¬í•­
            follow_up_recs = self._generate_follow_up_recommendations(detailed_analysis)
            recommendations.extend(follow_up_recs)
            
            # 5. ì‹¤ë¬´ì  ê¶Œê³ ì‚¬í•­
            practical_recs = self._generate_practical_recommendations(
                detailed_analysis, analysis_insights
            )
            recommendations.extend(practical_recs)
            
            # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
            unique_recommendations = list(set(recommendations))
            prioritized_recommendations = self._prioritize_recommendations(unique_recommendations)
            
            return prioritized_recommendations
            
        except Exception as e:
            self.logger.error(f"ì¶”ì²œì‚¬í•­ ë„ì¶œ ì˜¤ë¥˜: {e}")
            return [
                "ë¶„ì„ ê²°ê³¼ë¥¼ ì‹ ì¤‘í•˜ê²Œ í•´ì„í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
                "ì¶”ê°€ì ì¸ ë°ì´í„° ê²€í† ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "ì „ë¬¸ê°€ì™€ì˜ ìƒë‹´ì„ ê³ ë ¤í•´ë³´ì„¸ìš”."
            ]
    
    def _calculate_reliability_scores(self, execution_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            reliability_scores = {
                'data_quality_score': 0.0,
                'statistical_validity_score': 0.0,
                'assumption_compliance_score': 0.0,
                'effect_reliability_score': 0.0,
                'overall_reliability_score': 0.0
            }
            
            statistics = execution_results.get('statistics', {})
            
            # 1. ë°ì´í„° í’ˆì§ˆ ì ìˆ˜
            reliability_scores['data_quality_score'] = self._calculate_data_quality_score(statistics)
            
            # 2. í†µê³„ì  íƒ€ë‹¹ì„± ì ìˆ˜
            reliability_scores['statistical_validity_score'] = self._calculate_statistical_validity_score(statistics)
            
            # 3. ê°€ì • ì¤€ìˆ˜ ì ìˆ˜
            reliability_scores['assumption_compliance_score'] = self._calculate_assumption_compliance_score(statistics)
            
            # 4. íš¨ê³¼ ì‹ ë¢°ë„ ì ìˆ˜
            reliability_scores['effect_reliability_score'] = self._calculate_effect_reliability_score(statistics)
            
            # 5. ì „ì²´ ì‹ ë¢°ë„ ì ìˆ˜ (ê°€ì¤‘í‰ê· )
            weights = {
                'data_quality_score': 0.3,
                'statistical_validity_score': 0.3,
                'assumption_compliance_score': 0.2,
                'effect_reliability_score': 0.2
            }
            
            overall_score = sum(
                reliability_scores[key] * weight 
                for key, weight in weights.items()
            )
            reliability_scores['overall_reliability_score'] = overall_score
            
            # ì‹ ë¢°ë„ ë“±ê¸‰ ë¶€ì—¬
            reliability_scores['reliability_grade'] = self._assign_reliability_grade(overall_score)
            
            # ìƒì„¸ ë¶„ì„
            reliability_scores['detailed_analysis'] = {
                'strengths': self._identify_reliability_strengths(reliability_scores),
                'weaknesses': self._identify_reliability_weaknesses(reliability_scores),
                'improvement_suggestions': self._suggest_reliability_improvements(reliability_scores)
            }
            
            return reliability_scores
            
        except Exception as e:
            self.logger.error(f"ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {
                'data_quality_score': 0.0,
                'statistical_validity_score': 0.0,
                'assumption_compliance_score': 0.0,
                'effect_reliability_score': 0.0,
                'overall_reliability_score': 0.0,
                'reliability_grade': 'F',
                'error': str(e)
            }
    
    def _calculate_validation_metrics(self, execution_results: Dict[str, Any],
                                    statistical_design: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ì¦ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            validation_metrics = {
                'power_analysis': {},
                'sensitivity_analysis': {},
                'robustness_analysis': {},
                'cross_validation': {},
                'bootstrap_analysis': {}
            }
            
            statistics = execution_results.get('statistics', {})
            test_results = statistics.get('test_results', {})
            
            # 1. ê²€ì •ë ¥ ë¶„ì„
            validation_metrics['power_analysis'] = self._perform_power_analysis(
                test_results, statistical_design
            )
            
            # 2. ë¯¼ê°ë„ ë¶„ì„
            validation_metrics['sensitivity_analysis'] = self._perform_sensitivity_analysis(
                statistics, statistical_design
            )
            
            # 3. ê°•ê±´ì„± ë¶„ì„
            validation_metrics['robustness_analysis'] = self._perform_robustness_analysis(
                statistics, statistical_design
            )
            
            # 4. êµì°¨ ê²€ì¦ (í•´ë‹¹í•˜ëŠ” ê²½ìš°)
            if self._is_cross_validation_applicable(statistical_design):
                validation_metrics['cross_validation'] = self._perform_cross_validation(
                    statistics, statistical_design
                )
            
            # 5. ë¶€íŠ¸ìŠ¤íŠ¸ë© ë¶„ì„
            validation_metrics['bootstrap_analysis'] = self._perform_bootstrap_analysis(
                test_results, statistical_design
            )
            
            # ì „ì²´ ê²€ì¦ ì ìˆ˜
            validation_metrics['overall_validation_score'] = self._calculate_overall_validation_score(
                validation_metrics
            )
            
            return validation_metrics
            
        except Exception as e:
            self.logger.error(f"ê²€ì¦ ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {
                'power_analysis': {'power': 0.0, 'message': 'ê³„ì‚° ì‹¤íŒ¨'},
                'sensitivity_analysis': {'stable': False, 'message': 'ê³„ì‚° ì‹¤íŒ¨'},
                'robustness_analysis': {'robust': False, 'message': 'ê³„ì‚° ì‹¤íŒ¨'},
                'overall_validation_score': 0.0,
                'error': str(e)
            }
    
    def _calculate_confidence_intervals(self, execution_results: Dict[str, Any],
                                     statistical_design: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹ ë¢°êµ¬ê°„ ê³„ì‚°"""
        try:
            confidence_intervals = {}
            
            statistics = execution_results.get('statistics', {})
            test_results = statistics.get('test_results', {})
            
            # ê¸°ë³¸ ì‹ ë¢°ìˆ˜ì¤€ë“¤
            confidence_levels = [0.90, 0.95, 0.99]
            
            # 1. ëª¨ìˆ˜ ì‹ ë¢°êµ¬ê°„
            if 'parameter_estimates' in test_results:
                confidence_intervals['parameter_intervals'] = {}
                for level in confidence_levels:
                    confidence_intervals['parameter_intervals'][f'{int(level*100)}%'] = \
                        self._calculate_parameter_confidence_intervals(test_results, level)
            
            # 2. ì˜ˆì¸¡ ì‹ ë¢°êµ¬ê°„ (íšŒê·€ë¶„ì„ì¸ ê²½ìš°)
            if self._is_regression_analysis(statistical_design):
                confidence_intervals['prediction_intervals'] = {}
                for level in confidence_levels:
                    confidence_intervals['prediction_intervals'][f'{int(level*100)}%'] = \
                        self._calculate_prediction_intervals(test_results, level)
            
            # 3. íš¨ê³¼í¬ê¸° ì‹ ë¢°êµ¬ê°„
            if 'effect_size' in test_results:
                confidence_intervals['effect_size_intervals'] = {}
                for level in confidence_levels:
                    confidence_intervals['effect_size_intervals'][f'{int(level*100)}%'] = \
                        self._calculate_effect_size_confidence_intervals(test_results, level)
            
            # 4. ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„
            confidence_intervals['bootstrap_intervals'] = self._calculate_bootstrap_confidence_intervals(
                test_results, confidence_levels
            )
            
            # 5. ë² ì´ì§€ì•ˆ ì‹ ìš©êµ¬ê°„ (í•´ë‹¹í•˜ëŠ” ê²½ìš°)
            if statistical_design.get('bayesian_analysis', False):
                confidence_intervals['credible_intervals'] = self._calculate_credible_intervals(
                    test_results, confidence_levels
                )
            
            # ì‹ ë¢°êµ¬ê°„ í•´ì„
            confidence_intervals['interpretation'] = self._interpret_confidence_intervals(
                confidence_intervals, statistical_design
            )
            
            return confidence_intervals
            
        except Exception as e:
            self.logger.error(f"ì‹ ë¢°êµ¬ê°„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {
                'parameter_intervals': {},
                'effect_size_intervals': {},
                'bootstrap_intervals': {},
                'interpretation': f"ì‹ ë¢°êµ¬ê°„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                'error': str(e)
            }
    
    def get_step_info(self) -> Dict[str, Any]:
        """ë‹¨ê³„ ì •ë³´ ë°˜í™˜ (ë¶€ëª¨ í´ë˜ìŠ¤ ë©”ì„œë“œ í™•ì¥)"""
        base_info = super().get_step_info()
        base_info.update({
            'description': 'Agentic LLMì˜ í†µê³„ ë¶„ì„ ì‹¤í–‰ ë° ê²°ê³¼ í•´ì„',
            'input_requirements': [
                'analysis_code', 'execution_plan', 'data_requirements',
                'statistical_design', 'visualization_plan', 'documentation'
            ],
            'output_provides': [
                'execution_results', 'analysis_insights', 'visualizations',
                'interpretation', 'quality_metrics', 'execution_metadata'
            ],
            'capabilities': [
                'í†µê³„ ë¶„ì„ ì‹¤í–‰', 'RAG ê¸°ë°˜ ê²°ê³¼ í•´ì„', 'ì‹œê°í™” ìƒì„±',
                'ê²°ê³¼ í•´ì„ ë° ìš”ì•½', 'í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°', 'ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘'
            ]
        })
        return base_info

    # í—¬í¼ ë©”ì„œë“œë“¤
    def _interpret_effect_size(self, effect_size: float, test_type: str) -> str:
        """íš¨ê³¼í¬ê¸° í•´ì„"""
        try:
            if test_type.lower() in ['t_test', 't-test']:
                # Cohen's d ê¸°ì¤€
                if abs(effect_size) < 0.2:
                    return "ì‘ì€ íš¨ê³¼"
                elif abs(effect_size) < 0.5:
                    return "ì¤‘ê°„ íš¨ê³¼"
                elif abs(effect_size) < 0.8:
                    return "í° íš¨ê³¼"
                else:
                    return "ë§¤ìš° í° íš¨ê³¼"
            elif 'correlation' in test_type.lower():
                # ìƒê´€ê³„ìˆ˜ ê¸°ì¤€
                if abs(effect_size) < 0.1:
                    return "ë¬´ì‹œí•  ë§Œí•œ ìƒê´€"
                elif abs(effect_size) < 0.3:
                    return "ì•½í•œ ìƒê´€"
                elif abs(effect_size) < 0.5:
                    return "ì¤‘ê°„ ìƒê´€"
                elif abs(effect_size) < 0.7:
                    return "ê°•í•œ ìƒê´€"
                else:
                    return "ë§¤ìš° ê°•í•œ ìƒê´€"
            elif 'regression' in test_type.lower():
                # RÂ² ê¸°ì¤€
                if effect_size < 0.02:
                    return "ì‘ì€ ì„¤ëª…ë ¥"
                elif effect_size < 0.13:
                    return "ì¤‘ê°„ ì„¤ëª…ë ¥"
                elif effect_size < 0.26:
                    return "í° ì„¤ëª…ë ¥"
                else:
                    return "ë§¤ìš° í° ì„¤ëª…ë ¥"
            else:
                # ì¼ë°˜ì ì¸ ê¸°ì¤€
                if abs(effect_size) < 0.1:
                    return "ì‘ì€ íš¨ê³¼"
                elif abs(effect_size) < 0.3:
                    return "ì¤‘ê°„ íš¨ê³¼"
                else:
                    return "í° íš¨ê³¼"
                    
        except Exception:
            return "í•´ì„ ë¶ˆê°€"
    
    def _assess_sample_adequacy(self, statistics: Dict[str, Any]) -> float:
        """ìƒ˜í”Œ ì ì ˆì„± í‰ê°€"""
        try:
            desc_stats = statistics.get('descriptive_stats', {})
            sample_size = desc_stats.get('count', 0)
            
            # ê¸°ë³¸ ìƒ˜í”Œ í¬ê¸° ê¸°ì¤€
            if sample_size >= 100:
                return 1.0  # ë§¤ìš° ì ì ˆ
            elif sample_size >= 50:
                return 0.8  # ì ì ˆ
            elif sample_size >= 30:
                return 0.6  # ë³´í†µ
            elif sample_size >= 10:
                return 0.4  # ë¶€ì¡±
            else:
                return 0.2  # ë§¤ìš° ë¶€ì¡±
                
        except Exception:
            return 0.0
    
    def _calculate_assumption_score(self, assumption_validity: Dict[str, Any]) -> float:
        """ê°€ì • ì ìˆ˜ ê³„ì‚°"""
        try:
            if not assumption_validity:
                return 0.5  # ê°€ì • ê²€ì¦ ì•ˆ ë¨
            
            passed_count = sum(1 for test in assumption_validity.values() if test.get('passed', False))
            total_count = len(assumption_validity)
            
            return passed_count / total_count if total_count > 0 else 0.0
            
        except Exception:
            return 0.0
    
    def _calculate_overall_confidence(self, metrics: Dict[str, Any]) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            data_quality = metrics.get('data_quality', {}).get('completeness', 0.0)
            reliability = metrics.get('reliability', {})
            sample_adequacy = reliability.get('sample_adequacy', 0.0)
            assumption_score = reliability.get('assumption_score', 0.0)
            
            # ê°€ì¤‘í‰ê· 
            weights = [0.3, 0.4, 0.3]  # ë°ì´í„°í’ˆì§ˆ, ìƒ˜í”Œì ì ˆì„±, ê°€ì •ì ìˆ˜
            scores = [data_quality, sample_adequacy, assumption_score]
            
            return sum(w * s for w, s in zip(weights, scores)) / sum(weights)
            
        except Exception:
            return 0.0
    
    def _validate_data_completeness(self, statistics: Dict[str, Any], check: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ì™„ì„±ë„ ê²€ì¦"""
        try:
            desc_stats = statistics.get('descriptive_stats', {})
            completeness = desc_stats.get('completeness_ratio', 0.0)
            threshold = check.get('threshold', 0.95)
            
            passed = completeness >= threshold
            
            return {
                'passed': passed,
                'message': f"ë°ì´í„° ì™„ì„±ë„: {completeness:.2%} (ê¸°ì¤€: {threshold:.2%})",
                'details': {
                    'completeness_ratio': completeness,
                    'threshold': threshold,
                    'missing_count': desc_stats.get('missing_count', 0)
                }
            }
            
        except Exception as e:
            return {
                'passed': False,
                'message': f"ë°ì´í„° ì™„ì„±ë„ ê²€ì¦ ì˜¤ë¥˜: {str(e)}",
                'details': {'error': str(e)}
            }
    
    def _validate_outliers(self, statistics: Dict[str, Any], check: Dict[str, Any]) -> Dict[str, Any]:
        """ì´ìƒì¹˜ ê²€ì¦"""
        try:
            outliers = statistics.get('outliers', {})
            outlier_count = outliers.get('count', 0)
            total_count = statistics.get('descriptive_stats', {}).get('count', 1)
            outlier_ratio = outlier_count / total_count
            
            threshold = check.get('threshold', 0.05)  # 5% ê¸°ì¤€
            passed = outlier_ratio <= threshold
            
            return {
                'passed': passed,
                'message': f"ì´ìƒì¹˜ ë¹„ìœ¨: {outlier_ratio:.2%} (ê¸°ì¤€: {threshold:.2%})",
                'details': {
                    'outlier_count': outlier_count,
                    'total_count': total_count,
                    'outlier_ratio': outlier_ratio,
                    'outlier_indices': outliers.get('indices', [])
                }
            }
            
        except Exception as e:
            return {
                'passed': False,
                'message': f"ì´ìƒì¹˜ ê²€ì¦ ì˜¤ë¥˜: {str(e)}",
                'details': {'error': str(e)}
            }
    
    def _validate_normality(self, statistics: Dict[str, Any], check: Dict[str, Any]) -> Dict[str, Any]:
        """ì •ê·œì„± ê²€ì¦"""
        try:
            assumption_tests = statistics.get('assumption_tests', {})
            normality_tests = ['shapiro_wilk', 'kolmogorov_smirnov', 'anderson_darling']
            
            for test_name in normality_tests:
                if test_name in assumption_tests:
                    test_result = assumption_tests[test_name]
                    p_value = test_result.get('p_value', 0.0)
                    alpha = check.get('alpha', 0.05)
                    
                    passed = p_value > alpha  # ê·€ë¬´ê°€ì„¤: ì •ê·œë¶„í¬
                    
                    return {
                        'passed': passed,
                        'message': f"{test_name} ê²€ì •: p-value = {p_value:.4f} (Î± = {alpha})",
                        'details': {
                            'test_name': test_name,
                            'p_value': p_value,
                            'statistic': test_result.get('statistic', 0.0),
                            'alpha': alpha
                        }
                    }
            
            return {
                'passed': False,
                'message': "ì •ê·œì„± ê²€ì • ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                'details': {'available_tests': list(assumption_tests.keys())}
            }
            
        except Exception as e:
            return {
                'passed': False,
                'message': f"ì •ê·œì„± ê²€ì¦ ì˜¤ë¥˜: {str(e)}",
                'details': {'error': str(e)}
            }
    
    def _validate_independence(self, statistics: Dict[str, Any], check: Dict[str, Any]) -> Dict[str, Any]:
        """ë…ë¦½ì„± ê²€ì¦"""
        try:
            assumption_tests = statistics.get('assumption_tests', {})
            
            if 'durbin_watson' in assumption_tests:
                dw_result = assumption_tests['durbin_watson']
                dw_statistic = dw_result.get('statistic', 2.0)
                
                # Durbin-Watson í†µê³„ëŸ‰ í•´ì„ (1.5-2.5ê°€ ì ì ˆ)
                passed = 1.5 <= dw_statistic <= 2.5
                
                return {
                    'passed': passed,
                    'message': f"Durbin-Watson ê²€ì •: {dw_statistic:.3f} (ê¸°ì¤€: 1.5-2.5)",
                    'details': {
                        'durbin_watson_statistic': dw_statistic,
                        'interpretation': self._interpret_durbin_watson(dw_statistic)
                    }
                }
            
            # ë…ë¦½ì„± ê²€ì •ì´ ì—†ëŠ” ê²½ìš° ì¼ë°˜ì ì¸ ê°€ì •
            return {
                'passed': True,  # ê¸°ë³¸ì ìœ¼ë¡œ ë…ë¦½ì„± ê°€ì •
                'message': "ë…ë¦½ì„± ê°€ì • (ë³„ë„ ê²€ì • ì—†ìŒ)",
                'details': {'assumption': 'independence_assumed'}
            }
            
        except Exception as e:
            return {
                'passed': False,
                'message': f"ë…ë¦½ì„± ê²€ì¦ ì˜¤ë¥˜: {str(e)}",
                'details': {'error': str(e)}
            }
    
    def _validate_homoscedasticity(self, statistics: Dict[str, Any], check: Dict[str, Any]) -> Dict[str, Any]:
        """ë“±ë¶„ì‚°ì„± ê²€ì¦"""
        try:
            assumption_tests = statistics.get('assumption_tests', {})
            homoscedasticity_tests = ['levene', 'bartlett', 'breusch_pagan']
            
            for test_name in homoscedasticity_tests:
                if test_name in assumption_tests:
                    test_result = assumption_tests[test_name]
                    p_value = test_result.get('p_value', 0.0)
                    alpha = check.get('alpha', 0.05)
                    
                    passed = p_value > alpha  # ê·€ë¬´ê°€ì„¤: ë“±ë¶„ì‚°ì„±
                    
                    return {
                        'passed': passed,
                        'message': f"{test_name} ê²€ì •: p-value = {p_value:.4f} (Î± = {alpha})",
                        'details': {
                            'test_name': test_name,
                            'p_value': p_value,
                            'statistic': test_result.get('statistic', 0.0),
                            'alpha': alpha
                        }
                    }
            
            return {
                'passed': False,
                'message': "ë“±ë¶„ì‚°ì„± ê²€ì • ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                'details': {'available_tests': list(assumption_tests.keys())}
            }
            
        except Exception as e:
            return {
                'passed': False,
                'message': f"ë“±ë¶„ì‚°ì„± ê²€ì¦ ì˜¤ë¥˜: {str(e)}",
                'details': {'error': str(e)}
            }
    
    def _validate_linearity(self, statistics: Dict[str, Any], check: Dict[str, Any]) -> Dict[str, Any]:
        """ì„ í˜•ì„± ê²€ì¦"""
        try:
            assumption_tests = statistics.get('assumption_tests', {})
            
            if 'linearity_test' in assumption_tests:
                linearity_result = assumption_tests['linearity_test']
                p_value = linearity_result.get('p_value', 0.0)
                alpha = check.get('alpha', 0.05)
                
                passed = p_value > alpha
                
                return {
                    'passed': passed,
                    'message': f"ì„ í˜•ì„± ê²€ì •: p-value = {p_value:.4f} (Î± = {alpha})",
                    'details': {
                        'p_value': p_value,
                        'statistic': linearity_result.get('statistic', 0.0),
                        'alpha': alpha
                    }
                }
            
            # RÂ² ê¸°ë°˜ ì„ í˜•ì„± í‰ê°€
            model_summary = statistics.get('model_summary', {})
            r_squared = model_summary.get('r_squared', 0.0)
            
            threshold = check.get('r_squared_threshold', 0.1)
            passed = r_squared >= threshold
            
            return {
                'passed': passed,
                'message': f"ëª¨ë¸ ì„¤ëª…ë ¥ ê¸°ë°˜ ì„ í˜•ì„±: RÂ² = {r_squared:.3f} (ê¸°ì¤€: {threshold})",
                'details': {
                    'r_squared': r_squared,
                    'threshold': threshold,
                    'method': 'r_squared_based'
                }
            }
            
        except Exception as e:
            return {
                'passed': False,
                'message': f"ì„ í˜•ì„± ê²€ì¦ ì˜¤ë¥˜: {str(e)}",
                'details': {'error': str(e)}
            }
    
    def _validate_multicollinearity(self, statistics: Dict[str, Any], check: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¤ì¤‘ê³µì„ ì„± ê²€ì¦"""
        try:
            model_diagnostics = statistics.get('model_diagnostics', {})
            
            if 'vif_values' in model_diagnostics:
                vif_values = model_diagnostics['vif_values']
                threshold = check.get('vif_threshold', 10.0)
                
                max_vif = max(vif_values.values()) if vif_values else 0.0
                passed = max_vif < threshold
                
                return {
                    'passed': passed,
                    'message': f"VIF ê²€ì •: ìµœëŒ€ VIF = {max_vif:.2f} (ê¸°ì¤€: < {threshold})",
                    'details': {
                        'vif_values': vif_values,
                        'max_vif': max_vif,
                        'threshold': threshold
                    }
                }
            
            return {
                'passed': True,  # VIF ê°’ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ í†µê³¼
                'message': "ë‹¤ì¤‘ê³µì„ ì„± ê²€ì • ê²°ê³¼ ì—†ìŒ (ë‹¨ìˆœ ëª¨ë¸ë¡œ ì¶”ì •)",
                'details': {'method': 'not_applicable'}
            }
            
        except Exception as e:
            return {
                'passed': False,
                'message': f"ë‹¤ì¤‘ê³µì„ ì„± ê²€ì¦ ì˜¤ë¥˜: {str(e)}",
                'details': {'error': str(e)}
            }
    
    def _validate_sample_size(self, statistics: Dict[str, Any], check: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒ˜í”Œ í¬ê¸° ê²€ì¦"""
        try:
            desc_stats = statistics.get('descriptive_stats', {})
            sample_size = desc_stats.get('count', 0)
            
            min_sample_size = check.get('min_sample_size', 30)
            passed = sample_size >= min_sample_size
            
            return {
                'passed': passed,
                'message': f"ìƒ˜í”Œ í¬ê¸°: {sample_size} (ìµœì†Œ ê¸°ì¤€: {min_sample_size})",
                'details': {
                    'sample_size': sample_size,
                    'min_required': min_sample_size,
                    'adequacy': self._assess_sample_adequacy(statistics)
                }
            }
            
        except Exception as e:
            return {
                'passed': False,
                'message': f"ìƒ˜í”Œ í¬ê¸° ê²€ì¦ ì˜¤ë¥˜: {str(e)}",
                'details': {'error': str(e)}
            }
    
    def _interpret_durbin_watson(self, dw_statistic: float) -> str:
        """Durbin-Watson í†µê³„ëŸ‰ í•´ì„"""
        if dw_statistic < 1.5:
            return "ì–‘ì˜ ìê¸°ìƒê´€ ì˜ì‹¬"
        elif dw_statistic > 2.5:
            return "ìŒì˜ ìê¸°ìƒê´€ ì˜ì‹¬"
        else:
            return "ìê¸°ìƒê´€ ì—†ìŒ (ë…ë¦½ì„± ë§Œì¡±)"

    def _fallback_interpretation_parsing(self, text: str) -> Dict[str, Any]:
        """JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í•´ì„ ê²°ê³¼ ê¸°ë³¸ íŒŒì‹±"""
        try:
            # ê¸°ë³¸ êµ¬ì¡° ìƒì„±
            interpretation = {
                'statistical_significance': '',
                'practical_significance': '',
                'reliability_assessment': '',
                'limitations': [],
                'follow_up_suggestions': [],
                'key_insights': [],
                'conclusion': ''
            }
            
            # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            sentences = re.split(r'[.!?]', text)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
            for sentence in sentences:
                sentence_lower = sentence.lower()
                
                # í†µê³„ì  ìœ ì˜ì„± ê´€ë ¨
                if any(keyword in sentence_lower for keyword in ['p-value', 'pê°’', 'ìœ ì˜ìˆ˜ì¤€', 'í†µê³„ì ', 'significant']):
                    if not interpretation['statistical_significance']:
                        interpretation['statistical_significance'] = sentence
                
                # ì‹¤ë¬´ì  ìœ ì˜ì„± ê´€ë ¨
                elif any(keyword in sentence_lower for keyword in ['ì‹¤ë¬´', 'ì‹¤ìš©', 'íš¨ê³¼í¬ê¸°', 'effect size', 'ì‹¤ì œ']):
                    if not interpretation['practical_significance']:
                        interpretation['practical_significance'] = sentence
                
                # í•œê³„ì  ê´€ë ¨
                elif any(keyword in sentence_lower for keyword in ['í•œê³„', 'ì œí•œ', 'limitation', 'ì£¼ì˜']):
                    interpretation['limitations'].append(sentence)
                
                # ì œì•ˆì‚¬í•­ ê´€ë ¨
                elif any(keyword in sentence_lower for keyword in ['ì œì•ˆ', 'ê¶Œê³ ', 'ì¶”ì²œ', 'recommend', 'í›„ì†']):
                    interpretation['follow_up_suggestions'].append(sentence)
                
                # í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ê´€ë ¨
                elif any(keyword in sentence_lower for keyword in ['ì¤‘ìš”', 'í•µì‹¬', 'ì£¼ëª©', 'ê²°ê³¼ì ']):
                    interpretation['key_insights'].append(sentence)
            
            # ê²°ë¡ ì€ ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ ì¤‘ì—ì„œ
            if sentences:
                interpretation['conclusion'] = sentences[-1]
            
            # ë¹ˆ í•„ë“œë“¤ì— ëŒ€í•œ ê¸°ë³¸ê°’ ì„¤ì •
            if not interpretation['statistical_significance']:
                interpretation['statistical_significance'] = "í†µê³„ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
            
            if not interpretation['practical_significance']:
                interpretation['practical_significance'] = "íš¨ê³¼í¬ê¸°ì™€ ì‹¤ë¬´ì  ì¤‘ìš”ì„±ì„ ê³ ë ¤í•˜ì„¸ìš”."
            
            if not interpretation['reliability_assessment']:
                interpretation['reliability_assessment'] = "ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì„ ê²€í† í•˜ì„¸ìš”."
            
            if not interpretation['limitations']:
                interpretation['limitations'] = ["ë¶„ì„ ê²°ê³¼ í•´ì„ì‹œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."]
            
            if not interpretation['follow_up_suggestions']:
                interpretation['follow_up_suggestions'] = ["ì¶”ê°€ ë¶„ì„ì´ë‚˜ ì „ë¬¸ê°€ ìƒë‹´ì„ ê³ ë ¤í•˜ì„¸ìš”."]
            
            if not interpretation['conclusion']:
                interpretation['conclusion'] = "ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ì¢…í•©ì ì¸ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            
            return interpretation
            
        except Exception as e:
            self.logger.error(f"Fallback í•´ì„ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {
                'statistical_significance': 'íŒŒì‹± ì˜¤ë¥˜ë¡œ ì¸í•´ í•´ì„ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'practical_significance': 'ì „ë¬¸ê°€ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤.',
                'reliability_assessment': 'ê²°ê³¼ ì‹ ë¢°ì„± ê²€í†  í•„ìš”',
                'limitations': ['ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ'],
                'follow_up_suggestions': ['ì „ë¬¸ê°€ ìƒë‹´ ê¶Œì¥'],
                'key_insights': ['íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ'],
                'conclusion': 'ê²°ê³¼ í•´ì„ì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.',
                'error': str(e)
            }


# ë‹¨ê³„ ë“±ë¡
PipelineStepRegistry.register_step(7, AgentExecutionStep) 