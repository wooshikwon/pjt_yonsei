"""
Analysis Proposal Pipeline

4ë‹¨ê³„: Agentic LLMì˜ ë¶„ì„ ì „ëµ ì œì•ˆ
ì‚¬ìš©ìì˜ ìš”ì²­, ë°ì´í„° íŠ¹ì„±, RAGë¥¼ í†µí•´ í™•ë³´í•œ ë„ë©”ì¸ ì§€ì‹ ë° í†µê³„ì  ì§€ì‹ì„ ì¢…í•©í•˜ì—¬ 
ê°€ëŠ¥í•œ ë¶„ì„ ë°©ë²•ë“¤ê³¼ ê° ë°©ë²•ì˜ ì¥ë‹¨ì ì„ ì œì‹œí•©ë‹ˆë‹¤.
"""

import logging
from typing import Dict, Any, List, Optional
from pathlib import Path

from .base_pipeline_step import BasePipelineStep, PipelineStepRegistry
from core.rag.rag_manager import RAGManager
from services.llm.llm_client import LLMClient
from services.llm.prompt_engine import PromptEngine


class AnalysisProposalStep(BasePipelineStep):
    """4ë‹¨ê³„: Agentic LLMì˜ ë¶„ì„ ì „ëµ ì œì•ˆ"""
    
    def __init__(self):
        """AnalysisProposalStep ì´ˆê¸°í™”"""
        super().__init__("Agentic LLMì˜ ë¶„ì„ ì „ëµ ì œì•ˆ", 4)
        self.rag_manager = RAGManager()
        self.llm_client = LLMClient()
        self.prompt_engine = PromptEngine()
        
    def validate_input(self, input_data: Dict[str, Any]) -> bool:
        """
        ì…ë ¥ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
        
        Args:
            input_data: 3ë‹¨ê³„ì—ì„œ ì „ë‹¬ë°›ì€ ë°ì´í„°
            
        Returns:
            bool: ìœ íš¨ì„± ê²€ì¦ ê²°ê³¼
        """
        required_fields = [
            'data_overview', 'descriptive_statistics', 'data_quality_assessment',
            'variable_analysis', 'analysis_recommendations', 'summary_insights',
            'data_object'
        ]
        return all(field in input_data for field in required_fields)
    
    def get_expected_output_schema(self) -> Dict[str, Any]:
        """
        ì˜ˆìƒ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: ì¶œë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ
        """
        return {
            'analysis_proposals': {
                'recommended_methods': list,
                'alternative_methods': list,
                'method_details': dict,
                'rationale': dict
            },
            'statistical_context': {
                'assumptions': list,
                'limitations': list,
                'considerations': list
            },
            'domain_insights': {
                'business_context': dict,
                'similar_cases': list,
                'domain_specific_considerations': list
            },
            'execution_plan': {
                'steps': list,
                'required_validations': list,
                'potential_adjustments': list
            },
            'visualization_suggestions': {
                'pre_analysis': list,
                'during_analysis': list,
                'post_analysis': list
            }
        }
    
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Agentic LLMì˜ ë¶„ì„ ì „ëµ ì œì•ˆ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            input_data: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸
                - data_overview: ë°ì´í„° ê°œìš”
                - descriptive_statistics: ê¸°ìˆ  í†µê³„
                - data_quality_assessment: ë°ì´í„° í’ˆì§ˆ í‰ê°€
                - variable_analysis: ë³€ìˆ˜ ë¶„ì„
                - analysis_recommendations: ë¶„ì„ ì¶”ì²œì‚¬í•­
                - summary_insights: ìš”ì•½ ì¸ì‚¬ì´íŠ¸
                - data_object: ë°ì´í„° ê°ì²´
            
        Returns:
            Dict: ì‹¤í–‰ ê²°ê³¼
        """
        self.logger.info("4ë‹¨ê³„: Agentic LLMì˜ ë¶„ì„ ì „ëµ ì œì•ˆ ì‹œì‘")
        
        try:
            # 1. RAGë¥¼ í†µí•œ ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰
            rag_context = self._retrieve_relevant_knowledge(input_data)
            
            # 2. í†µê³„ì  ë¶„ì„ ë°©ë²• ì œì•ˆ
            analysis_proposals = self._generate_analysis_proposals(input_data, rag_context)
            
            # 3. í†µê³„ì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            statistical_context = self._build_statistical_context(input_data, rag_context)
            
            # 4. ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ ìƒì„±
            domain_insights = self._generate_domain_insights(input_data, rag_context)
            
            # 5. ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
            execution_plan = self._create_execution_plan(
                analysis_proposals, statistical_context, domain_insights
            )
            
            # 6. ì‹œê°í™” ì œì•ˆ
            visualization_suggestions = self._suggest_visualizations(
                input_data, analysis_proposals
            )
            
            self.logger.info("ë¶„ì„ ì „ëµ ì œì•ˆ ì™„ë£Œ")
            
            return {
                'analysis_proposals': analysis_proposals,
                'statistical_context': statistical_context,
                'domain_insights': domain_insights,
                'execution_plan': execution_plan,
                'visualization_suggestions': visualization_suggestions,
                'success_message': "ğŸ“Š ë¶„ì„ ì „ëµ ì œì•ˆì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            }
                
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ì „ëµ ì œì•ˆ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': str(e),
                'error_type': 'proposal_error'
            }
    
    def _retrieve_relevant_knowledge(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """RAGë¥¼ í†µí•œ ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰"""
        try:
            # 1. í†µê³„ ë°©ë²•ë¡  ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰
            statistical_knowledge = self.rag_manager.search(
                collection="statistical_concepts",
                query=self._build_statistical_query(input_data),
                top_k=5
            )
            
            # 2. ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸ ì§€ì‹ ê²€ìƒ‰
            domain_knowledge = self.rag_manager.search(
                collection="business_domains",
                query=self._build_domain_query(input_data),
                top_k=3
            )
            
            # 3. ì½”ë“œ í…œí”Œë¦¿ ê²€ìƒ‰
            code_templates = self.rag_manager.search(
                collection="code_templates",
                query=self._build_code_query(input_data),
                top_k=3
            )
            
            # 4. ì»¨í…ìŠ¤íŠ¸ í†µí•©
            integrated_context = self.rag_manager.build_context(
                statistical_knowledge=statistical_knowledge,
                domain_knowledge=domain_knowledge,
                code_templates=code_templates,
                analysis_context=input_data
            )
            
            return integrated_context
            
        except Exception as e:
            self.logger.error(f"RAG ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': str(e)
            }
    
    def _build_statistical_query(self, input_data: Dict[str, Any]) -> str:
        """í†µê³„ ë°©ë²•ë¡  ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±"""
        analysis_type = input_data.get('analysis_recommendations', {}).get('suitable_analyses', [])
        data_characteristics = input_data.get('summary_insights', {}).get('data_characteristics', [])
        
        query = f"""
        í†µê³„ ë¶„ì„ ë°©ë²•: {', '.join(analysis_type)}
        ë°ì´í„° íŠ¹ì„±: {', '.join(data_characteristics)}
        """
        return query
    
    def _build_domain_query(self, input_data: Dict[str, Any]) -> str:
        """ë„ë©”ì¸ ì§€ì‹ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±"""
        # ì‚¬ìš©ì ìš”ì²­ì—ì„œ ë„ë©”ì¸ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ
        domain_context = input_data.get('user_request', '')
        variables = input_data.get('variable_analysis', {})
        
        query = f"""
        ë¶„ì„ ì»¨í…ìŠ¤íŠ¸: {domain_context}
        ê´€ë ¨ ë³€ìˆ˜: {variables}
        """
        return query
    
    def _build_code_query(self, input_data: Dict[str, Any]) -> str:
        """ì½”ë“œ í…œí”Œë¦¿ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±"""
        analysis_type = input_data.get('analysis_recommendations', {}).get('suitable_analyses', [])
        return f"í†µê³„ ë¶„ì„ ì½”ë“œ í…œí”Œë¦¿: {', '.join(analysis_type)}"
    
    def _generate_analysis_proposals(self, input_data: Dict[str, Any], 
                                   rag_context: Dict[str, Any]) -> Dict[str, Any]:
        """í†µê³„ì  ë¶„ì„ ë°©ë²• ì œì•ˆ"""
        # LLMì— ë¶„ì„ ì œì•ˆ ìš”ì²­
        prompt = self.prompt_engine.create_analysis_proposal_prompt(
            input_data=input_data,
            rag_context=rag_context
        )
        
        llm_response = self.llm_client.generate(prompt)
        
        # LLM ì‘ë‹µ íŒŒì‹± ë° êµ¬ì¡°í™”
        proposals = self._parse_analysis_proposals(llm_response)
        
        return {
            'recommended_methods': proposals.get('recommended_methods', []),
            'alternative_methods': proposals.get('alternative_methods', []),
            'method_details': proposals.get('method_details', {}),
            'rationale': proposals.get('rationale', {})
        }
    
    def _build_statistical_context(self, input_data: Dict[str, Any], 
                                 rag_context: Dict[str, Any]) -> Dict[str, Any]:
        """í†µê³„ì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        # ë°ì´í„° íŠ¹ì„± ë° ì œì•½ì‚¬í•­ ë¶„ì„
        data_constraints = self._analyze_data_constraints(input_data)
        
        # í†µê³„ì  ê°€ì • ë° ìš”êµ¬ì‚¬í•­ ì‹ë³„
        statistical_requirements = self._identify_statistical_requirements(
            input_data, rag_context
        )
        
        return {
            'assumptions': statistical_requirements.get('assumptions', []),
            'limitations': data_constraints.get('limitations', []),
            'considerations': statistical_requirements.get('considerations', [])
        }
    
    def _generate_domain_insights(self, input_data: Dict[str, Any], 
                                rag_context: Dict[str, Any]) -> Dict[str, Any]:
        """ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        # LLMì— ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ ìš”ì²­
        prompt = self.prompt_engine.create_domain_insight_prompt(
            input_data=input_data,
            rag_context=rag_context
        )
        
        llm_response = self.llm_client.generate(prompt)
        
        # LLM ì‘ë‹µ íŒŒì‹± ë° êµ¬ì¡°í™”
        insights = self._parse_domain_insights(llm_response)
        
        return {
            'business_context': insights.get('business_context', {}),
            'similar_cases': insights.get('similar_cases', []),
            'domain_specific_considerations': insights.get('considerations', [])
        }
    
    def _create_execution_plan(self, analysis_proposals: Dict[str, Any],
                             statistical_context: Dict[str, Any],
                             domain_insights: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤í–‰ ê³„íš ìˆ˜ë¦½"""
        # ë¶„ì„ ë‹¨ê³„ ì •ì˜
        analysis_steps = self._define_analysis_steps(
            analysis_proposals, statistical_context
        )
        
        # í•„ìš”í•œ ê²€ì¦ ë‹¨ê³„ ì‹ë³„
        required_validations = self._identify_required_validations(
            analysis_proposals, statistical_context
        )
        
        # ì ì¬ì  ì¡°ì •ì‚¬í•­ ì‹ë³„
        potential_adjustments = self._identify_potential_adjustments(
            analysis_proposals, domain_insights
        )
        
        return {
            'steps': analysis_steps,
            'required_validations': required_validations,
            'potential_adjustments': potential_adjustments
        }
    
    def _suggest_visualizations(self, input_data: Dict[str, Any],
                              analysis_proposals: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹œê°í™” ì œì•ˆ"""
        # ë°ì´í„° íŠ¹ì„± ê¸°ë°˜ ì‹œê°í™” ì œì•ˆ
        pre_analysis_viz = self._suggest_pre_analysis_visualizations(input_data)
        
        # ë¶„ì„ ê³¼ì • ì‹œê°í™” ì œì•ˆ
        analysis_viz = self._suggest_analysis_visualizations(
            input_data, analysis_proposals
        )
        
        # ê²°ê³¼ ì‹œê°í™” ì œì•ˆ
        post_analysis_viz = self._suggest_post_analysis_visualizations(
            analysis_proposals
        )
        
        return {
            'pre_analysis': pre_analysis_viz,
            'during_analysis': analysis_viz,
            'post_analysis': post_analysis_viz
        }
    
    def _parse_analysis_proposals(self, llm_response: str) -> Dict[str, Any]:
        """LLM ì‘ë‹µì—ì„œ ë¶„ì„ ì œì•ˆ íŒŒì‹±"""
        try:
            # JSON í˜•íƒœì˜ ì‘ë‹µì´ í¬í•¨ëœ ê²½ìš° ì¶”ì¶œ
            import json
            import re
            
            # JSON ë¸”ë¡ ì°¾ê¸°
            json_match = re.search(r'```json\s*(.*?)\s*```', llm_response, re.DOTALL)
            if json_match:
                try:
                    parsed = json.loads(json_match.group(1))
                    return parsed
                except json.JSONDecodeError:
                    # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ êµ¬ì¡°ë¡œ í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„
                    self.logger.warning("JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ íŒŒì‹±ìœ¼ë¡œ ì „í™˜")
                    return self._fallback_text_parsing(llm_response)
            
            # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ íŒŒì‹±
            proposals = {
                'recommended_methods': [],
                'alternative_methods': [],
                'method_details': {},
                'rationale': {}
            }
            
            # ì¶”ì²œ ë°©ë²• ì¶”ì¶œ
            recommended_pattern = r'ì¶”ì²œ\s*ë°©ë²•[:\s]*(.+?)(?=ëŒ€ì•ˆ|ë°©ë²•|$)'
            recommended_match = re.search(recommended_pattern, llm_response, re.IGNORECASE | re.DOTALL)
            if recommended_match:
                methods_text = recommended_match.group(1)
                methods = re.findall(r'[-â€¢]\s*([^-â€¢\n]+)', methods_text)
                proposals['recommended_methods'] = [m.strip() for m in methods if m.strip()]
            
            # ëŒ€ì•ˆ ë°©ë²• ì¶”ì¶œ
            alternative_pattern = r'ëŒ€ì•ˆ\s*ë°©ë²•[:\s]*(.+?)(?=ê·¼ê±°|ì´ìœ |$)'
            alternative_match = re.search(alternative_pattern, llm_response, re.IGNORECASE | re.DOTALL)
            if alternative_match:
                alt_text = alternative_match.group(1)
                alt_methods = re.findall(r'[-â€¢]\s*([^-â€¢\n]+)', alt_text)
                proposals['alternative_methods'] = [m.strip() for m in alt_methods if m.strip()]
            
            # ê·¼ê±° ì¶”ì¶œ
            rationale_pattern = r'ê·¼ê±°[:\s]*(.+?)$'
            rationale_match = re.search(rationale_pattern, llm_response, re.IGNORECASE | re.DOTALL)
            if rationale_match:
                proposals['rationale']['general'] = rationale_match.group(1).strip()
            
            return proposals
            
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ì œì•ˆ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {
                'recommended_methods': ['ê¸°ìˆ í†µê³„ë¶„ì„'],
                'alternative_methods': [],
                'method_details': {},
                'rationale': {'general': 'ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ì‹œì‘'}
            }
    
    def _analyze_data_constraints(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ì œì•½ì‚¬í•­ ë¶„ì„"""
        try:
            constraints = {
                'limitations': [],
                'sample_size_issues': [],
                'data_quality_issues': [],
                'variable_constraints': []
            }
            
            # ë°ì´í„° ê°œìš”ì—ì„œ ì •ë³´ ì¶”ì¶œ
            data_overview = input_data.get('data_overview', {})
            quality_assessment = input_data.get('data_quality_assessment', {})
            
            # ìƒ˜í”Œ í¬ê¸° í™•ì¸
            sample_size = data_overview.get('shape', {}).get('rows', 0)
            if sample_size < 30:
                constraints['sample_size_issues'].append('ì†Œí‘œë³¸ìœ¼ë¡œ ì¸í•œ í†µê³„ì  ê²€ì •ë ¥ ë¶€ì¡±')
                constraints['limitations'].append('ë¹„ëª¨ìˆ˜ ê²€ì • ê³ ë ¤ í•„ìš”')
            elif sample_size < 100:
                constraints['sample_size_issues'].append('ì¤‘ê°„ ê·œëª¨ í‘œë³¸ìœ¼ë¡œ ì •ê·œì„± ê²€ì • ì£¼ì˜ í•„ìš”')
            
            # ê²°ì¸¡ê°’ í™•ì¸
            missing_data = quality_assessment.get('missing_data', {})
            if missing_data:
                for var, missing_info in missing_data.items():
                    missing_rate = missing_info.get('percentage', 0)
                    if missing_rate > 20:
                        constraints['data_quality_issues'].append(f'{var}: ë†’ì€ ê²°ì¸¡ë¥  ({missing_rate:.1f}%)')
                        constraints['limitations'].append('ê²°ì¸¡ê°’ ì²˜ë¦¬ ì „ëµ í•„ìš”')
                    elif missing_rate > 5:
                        constraints['data_quality_issues'].append(f'{var}: ì¤‘ê°„ ê²°ì¸¡ë¥  ({missing_rate:.1f}%)')
            
            # ì´ìƒê°’ í™•ì¸
            outliers = quality_assessment.get('outliers', {})
            if outliers:
                for var, outlier_info in outliers.items():
                    outlier_count = outlier_info.get('count', 0)
                    if outlier_count > 0:
                        constraints['data_quality_issues'].append(f'{var}: {outlier_count}ê°œ ì´ìƒê°’ ë°œê²¬')
                        constraints['limitations'].append('ì´ìƒê°’ ì²˜ë¦¬ ë°©ë²• ê²€í†  í•„ìš”')
            
            # ë³€ìˆ˜ ìœ í˜•ë³„ ì œì•½ì‚¬í•­
            variable_analysis = input_data.get('variable_analysis', {})
            for var_type, variables in variable_analysis.items():
                if var_type == 'categorical' and len(variables) > 0:
                    for var in variables:
                        if var.get('unique_values', 0) > 10:
                            constraints['variable_constraints'].append(f'{var["name"]}: ë²”ì£¼ê°€ ë§ìŒ (ì¬ì½”ë”© ê³ ë ¤)')
                elif var_type == 'numerical' and len(variables) > 0:
                    for var in variables:
                        skewness = var.get('skewness', 0)
                        if abs(skewness) > 2:
                            constraints['variable_constraints'].append(f'{var["name"]}: ì‹¬í•œ ë¹„ëŒ€ì¹­ì„± (ë³€í™˜ ê³ ë ¤)')
            
            return constraints
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ì œì•½ì‚¬í•­ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {'limitations': [], 'sample_size_issues': [], 'data_quality_issues': [], 'variable_constraints': []}
    
    def _identify_statistical_requirements(self, input_data: Dict[str, Any],
                                         rag_context: Dict[str, Any]) -> Dict[str, Any]:
        """í†µê³„ì  ìš”êµ¬ì‚¬í•­ ì‹ë³„"""
        try:
            requirements = {
                'assumptions': [],
                'considerations': [],
                'required_tests': []
            }
            
            # ë¶„ì„ ì¶”ì²œì‚¬í•­ì—ì„œ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            analysis_recs = input_data.get('analysis_recommendations', {})
            suitable_analyses = analysis_recs.get('suitable_analyses', [])
            
            for analysis in suitable_analyses:
                # t-ê²€ì • ê´€ë ¨ ìš”êµ¬ì‚¬í•­
                if 't-test' in analysis.lower() or 'tê²€ì •' in analysis:
                    requirements['assumptions'].extend([
                        'ì •ê·œì„± ê°€ì • í™•ì¸ í•„ìš”',
                        'ë…ë¦½ì„± ê°€ì • í™•ì¸ í•„ìš”'
                    ])
                    if 'ë…ë¦½' in analysis:
                        requirements['assumptions'].append('ë“±ë¶„ì‚°ì„± ê°€ì • í™•ì¸ í•„ìš”')
                    requirements['required_tests'].extend(['ì •ê·œì„± ê²€ì •', 'ë“±ë¶„ì‚°ì„± ê²€ì •'])
                
                # ANOVA ê´€ë ¨ ìš”êµ¬ì‚¬í•­
                elif 'anova' in analysis.lower() or 'ë¶„ì‚°ë¶„ì„' in analysis:
                    requirements['assumptions'].extend([
                        'ì •ê·œì„± ê°€ì • í™•ì¸ í•„ìš”',
                        'ë“±ë¶„ì‚°ì„± ê°€ì • í™•ì¸ í•„ìš”',
                        'ë…ë¦½ì„± ê°€ì • í™•ì¸ í•„ìš”'
                    ])
                    requirements['required_tests'].extend(['ì •ê·œì„± ê²€ì •', 'ë“±ë¶„ì‚°ì„± ê²€ì •'])
                    requirements['considerations'].append('ì‚¬í›„ê²€ì • ê³„íš í•„ìš”')
                
                # íšŒê·€ë¶„ì„ ê´€ë ¨ ìš”êµ¬ì‚¬í•­
                elif 'regression' in analysis.lower() or 'íšŒê·€' in analysis:
                    requirements['assumptions'].extend([
                        'ì„ í˜•ì„± ê°€ì • í™•ì¸ í•„ìš”',
                        'ì •ê·œì„± ê°€ì • í™•ì¸ í•„ìš”',
                        'ë“±ë¶„ì‚°ì„± ê°€ì • í™•ì¸ í•„ìš”',
                        'ë…ë¦½ì„± ê°€ì • í™•ì¸ í•„ìš”'
                    ])
                    requirements['required_tests'].extend(['ì„ í˜•ì„± ê²€ì •', 'ì •ê·œì„± ê²€ì •', 'ë“±ë¶„ì‚°ì„± ê²€ì •'])
                    requirements['considerations'].extend(['ë‹¤ì¤‘ê³µì„ ì„± ê²€í† ', 'ì”ì°¨ë¶„ì„ í•„ìš”'])
                
                # ë¹„ëª¨ìˆ˜ ê²€ì • ê´€ë ¨ ìš”êµ¬ì‚¬í•­
                elif any(nonparam in analysis.lower() for nonparam in ['mann-whitney', 'kruskal', 'wilcoxon']):
                    requirements['assumptions'].append('ë¶„í¬ì˜ ëª¨ì–‘ ìœ ì‚¬ì„± í™•ì¸ í•„ìš”')
                    requirements['considerations'].append('ëª¨ìˆ˜ ê²€ì • ëŒ€ë¹„ ê²€ì •ë ¥ ê³ ë ¤')
                
                # ë²”ì£¼í˜• ë¶„ì„ ê´€ë ¨ ìš”êµ¬ì‚¬í•­
                elif 'chi' in analysis.lower() or 'ì¹´ì´ì œê³±' in analysis:
                    requirements['assumptions'].extend([
                        'ê¸°ëŒ€ë¹ˆë„ 5 ì´ìƒ í™•ì¸ í•„ìš”',
                        'ë…ë¦½ì„± ê°€ì • í™•ì¸ í•„ìš”'
                    ])
                    requirements['considerations'].append('íš¨ê³¼í¬ê¸° ê³„ì‚° ê³ ë ¤')
            
            # RAG ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶”ê°€ ê³ ë ¤ì‚¬í•­ ì¶”ì¶œ
            statistical_context = rag_context.get('statistical_concepts', [])
            for concept in statistical_context:
                content = concept.get('content', '')
                if 'ê°€ì •' in content or 'assumption' in content.lower():
                    # í†µê³„ì  ê°€ì • ê´€ë ¨ ë‚´ìš© ì¶”ì¶œ
                    assumptions = re.findall(r'([^.!?]*ê°€ì •[^.!?]*)', content)
                    requirements['considerations'].extend([a.strip() for a in assumptions if a.strip()])
            
            # ì¤‘ë³µ ì œê±°
            requirements['assumptions'] = list(set(requirements['assumptions']))
            requirements['considerations'] = list(set(requirements['considerations']))
            requirements['required_tests'] = list(set(requirements['required_tests']))
            
            return requirements
            
        except Exception as e:
            self.logger.error(f"í†µê³„ì  ìš”êµ¬ì‚¬í•­ ì‹ë³„ ì˜¤ë¥˜: {e}")
            return {'assumptions': [], 'considerations': [], 'required_tests': []}
    
    def _parse_domain_insights(self, llm_response: str) -> Dict[str, Any]:
        """LLM ì‘ë‹µì—ì„œ ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ íŒŒì‹±"""
        try:
            import json
            import re
            
            insights = {
                'business_context': {},
                'similar_cases': [],
                'considerations': []
            }
            
            # JSON í˜•íƒœ ì‘ë‹µ ì‹œë„
            json_match = re.search(r'```json\s*(.*?)\s*```', llm_response, re.DOTALL)
            if json_match:
                try:
                    parsed = json.loads(json_match.group(1))
                    return parsed
                except json.JSONDecodeError:
                    # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ êµ¬ì¡°ë¡œ í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„
                    self.logger.warning("JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ íŒŒì‹±ìœ¼ë¡œ ì „í™˜")
                    return self._fallback_text_parsing(llm_response)
            
            # ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            business_pattern = r'ë¹„ì¦ˆë‹ˆìŠ¤\s*(?:ì»¨í…ìŠ¤íŠ¸|ë§¥ë½)[:\s]*(.+?)(?=ìœ ì‚¬|ê³ ë ¤|$)'
            business_match = re.search(business_pattern, llm_response, re.IGNORECASE | re.DOTALL)
            if business_match:
                business_text = business_match.group(1).strip()
                insights['business_context']['description'] = business_text
                
                # í•µì‹¬ ì§€í‘œ ì¶”ì¶œ
                kpi_pattern = r'(?:KPI|ì§€í‘œ|ì„±ê³¼)[:\s]*([^.\n]+)'
                kpi_matches = re.findall(kpi_pattern, business_text, re.IGNORECASE)
                if kpi_matches:
                    insights['business_context']['key_metrics'] = [kpi.strip() for kpi in kpi_matches]
            
            # ìœ ì‚¬ ì‚¬ë¡€ ì¶”ì¶œ
            similar_pattern = r'ìœ ì‚¬\s*(?:ì‚¬ë¡€|ê²½ìš°)[:\s]*(.+?)(?=ê³ ë ¤|ê¶Œê³ |$)'
            similar_match = re.search(similar_pattern, llm_response, re.IGNORECASE | re.DOTALL)
            if similar_match:
                similar_text = similar_match.group(1)
                cases = re.findall(r'[-â€¢]\s*([^-â€¢\n]+)', similar_text)
                insights['similar_cases'] = [case.strip() for case in cases if case.strip()]
            
            # ê³ ë ¤ì‚¬í•­ ì¶”ì¶œ
            consideration_pattern = r'ê³ ë ¤\s*(?:ì‚¬í•­|í• ì )[:\s]*(.+?)$'
            consideration_match = re.search(consideration_pattern, llm_response, re.IGNORECASE | re.DOTALL)
            if consideration_match:
                consideration_text = consideration_match.group(1)
                considerations = re.findall(r'[-â€¢]\s*([^-â€¢\n]+)', consideration_text)
                insights['considerations'] = [cons.strip() for cons in considerations if cons.strip()]
            
            return insights
            
        except Exception as e:
            self.logger.error(f"ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {'business_context': {}, 'similar_cases': [], 'considerations': []}
    
    def _define_analysis_steps(self, analysis_proposals: Dict[str, Any],
                             statistical_context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ë¶„ì„ ë‹¨ê³„ ì •ì˜"""
        try:
            steps = []
            
            # ê¸°ë³¸ ë°ì´í„° íƒìƒ‰ ë‹¨ê³„
            steps.append({
                'step_number': 1,
                'name': 'ë°ì´í„° íƒìƒ‰',
                'description': 'ê¸°ìˆ í†µê³„ ë° ì‹œê°í™”ë¥¼ í†µí•œ ë°ì´í„° ì´í•´',
                'tasks': ['ê¸°ìˆ í†µê³„ ê³„ì‚°', 'ë¶„í¬ í™•ì¸', 'ì´ìƒê°’ íƒì§€'],
                'estimated_time': '5-10ë¶„'
            })
            
            # ê°€ì • ê²€ì • ë‹¨ê³„
            required_tests = statistical_context.get('required_tests', [])
            if required_tests:
                steps.append({
                    'step_number': 2,
                    'name': 'í†µê³„ì  ê°€ì • ê²€ì •',
                    'description': 'ë¶„ì„ ì „ í•„ìš”í•œ ê°€ì •ë“¤ì„ ê²€ì¦',
                    'tasks': required_tests,
                    'estimated_time': '3-5ë¶„'
                })
            
            # ì£¼ ë¶„ì„ ë‹¨ê³„
            recommended_methods = analysis_proposals.get('recommended_methods', [])
            for i, method in enumerate(recommended_methods):
                steps.append({
                    'step_number': len(steps) + 1,
                    'name': f'ì£¼ ë¶„ì„ {i+1}: {method}',
                    'description': f'{method} ì‹¤í–‰ ë° ê²°ê³¼ í•´ì„',
                    'tasks': [f'{method} ì‹¤í–‰', 'ê²°ê³¼ í•´ì„', 'íš¨ê³¼í¬ê¸° ê³„ì‚°'],
                    'estimated_time': '10-15ë¶„'
                })
            
            # ëŒ€ì•ˆ ë¶„ì„ ë‹¨ê³„ (ì¡°ê±´ë¶€)
            alternative_methods = analysis_proposals.get('alternative_methods', [])
            if alternative_methods:
                steps.append({
                    'step_number': len(steps) + 1,
                    'name': 'ëŒ€ì•ˆ ë¶„ì„',
                    'description': 'ê°€ì • ìœ„ë°°ì‹œ ì‹¤í–‰í•  ëŒ€ì•ˆ ë¶„ì„',
                    'tasks': [f'{method} ì‹¤í–‰' for method in alternative_methods],
                    'estimated_time': '5-10ë¶„'
                })
            
            # ê²°ê³¼ ì¢…í•© ë‹¨ê³„
            steps.append({
                'step_number': len(steps) + 1,
                'name': 'ê²°ê³¼ ì¢…í•©',
                'description': 'ë¶„ì„ ê²°ê³¼ ì¢…í•© ë° í•´ì„',
                'tasks': ['ê²°ê³¼ ë¹„êµ', 'ìµœì¢… í•´ì„', 'ë³´ê³ ì„œ ì‘ì„±'],
                'estimated_time': '10-15ë¶„'
            })
            
            return steps
            
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ë‹¨ê³„ ì •ì˜ ì˜¤ë¥˜: {e}")
            return [{'step_number': 1, 'name': 'ê¸°ë³¸ ë¶„ì„', 'description': 'ê¸°ìˆ í†µê³„ ë¶„ì„', 'tasks': ['ê¸°ìˆ í†µê³„'], 'estimated_time': '5ë¶„'}]
    
    def _identify_required_validations(self, analysis_proposals: Dict[str, Any],
                                     statistical_context: Dict[str, Any]) -> List[str]:
        """í•„ìš”í•œ ê²€ì¦ ë‹¨ê³„ ì‹ë³„"""
        try:
            validations = []
            
            # í†µê³„ì  ê°€ì • ê²€ì¦
            assumptions = statistical_context.get('assumptions', [])
            for assumption in assumptions:
                if 'ì •ê·œì„±' in assumption:
                    validations.append('ì •ê·œì„± ê²€ì • (Shapiro-Wilk ë˜ëŠ” Kolmogorov-Smirnov)')
                elif 'ë“±ë¶„ì‚°ì„±' in assumption:
                    validations.append('ë“±ë¶„ì‚°ì„± ê²€ì • (Levene ë˜ëŠ” Bartlett)')
                elif 'ì„ í˜•ì„±' in assumption:
                    validations.append('ì„ í˜•ì„± ê²€ì • (ì‚°ì ë„ ë° ì”ì°¨ë¶„ì„)')
                elif 'ë…ë¦½ì„±' in assumption:
                    validations.append('ë…ë¦½ì„± ê²€ì • (Durbin-Watson ë˜ëŠ” ì‹œê°ì  í™•ì¸)')
            
            # ë¶„ì„ë³„ íŠ¹í™” ê²€ì¦
            recommended_methods = analysis_proposals.get('recommended_methods', [])
            for method in recommended_methods:
                if 'íšŒê·€' in method:
                    validations.extend([
                        'ë‹¤ì¤‘ê³µì„ ì„± ê²€ì • (VIF)',
                        'ì”ì°¨ì˜ ì •ê·œì„± ê²€ì •',
                        'ì˜í–¥ë ¥ ìˆëŠ” ê´€ì¸¡ê°’ íƒì§€'
                    ])
                elif 'anova' in method.lower() or 'ë¶„ì‚°ë¶„ì„' in method:
                    validations.append('ì§‘ë‹¨ í¬ê¸°ì˜ ê· í˜•ì„± í™•ì¸')
                elif 'ì¹´ì´ì œê³±' in method:
                    validations.append('ê¸°ëŒ€ë¹ˆë„ ì¡°ê±´ í™•ì¸ (ëª¨ë“  ì…€ â‰¥ 5)')
            
            # ì¤‘ë³µ ì œê±°
            validations = list(set(validations))
            
            return validations
            
        except Exception as e:
            self.logger.error(f"ê²€ì¦ ë‹¨ê³„ ì‹ë³„ ì˜¤ë¥˜: {e}")
            return ['ê¸°ë³¸ ë°ì´í„° ê²€ì¦']
    
    def _identify_potential_adjustments(self, analysis_proposals: Dict[str, Any],
                                      domain_insights: Dict[str, Any]) -> List[str]:
        """ì ì¬ì  ì¡°ì •ì‚¬í•­ ì‹ë³„"""
        try:
            adjustments = []
            
            # ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¡°ì •
            business_context = domain_insights.get('business_context', {})
            if business_context:
                key_metrics = business_context.get('key_metrics', [])
                if key_metrics:
                    adjustments.append('ë¹„ì¦ˆë‹ˆìŠ¤ í•µì‹¬ ì§€í‘œì— ë§ì¶˜ í•´ì„ ë°©í–¥ ì¡°ì •')
            
            # ë„ë©”ì¸ë³„ ê³ ë ¤ì‚¬í•­
            considerations = domain_insights.get('considerations', [])
            if considerations:
                adjustments.extend([
                    f'ë„ë©”ì¸ íŠ¹í™” ê³ ë ¤ì‚¬í•­ ë°˜ì˜: {cons}' 
                    for cons in considerations[:3]  # ìƒìœ„ 3ê°œë§Œ
                ])
            
            # ìœ ì‚¬ ì‚¬ë¡€ ê¸°ë°˜ ì¡°ì •
            similar_cases = domain_insights.get('similar_cases', [])
            if similar_cases:
                adjustments.append('ìœ ì‚¬ ì‚¬ë¡€ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•œ í•´ì„ ë°©í–¥ ì„¤ì •')
            
            # ë¶„ì„ ë°©ë²•ë³„ ì¼ë°˜ì  ì¡°ì •ì‚¬í•­
            recommended_methods = analysis_proposals.get('recommended_methods', [])
            for method in recommended_methods:
                if 'íšŒê·€' in method:
                    adjustments.extend([
                        'ë³€ìˆ˜ ì„ íƒ ë°©ë²• ì¡°ì • (stepwise, forward, backward)',
                        'ìƒí˜¸ì‘ìš© í•­ ì¶”ê°€ ê³ ë ¤'
                    ])
                elif 't-test' in method.lower() or 'tê²€ì •' in method:
                    adjustments.append('íš¨ê³¼í¬ê¸° ê¸°ì¤€ ì‹¤ë¬´ì  ìœ ì˜ì„± íŒë‹¨')
                elif 'anova' in method.lower():
                    adjustments.append('ì‚¬í›„ê²€ì • ë°©ë²• ì„ íƒ (Bonferroni, Tukey, ë“±)')
            
            # ì¤‘ë³µ ì œê±° ë° ê°œìˆ˜ ì œí•œ
            adjustments = list(set(adjustments))[:8]  # ìµœëŒ€ 8ê°œë¡œ ì œí•œ
            
            return adjustments
            
        except Exception as e:
            self.logger.error(f"ì¡°ì •ì‚¬í•­ ì‹ë³„ ì˜¤ë¥˜: {e}")
            return ['ê²°ê³¼ í•´ì„ì‹œ ë„ë©”ì¸ ì „ë¬¸ì„± ë°˜ì˜']
    
    def _suggest_pre_analysis_visualizations(self, input_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ë¶„ì„ ì „ ì‹œê°í™” ì œì•ˆ"""
        try:
            visualizations = []
            
            # ë³€ìˆ˜ ë¶„ì„ì—ì„œ ì‹œê°í™” ì œì•ˆ
            variable_analysis = input_data.get('variable_analysis', {})
            
            # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì‹œê°í™”
            numerical_vars = variable_analysis.get('numerical', [])
            if numerical_vars:
                visualizations.extend([
                    {
                        'type': 'histogram',
                        'title': 'ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ í™•ì¸',
                        'description': 'ê° ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ ë¶„í¬ì™€ ì •ê·œì„± í™•ì¸',
                        'variables': [var['name'] for var in numerical_vars[:4]],  # ìµœëŒ€ 4ê°œ
                        'purpose': 'ì •ê·œì„± ê°€ì • ê²€í† '
                    },
                    {
                        'type': 'boxplot',
                        'title': 'ì´ìƒê°’ íƒì§€',
                        'description': 'ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ ì´ìƒê°’ ì‹œê°ì  í™•ì¸',
                        'variables': [var['name'] for var in numerical_vars[:4]],
                        'purpose': 'ì´ìƒê°’ ì‹ë³„'
                    }
                ])
                
                # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ (ë³€ìˆ˜ê°€ 2ê°œ ì´ìƒì¼ ë•Œ)
                if len(numerical_vars) >= 2:
                    visualizations.append({
                        'type': 'correlation_matrix',
                        'title': 'ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„',
                        'description': 'ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ ê°„ì˜ ì„ í˜• ê´€ê³„ í™•ì¸',
                        'variables': [var['name'] for var in numerical_vars],
                        'purpose': 'ë‹¤ì¤‘ê³µì„ ì„± ì˜ˆë¹„ í™•ì¸'
                    })
            
            # ë²”ì£¼í˜• ë³€ìˆ˜ ì‹œê°í™”
            categorical_vars = variable_analysis.get('categorical', [])
            if categorical_vars:
                visualizations.extend([
                    {
                        'type': 'bar_chart',
                        'title': 'ë²”ì£¼í˜• ë³€ìˆ˜ ë¹ˆë„',
                        'description': 'ê° ë²”ì£¼ì˜ ë¹ˆë„ ë° ë¶„í¬ í™•ì¸',
                        'variables': [var['name'] for var in categorical_vars[:3]],
                        'purpose': 'ë²”ì£¼ ê· í˜•ì„± í™•ì¸'
                    }
                ])
            
            # ë³€ìˆ˜ ê°„ ê´€ê³„ ì‹œê°í™”
            if numerical_vars and categorical_vars:
                visualizations.append({
                    'type': 'grouped_boxplot',
                    'title': 'ê·¸ë£¹ë³„ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬',
                    'description': 'ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ ë¶„í¬ ë¹„êµ',
                    'variables': {
                        'numerical': numerical_vars[0]['name'],
                        'categorical': categorical_vars[0]['name']
                    },
                    'purpose': 'ê·¸ë£¹ ê°„ ì°¨ì´ ì˜ˆë¹„ íƒìƒ‰'
                })
            
            return visualizations
            
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ì „ ì‹œê°í™” ì œì•ˆ ì˜¤ë¥˜: {e}")
            return [{'type': 'basic_plot', 'title': 'ê¸°ë³¸ ë°ì´í„° íƒìƒ‰', 'description': 'ë°ì´í„° ê¸°ë³¸ êµ¬ì¡° í™•ì¸', 'variables': [], 'purpose': 'ë°ì´í„° ì´í•´'}]
    
    def _suggest_analysis_visualizations(self, input_data: Dict[str, Any],
                                       analysis_proposals: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ë¶„ì„ ê³¼ì • ì‹œê°í™” ì œì•ˆ"""
        try:
            visualizations = []
            
            recommended_methods = analysis_proposals.get('recommended_methods', [])
            
            for method in recommended_methods:
                if 't-test' in method.lower() or 'tê²€ì •' in method:
                    visualizations.extend([
                        {
                            'type': 'qq_plot',
                            'title': 'Q-Q í”Œë¡¯',
                            'description': 'ì •ê·œì„± ê°€ì • ì‹œê°ì  ê²€ì¦',
                            'purpose': 'ì •ê·œì„± ê°€ì • í™•ì¸'
                        },
                        {
                            'type': 'group_comparison',
                            'title': 'ê·¸ë£¹ ë¹„êµ ì‹œê°í™”',
                            'description': 'ê·¸ë£¹ ê°„ í‰ê·  ë° ë¶„ì‚° ë¹„êµ',
                            'purpose': 'ì°¨ì´ ì‹œê°í™”'
                        }
                    ])
                
                elif 'anova' in method.lower() or 'ë¶„ì‚°ë¶„ì„' in method:
                    visualizations.extend([
                        {
                            'type': 'residual_plot',
                            'title': 'ì”ì°¨ ë¶„ì„',
                            'description': 'ANOVA ê°€ì • ê²€ì¦ì„ ìœ„í•œ ì”ì°¨ ë¶„ì„',
                            'purpose': 'ê°€ì • ê²€ì¦'
                        },
                        {
                            'type': 'means_plot',
                            'title': 'ê·¸ë£¹ë³„ í‰ê·  ë¹„êµ',
                            'description': 'ê° ê·¸ë£¹ì˜ í‰ê· ê³¼ ì‹ ë¢°êµ¬ê°„',
                            'purpose': 'ê·¸ë£¹ ì°¨ì´ ì‹œê°í™”'
                        }
                    ])
                
                elif 'íšŒê·€' in method or 'regression' in method.lower():
                    visualizations.extend([
                        {
                            'type': 'scatter_regression',
                            'title': 'íšŒê·€ì„  í¬í•¨ ì‚°ì ë„',
                            'description': 'ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ì˜ ê´€ê³„ ë° íšŒê·€ì„ ',
                            'purpose': 'ì„ í˜•ê´€ê³„ í™•ì¸'
                        },
                        {
                            'type': 'residual_vs_fitted',
                            'title': 'ì”ì°¨ vs ì í•©ê°’',
                            'description': 'íšŒê·€ ê°€ì • ê²€ì¦ì„ ìœ„í•œ ì”ì°¨ ë¶„ì„',
                            'purpose': 'ë“±ë¶„ì‚°ì„± ë° ì„ í˜•ì„± í™•ì¸'
                        }
                    ])
                
                elif 'ìƒê´€' in method or 'correlation' in method.lower():
                    visualizations.append({
                        'type': 'correlation_heatmap',
                        'title': 'ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µ',
                        'description': 'ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ ê°•ë„ ì‹œê°í™”',
                        'purpose': 'ìƒê´€ê´€ê³„ íŒ¨í„´ ì´í•´'
                    })
            
            return visualizations
            
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ê³¼ì • ì‹œê°í™” ì œì•ˆ ì˜¤ë¥˜: {e}")
            return [{'type': 'basic_analysis_plot', 'title': 'ê¸°ë³¸ ë¶„ì„ ì‹œê°í™”', 'description': 'ë¶„ì„ ê²°ê³¼ ì‹œê°í™”', 'purpose': 'ê²°ê³¼ ì´í•´'}]
    
    def _suggest_post_analysis_visualizations(self, analysis_proposals: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ë¶„ì„ í›„ ì‹œê°í™” ì œì•ˆ"""
        try:
            visualizations = []
            
            recommended_methods = analysis_proposals.get('recommended_methods', [])
            
            for method in recommended_methods:
                if 't-test' in method.lower() or 'tê²€ì •' in method:
                    visualizations.extend([
                        {
                            'type': 'effect_size_plot',
                            'title': 'íš¨ê³¼í¬ê¸° ì‹œê°í™”',
                            'description': 'Cohen\'s dì™€ ì‹ ë¢°êµ¬ê°„ í‘œì‹œ',
                            'purpose': 'ì‹¤ë¬´ì  ìœ ì˜ì„± íŒë‹¨'
                        },
                        {
                            'type': 'mean_difference_plot',
                            'title': 'í‰ê·  ì°¨ì´ ì‹œê°í™”',
                            'description': 'ê·¸ë£¹ ê°„ í‰ê·  ì°¨ì´ì™€ ì‹ ë¢°êµ¬ê°„',
                            'purpose': 'ê²°ê³¼ í•´ì„ ì§€ì›'
                        }
                    ])
                
                elif 'anova' in method.lower() or 'ë¶„ì‚°ë¶„ì„' in method:
                    visualizations.extend([
                        {
                            'type': 'posthoc_comparison',
                            'title': 'ì‚¬í›„ê²€ì • ê²°ê³¼',
                            'description': 'ê·¸ë£¹ ê°„ ë‹¤ì¤‘ë¹„êµ ê²°ê³¼ ì‹œê°í™”',
                            'purpose': 'êµ¬ì²´ì  ì°¨ì´ íŒŒì•…'
                        },
                        {
                            'type': 'eta_squared_plot',
                            'title': 'íš¨ê³¼í¬ê¸° (Eta-squared)',
                            'description': 'ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚°ì˜ ë¹„ìœ¨',
                            'purpose': 'ì‹¤ë¬´ì  ì¤‘ìš”ì„± í‰ê°€'
                        }
                    ])
                
                elif 'íšŒê·€' in method or 'regression' in method.lower():
                    visualizations.extend([
                        {
                            'type': 'coefficient_plot',
                            'title': 'íšŒê·€ê³„ìˆ˜ ì‹œê°í™”',
                            'description': 'íšŒê·€ê³„ìˆ˜ì™€ ì‹ ë¢°êµ¬ê°„',
                            'purpose': 'ë³€ìˆ˜ ì˜í–¥ë ¥ ë¹„êµ'
                        },
                        {
                            'type': 'prediction_plot',
                            'title': 'ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’',
                            'description': 'ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ ì‹œê°í™”',
                            'purpose': 'ëª¨ë¸ ì„±ëŠ¥ í‰ê°€'
                        }
                    ])
                
                elif 'ì¹´ì´ì œê³±' in method or 'chi' in method.lower():
                    visualizations.extend([
                        {
                            'type': 'contingency_heatmap',
                            'title': 'ë¶„í• í‘œ íˆíŠ¸ë§µ',
                            'description': 'ê´€ì°°ë¹ˆë„ì™€ ê¸°ëŒ€ë¹ˆë„ ë¹„êµ',
                            'purpose': 'ì—°ê´€ì„± íŒ¨í„´ ì‹œê°í™”'
                        },
                        {
                            'type': 'cramers_v_plot',
                            'title': 'Cramer\'s V íš¨ê³¼í¬ê¸°',
                            'description': 'ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ ì—°ê´€ì„± ê°•ë„',
                            'purpose': 'ì—°ê´€ì„± í¬ê¸° í‰ê°€'
                        }
                    ])
            
            # ê³µí†µ ê²°ê³¼ ì‹œê°í™”
            visualizations.append({
                'type': 'summary_dashboard',
                'title': 'ë¶„ì„ ê²°ê³¼ ëŒ€ì‹œë³´ë“œ',
                'description': 'ì£¼ìš” ê²°ê³¼ë¥¼ ì¢…í•©í•œ ëŒ€ì‹œë³´ë“œ',
                'purpose': 'ì „ì²´ ê²°ê³¼ ìš”ì•½'
            })
            
            return visualizations
            
        except Exception as e:
            self.logger.error(f"ë¶„ì„ í›„ ì‹œê°í™” ì œì•ˆ ì˜¤ë¥˜: {e}")
            return [{'type': 'results_summary', 'title': 'ê²°ê³¼ ìš”ì•½', 'description': 'ë¶„ì„ ê²°ê³¼ ìš”ì•½', 'purpose': 'ê²°ê³¼ ì •ë¦¬'}]
    
    def get_step_info(self) -> Dict[str, Any]:
        """ë‹¨ê³„ ì •ë³´ ë°˜í™˜"""
        return {
            'step_number': 4,
            'step_name': 'analysis_proposal',
            'description': 'Agentic LLMì˜ ë¶„ì„ ì „ëµ ì œì•ˆ',
            'input_requirements': [
                'user_request',
                'data_overview', 
                'data_quality_assessment',
                'variable_analysis',
                'analysis_recommendations'
            ],
            'output_format': {
                'analysis_proposals': 'Dict',
                'statistical_context': 'Dict', 
                'domain_insights': 'Dict',
                'execution_plan': 'Dict',
                'visualization_suggestions': 'Dict'
            },
            'estimated_duration': '3-5 minutes'
        }

    def _fallback_text_parsing(self, text: str) -> Dict[str, Any]:
        """JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê¸°ë³¸ íŒŒì‹±"""
        try:
            # ê¸°ë³¸ êµ¬ì¡° ìƒì„±
            fallback_result = {
                'recommended_methods': [],
                'alternative_methods': [],
                'method_details': {},
                'rationale': {}
            }
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
            text_lower = text.lower()
            
            # ì¼ë°˜ì ì¸ í†µê³„ ë°©ë²•ë“¤ ê²€ìƒ‰
            common_methods = [
                't-test', 'tê²€ì •', 'anova', 'ë¶„ì‚°ë¶„ì„', 'íšŒê·€ë¶„ì„', 'regression',
                'ìƒê´€ë¶„ì„', 'correlation', 'ì¹´ì´ì œê³±', 'chi-square', 'mann-whitney',
                'kruskal-wallis', 'wilcoxon'
            ]
            
            found_methods = []
            for method in common_methods:
                if method in text_lower:
                    found_methods.append(method)
            
            # ë°œê²¬ëœ ë°©ë²•ì´ ìˆìœ¼ë©´ ì¶”ì²œ ë°©ë²•ìœ¼ë¡œ ì„¤ì •
            if found_methods:
                fallback_result['recommended_methods'] = found_methods[:3]  # ìµœëŒ€ 3ê°œ
                fallback_result['rationale']['general'] = 'í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ë¶„ì„ ë°©ë²•'
            else:
                # ê¸°ë³¸ ë¶„ì„ ë°©ë²• ì œê³µ
                fallback_result['recommended_methods'] = ['ê¸°ìˆ í†µê³„ë¶„ì„', 'íƒìƒ‰ì  ë°ì´í„° ë¶„ì„']
                fallback_result['rationale']['general'] = 'ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ì‹œì‘'
            
            return fallback_result
            
        except Exception as e:
            self.logger.error(f"Fallback íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {
                'recommended_methods': ['ê¸°ìˆ í†µê³„ë¶„ì„'],
                'alternative_methods': [],
                'method_details': {},
                'rationale': {'general': 'ê¸°ë³¸ ë¶„ì„'}
            }


# ë‹¨ê³„ ë“±ë¡
PipelineStepRegistry.register_step(4, AnalysisProposalStep) 