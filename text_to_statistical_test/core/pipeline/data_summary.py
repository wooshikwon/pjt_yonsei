"""
Data Summary Pipeline

3Îã®Í≥Ñ: Îç∞Ïù¥ÌÑ∞ Ïã¨Ï∏µ Î∂ÑÏÑù Î∞è ÏöîÏïΩ
ÏÑ†ÌÉùÎêú Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú Í∏∞Ïà† ÌÜµÍ≥Ñ, Î≥ÄÏàò Î∂ÑÌè¨, Ïû†Ïû¨Ï†Å Ïù¥Ïäà (Í≤∞Ï∏°Ïπò, Ïù¥ÏÉÅÏπò Îì±)Î•º 
Ïã¨Ï∏µÏ†ÅÏúºÎ°ú Î∂ÑÏÑùÌïòÍ≥† ÏöîÏïΩÌïòÏó¨ ÏÇ¨Ïö©ÏûêÏóêÍ≤å Ï†úÍ≥µÌï©ÎãàÎã§.
"""

import logging
from typing import Dict, Any, Optional, List
import pandas as pd
import numpy as np
from pathlib import Path

from .base_pipeline_step import BasePipelineStep, PipelineStepRegistry
from utils.data_loader import DataLoader
from services.statistics.descriptive_stats import DescriptiveStats
from services.statistics.data_preprocessor import DataPreprocessor


class DataSummaryStep(BasePipelineStep):
    """3Îã®Í≥Ñ: Îç∞Ïù¥ÌÑ∞ Ïã¨Ï∏µ Î∂ÑÏÑù Î∞è ÏöîÏïΩ"""
    
    def __init__(self):
        """DataSummaryStep Ï¥àÍ∏∞Ìôî"""
        super().__init__("Îç∞Ïù¥ÌÑ∞ Ïã¨Ï∏µ Î∂ÑÏÑù Î∞è ÏöîÏïΩ", 3)
        self.data_loader = DataLoader()
        self.stats_calculator = DescriptiveStats()
        self.preprocessor = DataPreprocessor()
        
    def validate_input(self, input_data: Dict[str, Any]) -> bool:
        """
        ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù
        
        Args:
            input_data: 2Îã®Í≥ÑÏóêÏÑú Ï†ÑÎã¨Î∞õÏùÄ Îç∞Ïù¥ÌÑ∞
            
        Returns:
            bool: Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù Í≤∞Í≥º
        """
        required_fields = ['selected_file', 'file_info', 'user_request', 'refined_objectives']
        return all(field in input_data for field in required_fields)
    
    def get_expected_output_schema(self) -> Dict[str, Any]:
        """
        ÏòàÏÉÅ Ï∂úÎ†• Ïä§ÌÇ§Îßà Î∞òÌôò
        
        Returns:
            Dict[str, Any]: Ï∂úÎ†• Îç∞Ïù¥ÌÑ∞ Ïä§ÌÇ§Îßà
        """
        return {
            'data_overview': {
                'basic_info': dict,
                'shape': dict,
                'data_types': dict,
                'memory_usage': dict
            },
            'descriptive_statistics': {
                'numerical_summary': dict,
                'categorical_summary': dict,
                'correlation_matrix': dict
            },
            'data_quality_assessment': {
                'missing_values': dict,
                'outliers': dict,
                'duplicates': dict,
                'data_issues': list
            },
            'variable_analysis': {
                'numerical_variables': list,
                'categorical_variables': list,
                'variable_relationships': dict,
                'feature_importance': dict
            },
            'analysis_recommendations': {
                'preprocessing_needed': list,
                'suitable_analyses': list,
                'potential_challenges': list
            },
            'summary_insights': {
                'key_findings': list,
                'data_characteristics': list,
                'analysis_readiness': str
            }
        }
    
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Îç∞Ïù¥ÌÑ∞ Ïã¨Ï∏µ Î∂ÑÏÑù Î∞è ÏöîÏïΩ ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ
        
        Args:
            input_data: ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ Ïª®ÌÖçÏä§Ìä∏
                - selected_file: ÏÑ†ÌÉùÎêú ÌååÏùº Í≤ΩÎ°ú
                - file_info: ÌååÏùº Í∏∞Î≥∏ Ï†ïÎ≥¥
                - user_request: ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠
                - refined_objectives: Î∂ÑÏÑù Î™©Ìëú
                - request_metadata: ÏöîÏ≤≠ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            
        Returns:
            Dict: Ïã§Ìñâ Í≤∞Í≥º
        """
        self.logger.info("3Îã®Í≥Ñ: Îç∞Ïù¥ÌÑ∞ Ïã¨Ï∏µ Î∂ÑÏÑù Î∞è ÏöîÏïΩ ÏãúÏûë")
        
        try:
            # Îç∞Ïù¥ÌÑ∞ Î°úÎî©
            data = self.data_loader.load_data(input_data['selected_file'])
            if data is None:
                return {
                    "success": False,
                    "error": "Îç∞Ïù¥ÌÑ∞ Î°úÎî© Ïã§Ìå®",
                    "file_path": input_data['selected_file']
                }
            
            # 1. Îç∞Ïù¥ÌÑ∞ Í∞úÏöî Î∂ÑÏÑù
            data_overview = self._analyze_data_overview(data, input_data)
            
            # 2. Í∏∞Ïà† ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
            descriptive_stats = self._calculate_descriptive_statistics(data)
            
            # 3. Îç∞Ïù¥ÌÑ∞ ÌíàÏßà ÌèâÍ∞Ä
            quality_assessment = self._assess_data_quality(data)
            
            # 4. Î≥ÄÏàò Î∂ÑÏÑù
            variable_analysis = self._analyze_variables(data, input_data)
            
            # 5. Î∂ÑÏÑù Ï∂îÏ≤úÏÇ¨Ìï≠ ÏÉùÏÑ±
            recommendations = self._generate_analysis_recommendations(
                data, input_data, quality_assessment, variable_analysis
            )
            
            # 6. ÏöîÏïΩ Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±
            summary_insights = self._generate_summary_insights(
                data_overview, descriptive_stats, quality_assessment, 
                variable_analysis, recommendations
            )
            
            self.logger.info("Îç∞Ïù¥ÌÑ∞ Ïã¨Ï∏µ Î∂ÑÏÑù Î∞è ÏöîÏïΩ ÏôÑÎ£å")
            
            return {
                'data_overview': data_overview,
                'descriptive_statistics': descriptive_stats,
                'data_quality_assessment': quality_assessment,
                'variable_analysis': variable_analysis,
                'analysis_recommendations': recommendations,
                'summary_insights': summary_insights,
                'data_object': data,  # Îã§Ïùå Îã®Í≥ÑÏóêÏÑú ÏÇ¨Ïö©Ìï† Îç∞Ïù¥ÌÑ∞ Í∞ùÏ≤¥
                'success_message': f"üìä Îç∞Ïù¥ÌÑ∞ Ïã¨Ï∏µ Î∂ÑÏÑùÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§."
            }
                
        except Exception as e:
            self.logger.error(f"Îç∞Ïù¥ÌÑ∞ Ïã¨Ï∏µ Î∂ÑÏÑù ÌååÏù¥ÌîÑÎùºÏù∏ Ïò§Î•ò: {e}")
            return {
                'error': True,
                'error_message': str(e),
                'error_type': 'analysis_error'
            }
    
    def _load_and_validate_data(self, file_path: str) -> Any:
        """Îç∞Ïù¥ÌÑ∞ Î°úÎî© Î∞è Í∏∞Î≥∏ Í≤ÄÏ¶ù"""
        try:
            data = self.data_loader.load_data(file_path)
            
            if data.empty:
                return {
                    'error': True,
                    'error_message': 'Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§.',
                    'error_type': 'empty_data'
                }
            
            return data
            
        except Exception as e:
            self.logger.error(f"Îç∞Ïù¥ÌÑ∞ Î°úÎî© Ïò§Î•ò: {e}")
            return {
                'error': True,
                'error_message': f'Îç∞Ïù¥ÌÑ∞ Î°úÎî© Ïã§Ìå®: {str(e)}',
                'error_type': 'loading_error'
            }
    
    def _analyze_data_overview(self, data: pd.DataFrame, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Îç∞Ïù¥ÌÑ∞ Í∞úÏöî Î∂ÑÏÑù"""
        basic_info = {
            'file_name': Path(input_data['selected_file']).name,
            'total_rows': len(data),
            'total_columns': len(data.columns),
            'column_names': list(data.columns),
            'index_type': str(type(data.index).__name__)
        }
        
        shape_info = {
            'dimensions': f"{data.shape[0]} rows √ó {data.shape[1]} columns",
            'size': data.size,
            'memory_usage_mb': round(data.memory_usage(deep=True).sum() / 1024 / 1024, 2)
        }
        
        # Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î∂ÑÏÑù
        data_types = self._analyze_data_types(data)
        
        memory_info = {
            'total_memory_mb': round(data.memory_usage(deep=True).sum() / 1024 / 1024, 2),
            'memory_per_column': {
                col: round(data.memory_usage(deep=True)[col] / 1024 / 1024, 2) 
                for col in data.columns
            }
        }
        
        return {
            'basic_info': basic_info,
            'shape': shape_info,
            'data_types': data_types,
            'memory_usage': memory_info
        }
    
    def _analyze_data_types(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î∂ÑÏÑù"""
        type_summary = {
            'numerical': [],
            'categorical': [],
            'datetime': [],
            'boolean': [],
            'object': []
        }
        
        type_counts = {
            'numerical': 0,
            'categorical': 0,
            'datetime': 0,
            'boolean': 0,
            'object': 0
        }
        
        for col in data.columns:
            dtype = data[col].dtype
            
            if pd.api.types.is_numeric_dtype(dtype):
                if data[col].nunique() <= 10 and data[col].nunique() < len(data) * 0.05:
                    # ÏàòÏπòÌòïÏù¥ÏßÄÎßå Î≤îÏ£ºÌòïÏúºÎ°ú Î≥¥Ïù¥Îäî Í≤ΩÏö∞
                    type_summary['categorical'].append(col)
                    type_counts['categorical'] += 1
                else:
                    type_summary['numerical'].append(col)
                    type_counts['numerical'] += 1
            elif pd.api.types.is_datetime64_any_dtype(dtype):
                type_summary['datetime'].append(col)
                type_counts['datetime'] += 1
            elif pd.api.types.is_bool_dtype(dtype):
                type_summary['boolean'].append(col)
                type_counts['boolean'] += 1
            else:
                # ÌÖçÏä§Ìä∏ÎÇò Î≤îÏ£ºÌòï
                if data[col].nunique() <= 50:  # Î≤îÏ£ºÌòïÏúºÎ°ú Í∞ÑÏ£º
                    type_summary['categorical'].append(col)
                    type_counts['categorical'] += 1
                else:
                    type_summary['object'].append(col)
                    type_counts['object'] += 1
        
        return {
            'type_summary': type_summary,
            'type_counts': type_counts,
            'detailed_types': {col: str(data[col].dtype) for col in data.columns}
        }
    
    def _calculate_descriptive_statistics(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Í∏∞Ïà† ÌÜµÍ≥Ñ Í≥ÑÏÇ∞"""
        numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
        
        # ÏàòÏπòÌòï Î≥ÄÏàò Í∏∞Ïà† ÌÜµÍ≥Ñ
        numerical_summary = {}
        if numerical_cols:
            numerical_summary = self.stats_calculator.calculate_numerical_stats(data[numerical_cols])
        
        # Î≤îÏ£ºÌòï Î≥ÄÏàò Í∏∞Ïà† ÌÜµÍ≥Ñ
        categorical_summary = {}
        if categorical_cols:
            categorical_summary = self.stats_calculator.calculate_categorical_stats(data[categorical_cols])
        
        # ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌñâÎ†¨ (ÏàòÏπòÌòï Î≥ÄÏàòÎì§Ïóê ÎåÄÌï¥ÏÑúÎßå)
        correlation_matrix = {}
        if len(numerical_cols) > 1:
            correlation_matrix = self.stats_calculator.calculate_correlation_matrix(data[numerical_cols])
        
        return {
            'numerical_summary': numerical_summary,
            'categorical_summary': categorical_summary,
            'correlation_matrix': correlation_matrix
        }
    
    def _assess_data_quality(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Îç∞Ïù¥ÌÑ∞ ÌíàÏßà ÌèâÍ∞Ä"""
        # Í≤∞Ï∏°Ïπò Î∂ÑÏÑù
        missing_analysis = self._analyze_missing_values(data)
        
        # Ïù¥ÏÉÅÏπò Î∂ÑÏÑù (ÏàòÏπòÌòï Î≥ÄÏàòÎì§Ïóê ÎåÄÌï¥ÏÑúÎßå)
        outlier_analysis = self._analyze_outliers(data)
        
        # Ï§ëÎ≥µ Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù
        duplicate_analysis = self._analyze_duplicates(data)
        
        # Ï†ÑÎ∞òÏ†ÅÏù∏ Îç∞Ïù¥ÌÑ∞ Ïù¥Ïäà ÏãùÎ≥Ñ
        data_issues = self._identify_data_issues(data, missing_analysis, outlier_analysis, duplicate_analysis)
        
        return {
            'missing_values': missing_analysis,
            'outliers': outlier_analysis,
            'duplicates': duplicate_analysis,
            'data_issues': data_issues
        }
    
    def _analyze_missing_values(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Í≤∞Ï∏°Ïπò Î∂ÑÏÑù"""
        missing_count = data.isnull().sum()
        missing_percent = (missing_count / len(data)) * 100
        
        missing_summary = {
            'total_missing': missing_count.sum(),
            'columns_with_missing': missing_count[missing_count > 0].to_dict(),
            'missing_percentages': missing_percent[missing_percent > 0].to_dict(),
            'complete_rows': len(data.dropna()),
            'missing_patterns': self._analyze_missing_patterns(data)
        }
        
        return missing_summary
    
    def _analyze_missing_patterns(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Í≤∞Ï∏°Ïπò Ìå®ÌÑ¥ Î∂ÑÏÑù"""
        # Í≤∞Ï∏°ÏπòÍ∞Ä ÏûàÎäî Ïª¨ÎüºÎì§Îßå Î∂ÑÏÑù
        missing_cols = data.columns[data.isnull().any()].tolist()
        
        if not missing_cols:
            return {'pattern_analysis': 'No missing values found'}
        
        # Í≤∞Ï∏°Ïπò Ìå®ÌÑ¥ Ï°∞Ìï© Î∂ÑÏÑù
        missing_patterns = data[missing_cols].isnull().value_counts().head(10)
        
        return {
            'top_patterns': missing_patterns.to_dict(),
            'pattern_description': 'Most common combinations of missing values across columns'
        }
    
    def _analyze_outliers(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Ïù¥ÏÉÅÏπò Î∂ÑÏÑù"""
        numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        
        if not numerical_cols:
            return {'analysis': 'No numerical columns for outlier analysis'}
        
        outlier_summary = {}
        
        for col in numerical_cols:
            Q1 = data[col].quantile(0.25)
            Q3 = data[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]
            
            outlier_summary[col] = {
                'outlier_count': len(outliers),
                'outlier_percentage': round((len(outliers) / len(data)) * 100, 2),
                'lower_bound': lower_bound,
                'upper_bound': upper_bound,
                'extreme_values': {
                    'min': data[col].min(),
                    'max': data[col].max()
                }
            }
        
        return outlier_summary
    
    def _analyze_duplicates(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Ï§ëÎ≥µ Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù"""
        duplicate_rows = data.duplicated()
        
        return {
            'total_duplicates': duplicate_rows.sum(),
            'duplicate_percentage': round((duplicate_rows.sum() / len(data)) * 100, 2),
            'unique_rows': len(data) - duplicate_rows.sum(),
            'duplicate_subset_analysis': self._analyze_partial_duplicates(data)
        }
    
    def _analyze_partial_duplicates(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Î∂ÄÎ∂Ñ Ï§ëÎ≥µ Î∂ÑÏÑù"""
        # Ï£ºÏöî Ïª¨ÎüºÎì§Ïóê ÎåÄÌïú Î∂ÄÎ∂Ñ Ï§ëÎ≥µ Í≤ÄÏÇ¨
        important_cols = data.columns[:5].tolist()  # Ï≤òÏùå 5Í∞ú Ïª¨ÎüºÎßå Î∂ÑÏÑù
        
        partial_duplicates = {}
        for col in important_cols:
            duplicate_values = data[col].duplicated()
            partial_duplicates[col] = {
                'duplicate_count': duplicate_values.sum(),
                'unique_values': data[col].nunique(),
                'most_common': data[col].value_counts().head(3).to_dict()
            }
        
        return partial_duplicates
    
    def _identify_data_issues(self, data: pd.DataFrame, missing_analysis: Dict, 
                            outlier_analysis: Dict, duplicate_analysis: Dict) -> List[str]:
        """Ï†ÑÎ∞òÏ†ÅÏù∏ Îç∞Ïù¥ÌÑ∞ Ïù¥Ïäà ÏãùÎ≥Ñ"""
        issues = []
        
        # Í≤∞Ï∏°Ïπò Í¥ÄÎ†® Ïù¥Ïäà
        if missing_analysis['total_missing'] > 0:
            high_missing_cols = [
                col for col, pct in missing_analysis['missing_percentages'].items() 
                if pct > 20
            ]
            if high_missing_cols:
                issues.append(f"ÎÜíÏùÄ Í≤∞Ï∏°Ïπò ÎπÑÏú® Ïª¨Îüº: {', '.join(high_missing_cols)}")
        
        # Ïù¥ÏÉÅÏπò Í¥ÄÎ†® Ïù¥Ïäà
        if isinstance(outlier_analysis, dict) and outlier_analysis.get('analysis') != 'No numerical columns for outlier analysis':
            high_outlier_cols = [
                col for col, info in outlier_analysis.items() 
                if info.get('outlier_percentage', 0) > 5
            ]
            if high_outlier_cols:
                issues.append(f"Ïù¥ÏÉÅÏπòÍ∞Ä ÎßéÏùÄ Ïª¨Îüº: {', '.join(high_outlier_cols)}")
        
        # Ï§ëÎ≥µ Îç∞Ïù¥ÌÑ∞ Ïù¥Ïäà
        if duplicate_analysis['duplicate_percentage'] > 5:
            issues.append(f"Ï§ëÎ≥µ Îç∞Ïù¥ÌÑ∞ ÎπÑÏú®Ïù¥ ÎÜíÏùå: {duplicate_analysis['duplicate_percentage']}%")
        
        # Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞ Í¥ÄÎ†® Ïù¥Ïäà
        if len(data) < 30:
            issues.append("ÌëúÎ≥∏ ÌÅ¨Í∏∞Í∞Ä ÏûëÏùå (ÌÜµÍ≥ÑÏ†Å Í≤ÄÏ†ïÏóê Ï†úÌïúÏù¥ ÏûàÏùÑ Ïàò ÏûàÏùå)")
        
        # Î≥ÄÏàò Ïàò Í¥ÄÎ†® Ïù¥Ïäà
        if len(data.columns) > len(data):
            issues.append("Î≥ÄÏàò ÏàòÍ∞Ä Í¥ÄÏ∏°Ïπò ÏàòÎ≥¥Îã§ ÎßéÏùå (Ï∞®ÏõêÏùò Ï†ÄÏ£º Í∞ÄÎä•ÏÑ±)")
        
        return issues
    
    def _analyze_variables(self, data: pd.DataFrame, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Î≥ÄÏàò Î∂ÑÏÑù"""
        numerical_vars = data.select_dtypes(include=[np.number]).columns.tolist()
        categorical_vars = data.select_dtypes(include=['object', 'category']).columns.tolist()
        
        # ÏÇ¨Ïö©ÏûêÍ∞Ä Ïñ∏Í∏âÌïú Î≥ÄÏàòÎì§Í≥º Îß§Ïπ≠
        request_metadata = input_data.get('request_metadata', {})
        target_variables = request_metadata.get('target_variables', [])
        group_variables = request_metadata.get('group_variables', [])
        
        # Î≥ÄÏàò Í∞Ñ Í¥ÄÍ≥Ñ Î∂ÑÏÑù
        relationships = self._analyze_variable_relationships(data, numerical_vars, categorical_vars)
        
        # ÌäπÏÑ± Ï§ëÏöîÎèÑ Î∂ÑÏÑù (Í∞ÑÎã®Ìïú Î≤ÑÏ†Ñ)
        feature_importance = self._analyze_feature_importance(data, target_variables)
        
        return {
            'numerical_variables': numerical_vars,
            'categorical_variables': categorical_vars,
            'target_variables': target_variables,
            'group_variables': group_variables,
            'variable_relationships': relationships,
            'feature_importance': feature_importance
        }
    
    def _analyze_variable_relationships(self, data: pd.DataFrame, 
                                      numerical_vars: List[str], 
                                      categorical_vars: List[str]) -> Dict[str, Any]:
        """Î≥ÄÏàò Í∞Ñ Í¥ÄÍ≥Ñ Î∂ÑÏÑù"""
        relationships = {}
        
        # ÏàòÏπòÌòï Î≥ÄÏàò Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ
        if len(numerical_vars) > 1:
            corr_matrix = data[numerical_vars].corr()
            high_correlations = []
            
            for i in range(len(numerical_vars)):
                for j in range(i+1, len(numerical_vars)):
                    corr_value = corr_matrix.iloc[i, j]
                    if abs(corr_value) > 0.7:  # ÎÜíÏùÄ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ
                        high_correlations.append({
                            'var1': numerical_vars[i],
                            'var2': numerical_vars[j],
                            'correlation': round(corr_value, 3)
                        })
            
            relationships['high_correlations'] = high_correlations
        
        # Î≤îÏ£ºÌòï Î≥ÄÏàòÏôÄ ÏàòÏπòÌòï Î≥ÄÏàò Í∞Ñ Í¥ÄÍ≥Ñ (Í∞ÑÎã®Ìïú Î∂ÑÏÑù)
        cat_num_relationships = []
        for cat_var in categorical_vars[:3]:  # Ï≤òÏùå 3Í∞úÎßå Î∂ÑÏÑù
            for num_var in numerical_vars[:3]:  # Ï≤òÏùå 3Í∞úÎßå Î∂ÑÏÑù
                try:
                    grouped = data.groupby(cat_var)[num_var].agg(['mean', 'std', 'count'])
                    if len(grouped) > 1:  # Í∑∏Î£πÏù¥ Ïó¨Îü¨ Í∞ú ÏûàÎäî Í≤ΩÏö∞
                        cat_num_relationships.append({
                            'categorical_var': cat_var,
                            'numerical_var': num_var,
                            'group_stats': grouped.to_dict('index')
                        })
                except:
                    continue
        
        relationships['categorical_numerical'] = cat_num_relationships[:5]  # ÏµúÎåÄ 5Í∞úÍπåÏßÄ
        
        return relationships
    
    def _analyze_feature_importance(self, data: pd.DataFrame, target_variables: List[str]) -> Dict[str, Any]:
        """ÌäπÏÑ± Ï§ëÏöîÎèÑ Î∂ÑÏÑù (Í∞ÑÎã®Ìïú Î≤ÑÏ†Ñ)"""
        if not target_variables:
            return {'analysis': 'No target variables specified'}
        
        importance_analysis = {}
        
        for target_var in target_variables:
            if target_var in data.columns:
                # Í∞ÑÎã®Ìïú ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Í∏∞Î∞ò Ï§ëÏöîÎèÑ
                numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()
                if target_var in numerical_cols and len(numerical_cols) > 1:
                    correlations = data[numerical_cols].corr()[target_var].abs().sort_values(ascending=False)
                    importance_analysis[target_var] = correlations.head(5).to_dict()
        
        return importance_analysis
    
    def _generate_analysis_recommendations(self, data: pd.DataFrame, input_data: Dict[str, Any],
                                         quality_assessment: Dict[str, Any],
                                         variable_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Î∂ÑÏÑù Ï∂îÏ≤úÏÇ¨Ìï≠ ÏÉùÏÑ±"""
        preprocessing_needed = []
        suitable_analyses = []
        potential_challenges = []
        
        # Ï†ÑÏ≤òÎ¶¨ Ï∂îÏ≤ú
        if quality_assessment['missing_values']['total_missing'] > 0:
            preprocessing_needed.append("Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨ (Ï†úÍ±∞ ÎòêÎäî ÎåÄÏ≤¥)")
        
        if quality_assessment['duplicates']['duplicate_percentage'] > 1:
            preprocessing_needed.append("Ï§ëÎ≥µ Îç∞Ïù¥ÌÑ∞ Ï†úÍ±∞")
        
        # Ïù¥ÏÉÅÏπòÍ∞Ä ÎßéÏùÄ Í≤ΩÏö∞
        outlier_analysis = quality_assessment['outliers']
        if isinstance(outlier_analysis, dict):
            high_outlier_cols = [
                col for col, info in outlier_analysis.items() 
                if isinstance(info, dict) and info.get('outlier_percentage', 0) > 10
            ]
            if high_outlier_cols:
                preprocessing_needed.append("Ïù¥ÏÉÅÏπò Ï≤òÎ¶¨ Í≥†Î†§")
        
        # Ï†ÅÌï©Ìïú Î∂ÑÏÑù Î∞©Î≤ï Ï∂îÏ≤ú
        request_metadata = input_data.get('request_metadata', {})
        analysis_type = request_metadata.get('analysis_type', 'unknown')
        
        num_vars = len(variable_analysis['numerical_variables'])
        cat_vars = len(variable_analysis['categorical_variables'])
        
        if analysis_type == 'group_comparison':
            if cat_vars > 0 and num_vars > 0:
                suitable_analyses.append("Í∑∏Î£π Í∞Ñ ÌèâÍ∑† ÎπÑÍµê (t-Í≤ÄÏ†ï, ANOVA)")
            if cat_vars > 1:
                suitable_analyses.append("Î≤îÏ£ºÌòï Î≥ÄÏàò Í∞Ñ Ïó∞Í¥ÄÏÑ± Î∂ÑÏÑù (Ïπ¥Ïù¥Ï†úÍ≥± Í≤ÄÏ†ï)")
        
        elif analysis_type == 'relationship':
            if num_vars > 1:
                suitable_analyses.append("ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù")
                suitable_analyses.append("ÌöåÍ∑ÄÎ∂ÑÏÑù")
        
        elif analysis_type == 'categorical':
            if cat_vars > 1:
                suitable_analyses.append("Ïπ¥Ïù¥Ï†úÍ≥± ÎèÖÎ¶ΩÏÑ± Í≤ÄÏ†ï")
                suitable_analyses.append("FisherÏùò Ï†ïÌôïÍ≤ÄÏ†ï")
        
        # Ïû†Ïû¨Ï†Å ÎèÑÏ†ÑÍ≥ºÏ†ú
        if len(data) < 30:
            potential_challenges.append("ÏûëÏùÄ ÌëúÎ≥∏ ÌÅ¨Í∏∞Î°ú Ïù∏Ìïú Í≤ÄÏ†ïÎ†• Ï†úÌïú")
        
        if quality_assessment['missing_values']['total_missing'] > len(data) * 0.1:
            potential_challenges.append("ÎÜíÏùÄ Í≤∞Ï∏°Ïπò ÎπÑÏú®Î°ú Ïù∏Ìïú Ìé∏Ìñ• Í∞ÄÎä•ÏÑ±")
        
        if num_vars > 0:
            # Ï†ïÍ∑úÏÑ± Í∞ÑÎã® Ï≤¥ÌÅ¨
            numerical_data = data.select_dtypes(include=[np.number])
            for col in numerical_data.columns:
                if len(numerical_data[col].dropna()) > 0:
                    # Í∞ÑÎã®Ìïú Ï†ïÍ∑úÏÑ± Ï≤¥ÌÅ¨ (ÏôúÎèÑ, Ï≤®ÎèÑ)
                    skewness = numerical_data[col].skew()
                    if abs(skewness) > 2:
                        potential_challenges.append(f"{col} Î≥ÄÏàòÏùò ÎπÑÏ†ïÍ∑úÏÑ± (ÏôúÎèÑ: {round(skewness, 2)})")
                        break
        
        return {
            'preprocessing_needed': preprocessing_needed,
            'suitable_analyses': suitable_analyses,
            'potential_challenges': potential_challenges
        }
    
    def _generate_summary_insights(self, data_overview: Dict, descriptive_stats: Dict,
                                 quality_assessment: Dict, variable_analysis: Dict,
                                 recommendations: Dict) -> Dict[str, Any]:
        """ÏöîÏïΩ Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±"""
        key_findings = []
        data_characteristics = []
        
        # Ï£ºÏöî Î∞úÍ≤¨ÏÇ¨Ìï≠
        total_rows = data_overview['basic_info']['total_rows']
        total_cols = data_overview['basic_info']['total_columns']
        key_findings.append(f"Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞: {total_rows:,}Ìñâ √ó {total_cols}Ïó¥")
        
        num_vars = len(variable_analysis['numerical_variables'])
        cat_vars = len(variable_analysis['categorical_variables'])
        key_findings.append(f"Î≥ÄÏàò Íµ¨ÏÑ±: ÏàòÏπòÌòï {num_vars}Í∞ú, Î≤îÏ£ºÌòï {cat_vars}Í∞ú")
        
        missing_total = quality_assessment['missing_values']['total_missing']
        if missing_total > 0:
            missing_pct = round((missing_total / (total_rows * total_cols)) * 100, 1)
            key_findings.append(f"Í≤∞Ï∏°Ïπò: Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Ïùò {missing_pct}%")
        
        # Îç∞Ïù¥ÌÑ∞ ÌäπÏÑ±
        if total_rows >= 1000:
            data_characteristics.append("ÎåÄÏö©Îüâ Îç∞Ïù¥ÌÑ∞ÏÖã")
        elif total_rows < 100:
            data_characteristics.append("ÏÜåÍ∑úÎ™® Îç∞Ïù¥ÌÑ∞ÏÖã")
        else:
            data_characteristics.append("Ï§ëÍ∞Ñ Í∑úÎ™® Îç∞Ïù¥ÌÑ∞ÏÖã")
        
        if quality_assessment['duplicates']['duplicate_percentage'] < 1:
            data_characteristics.append("Ï§ëÎ≥µ Îç∞Ïù¥ÌÑ∞ Í±∞Ïùò ÏóÜÏùå")
        
        if missing_total == 0:
            data_characteristics.append("Í≤∞Ï∏°Ïπò ÏóÜÎäî ÏôÑÏ†ÑÌïú Îç∞Ïù¥ÌÑ∞")
        
        # Î∂ÑÏÑù Ï§ÄÎπÑÎèÑ ÌèâÍ∞Ä
        readiness_score = 0
        
        # Í∏çÏ†ïÏ†Å ÏöîÏÜå
        if missing_total == 0:
            readiness_score += 30
        elif missing_total < total_rows * total_cols * 0.05:
            readiness_score += 20
        
        if quality_assessment['duplicates']['duplicate_percentage'] < 5:
            readiness_score += 20
        
        if total_rows >= 30:
            readiness_score += 20
        
        if len(recommendations['preprocessing_needed']) <= 2:
            readiness_score += 15
        
        if num_vars > 0 and cat_vars > 0:
            readiness_score += 15  # Îã§ÏñëÌïú Î≥ÄÏàò ÌÉÄÏûÖ
        
        # Î∂ÑÏÑù Ï§ÄÎπÑÎèÑ Í≤∞Ï†ï
        if readiness_score >= 80:
            analysis_readiness = "excellent"
        elif readiness_score >= 60:
            analysis_readiness = "good"
        elif readiness_score >= 40:
            analysis_readiness = "fair"
        else:
            analysis_readiness = "poor"
        
        return {
            'key_findings': key_findings,
            'data_characteristics': data_characteristics,
            'analysis_readiness': analysis_readiness,
            'readiness_score': readiness_score
        }
    
    def get_step_info(self) -> Dict[str, Any]:
        """Îã®Í≥Ñ Ï†ïÎ≥¥ Î∞òÌôò (Î∂ÄÎ™® ÌÅ¥ÎûòÏä§ Î©îÏÑúÎìú ÌôïÏû•)"""
        base_info = super().get_step_info()
        base_info.update({
            'description': 'Îç∞Ïù¥ÌÑ∞ Ïã¨Ï∏µ Î∂ÑÏÑù Î∞è ÏöîÏïΩ',
            'input_requirements': ['selected_file', 'file_info', 'user_request', 'refined_objectives'],
            'output_provides': [
                'data_overview', 'descriptive_statistics', 'data_quality_assessment',
                'variable_analysis', 'analysis_recommendations', 'summary_insights'
            ],
            'capabilities': [
                'Í∏∞Ïà† ÌÜµÍ≥Ñ Í≥ÑÏÇ∞', 'Îç∞Ïù¥ÌÑ∞ ÌíàÏßà ÌèâÍ∞Ä', 'Î≥ÄÏàò Í¥ÄÍ≥Ñ Î∂ÑÏÑù',
                'Í≤∞Ï∏°Ïπò/Ïù¥ÏÉÅÏπò ÌÉêÏßÄ', 'Î∂ÑÏÑù Ï∂îÏ≤úÏÇ¨Ìï≠ Ï†úÍ≥µ'
            ]
        })
        return base_info


# Îã®Í≥Ñ Îì±Î°ù
PipelineStepRegistry.register_step(3, DataSummaryStep) 