"""
Data Summary Pipeline

3ë‹¨ê³„: ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ ë° ìš”ì•½
ì„ íƒëœ ë°ì´í„°ì— ëŒ€í•œ ê¸°ìˆ  í†µê³„, ë³€ìˆ˜ ë¶„í¬, ì ì¬ì  ì´ìŠˆ (ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ ë“±)ë¥¼ 
ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ìš”ì•½í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì œê³µí•©ë‹ˆë‹¤.
"""

import logging
from typing import Dict, Any, Optional, List
import pandas as pd
import numpy as np
from pathlib import Path

from .base_pipeline_step import BasePipelineStep, PipelineStepRegistry
from utils.data_loader import DataLoader
from services.statistics.descriptive_stats import DescriptiveStats
from services.statistics.data_preprocessor import DataPreprocessor


class DataSummaryStep(BasePipelineStep):
    """3ë‹¨ê³„: ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ ë° ìš”ì•½"""
    
    def __init__(self):
        """DataSummaryStep ì´ˆê¸°í™”"""
        super().__init__("ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ ë° ìš”ì•½", 3)
        self.data_loader = DataLoader()
        self.stats_calculator = DescriptiveStats()
        self.preprocessor = DataPreprocessor()
        
    def validate_input(self, input_data: Dict[str, Any]) -> bool:
        """
        ì…ë ¥ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
        
        Args:
            input_data: 2ë‹¨ê³„ì—ì„œ ì „ë‹¬ë°›ì€ ë°ì´í„°
            
        Returns:
            bool: ìœ íš¨ì„± ê²€ì¦ ê²°ê³¼
        """
        required_fields = ['selected_file', 'file_info', 'user_request', 'refined_objectives']
        return all(field in input_data for field in required_fields)
    
    def get_expected_output_schema(self) -> Dict[str, Any]:
        """
        ì˜ˆìƒ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: ì¶œë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ
        """
        return {
            'data_overview': {
                'basic_info': dict,
                'shape': dict,
                'data_types': dict,
                'memory_usage': dict
            },
            'descriptive_statistics': {
                'numerical_summary': dict,
                'categorical_summary': dict,
                'correlation_matrix': dict
            },
            'data_quality_assessment': {
                'missing_values': dict,
                'outliers': dict,
                'duplicates': dict,
                'data_issues': list
            },
            'variable_analysis': {
                'numerical_variables': list,
                'categorical_variables': list,
                'variable_relationships': dict,
                'feature_importance': dict
            },
            'analysis_recommendations': {
                'preprocessing_needed': list,
                'suitable_analyses': list,
                'potential_challenges': list
            },
            'summary_insights': {
                'key_findings': list,
                'data_characteristics': list,
                'analysis_readiness': str
            }
        }
    
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ ë° ìš”ì•½ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            input_data: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸
                - selected_file: ì„ íƒëœ íŒŒì¼ ê²½ë¡œ
                - file_info: íŒŒì¼ ê¸°ë³¸ ì •ë³´
                - user_request: ì‚¬ìš©ì ìš”ì²­
                - refined_objectives: ë¶„ì„ ëª©í‘œ
                - request_metadata: ìš”ì²­ ë©”íƒ€ë°ì´í„°
            
        Returns:
            Dict: ì‹¤í–‰ ê²°ê³¼
        """
        self.logger.info("3ë‹¨ê³„: ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ ë° ìš”ì•½ ì‹œì‘")
        
        try:
            # ë°ì´í„° ë¡œë”©
            data = self.data_loader.load_data(input_data['selected_file'])
            if data is None:
                return {
                    "success": False,
                    "error": "ë°ì´í„° ë¡œë”© ì‹¤íŒ¨",
                    "file_path": input_data['selected_file']
                }
            
            # 1. ë°ì´í„° ê°œìš” ë¶„ì„
            data_overview = self._analyze_data_overview(data, input_data)
            
            # 2. ê¸°ìˆ  í†µê³„ ê³„ì‚°
            descriptive_stats = self._calculate_descriptive_statistics(data)
            
            # 3. ë°ì´í„° í’ˆì§ˆ í‰ê°€
            quality_assessment = self._assess_data_quality(data)
            
            # 4. ë³€ìˆ˜ ë¶„ì„
            variable_analysis = self._analyze_variables(data, input_data)
            
            # 5. ë¶„ì„ ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = self._generate_analysis_recommendations(
                data, input_data, quality_assessment, variable_analysis
            )
            
            # 6. ìš”ì•½ ì¸ì‚¬ì´íŠ¸ ìƒì„±
            summary_insights = self._generate_summary_insights(
                data_overview, descriptive_stats, quality_assessment, 
                variable_analysis, recommendations
            )
            
            self.logger.info("ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ ë° ìš”ì•½ ì™„ë£Œ")
            
            return {
                'data_overview': data_overview,
                'descriptive_statistics': descriptive_stats,
                'data_quality_assessment': quality_assessment,
                'variable_analysis': variable_analysis,
                'analysis_recommendations': recommendations,
                'summary_insights': summary_insights,
                'data_object': data,  # ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ë°ì´í„° ê°ì²´
                'success_message': f"ğŸ“Š ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            }
                
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': str(e),
                'error_type': 'analysis_error'
            }
    
    def _load_and_validate_data(self, file_path: str) -> Any:
        """ë°ì´í„° ë¡œë”© ë° ê¸°ë³¸ ê²€ì¦"""
        try:
            data = self.data_loader.load_data(file_path)
            
            if data.empty:
                return {
                    'error': True,
                    'error_message': 'ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.',
                    'error_type': 'empty_data'
                }
            
            return data
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': f'ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}',
                'error_type': 'loading_error'
            }
    
    def _analyze_data_overview(self, data: pd.DataFrame, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ê°œìš” ë¶„ì„"""
        basic_info = {
            'file_name': Path(input_data['selected_file']).name,
            'total_rows': len(data),
            'total_columns': len(data.columns),
            'column_names': list(data.columns),
            'index_type': str(type(data.index).__name__)
        }
        
        shape_info = {
            'dimensions': f"{data.shape[0]} rows Ã— {data.shape[1]} columns",
            'size': data.size,
            'memory_usage_mb': round(data.memory_usage(deep=True).sum() / 1024 / 1024, 2)
        }
        
        # ë°ì´í„° íƒ€ì… ë¶„ì„
        data_types = self._analyze_data_types(data)
        
        memory_info = {
            'total_memory_mb': round(data.memory_usage(deep=True).sum() / 1024 / 1024, 2),
            'memory_per_column': {
                col: round(data.memory_usage(deep=True)[col] / 1024 / 1024, 2) 
                for col in data.columns
            }
        }
        
        return {
            'basic_info': basic_info,
            'shape': shape_info,
            'data_types': data_types,
            'memory_usage': memory_info
        }
    
    def _analyze_data_types(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° íƒ€ì… ë¶„ì„"""
        type_summary = {
            'numerical': [],
            'categorical': [],
            'datetime': [],
            'boolean': [],
            'object': []
        }
        
        type_counts = {
            'numerical': 0,
            'categorical': 0,
            'datetime': 0,
            'boolean': 0,
            'object': 0
        }
        
        for col in data.columns:
            dtype = data[col].dtype
            
            if pd.api.types.is_numeric_dtype(dtype):
                if data[col].nunique() <= 10 and data[col].nunique() < len(data) * 0.05:
                    # ìˆ˜ì¹˜í˜•ì´ì§€ë§Œ ë²”ì£¼í˜•ìœ¼ë¡œ ë³´ì´ëŠ” ê²½ìš°
                    type_summary['categorical'].append(col)
                    type_counts['categorical'] += 1
                else:
                    type_summary['numerical'].append(col)
                    type_counts['numerical'] += 1
            elif pd.api.types.is_datetime64_any_dtype(dtype):
                type_summary['datetime'].append(col)
                type_counts['datetime'] += 1
            elif pd.api.types.is_bool_dtype(dtype):
                type_summary['boolean'].append(col)
                type_counts['boolean'] += 1
            else:
                # í…ìŠ¤íŠ¸ë‚˜ ë²”ì£¼í˜•
                if data[col].nunique() <= 50:  # ë²”ì£¼í˜•ìœ¼ë¡œ ê°„ì£¼
                    type_summary['categorical'].append(col)
                    type_counts['categorical'] += 1
                else:
                    type_summary['object'].append(col)
                    type_counts['object'] += 1
        
        return {
            'type_summary': type_summary,
            'type_counts': type_counts,
            'detailed_types': {col: str(data[col].dtype) for col in data.columns}
        }
    
    def _calculate_descriptive_statistics(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ê¸°ìˆ  í†µê³„ ê³„ì‚°"""
        numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê¸°ìˆ  í†µê³„
        numerical_summary = {}
        if numerical_cols:
            numerical_summary = self.stats_calculator.calculate_numerical_stats(data[numerical_cols])
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ê¸°ìˆ  í†µê³„
        categorical_summary = {}
        if categorical_cols:
            categorical_summary = self.stats_calculator.calculate_categorical_stats(data[categorical_cols])
        
        # ìƒê´€ê´€ê³„ í–‰ë ¬ (ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ì— ëŒ€í•´ì„œë§Œ)
        correlation_matrix = {}
        if len(numerical_cols) > 1:
            correlation_matrix = self.stats_calculator.calculate_correlation_matrix(data[numerical_cols])
        
        return {
            'numerical_summary': numerical_summary,
            'categorical_summary': categorical_summary,
            'correlation_matrix': correlation_matrix
        }
    
    def _assess_data_quality(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        # ê²°ì¸¡ì¹˜ ë¶„ì„
        missing_analysis = self._analyze_missing_values(data)
        
        # ì´ìƒì¹˜ ë¶„ì„ (ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ì— ëŒ€í•´ì„œë§Œ)
        outlier_analysis = self._analyze_outliers(data)
        
        # ì¤‘ë³µ ë°ì´í„° ë¶„ì„
        duplicate_analysis = self._analyze_duplicates(data)
        
        # ì „ë°˜ì ì¸ ë°ì´í„° ì´ìŠˆ ì‹ë³„
        data_issues = self._identify_data_issues(data, missing_analysis, outlier_analysis, duplicate_analysis)
        
        return {
            'missing_values': missing_analysis,
            'outliers': outlier_analysis,
            'duplicates': duplicate_analysis,
            'data_issues': data_issues
        }
    
    def _analyze_missing_values(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ê²°ì¸¡ì¹˜ ë¶„ì„"""
        missing_count = data.isnull().sum()
        missing_percent = (missing_count / len(data)) * 100
        
        missing_summary = {
            'total_missing': missing_count.sum(),
            'columns_with_missing': missing_count[missing_count > 0].to_dict(),
            'missing_percentages': missing_percent[missing_percent > 0].to_dict(),
            'complete_rows': len(data.dropna()),
            'missing_patterns': self._analyze_missing_patterns(data)
        }
        
        return missing_summary
    
    def _analyze_missing_patterns(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„"""
        # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ë“¤ë§Œ ë¶„ì„
        missing_cols = data.columns[data.isnull().any()].tolist()
        
        if not missing_cols:
            return {'pattern_analysis': 'No missing values found'}
        
        # ê²°ì¸¡ì¹˜ íŒ¨í„´ ì¡°í•© ë¶„ì„
        missing_patterns = data[missing_cols].isnull().value_counts().head(10)
        
        return {
            'top_patterns': missing_patterns.to_dict(),
            'pattern_description': 'Most common combinations of missing values across columns'
        }
    
    def _analyze_outliers(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ì´ìƒì¹˜ ë¶„ì„"""
        numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        
        if not numerical_cols:
            return {'analysis': 'No numerical columns for outlier analysis'}
        
        outlier_summary = {}
        
        for col in numerical_cols:
            Q1 = data[col].quantile(0.25)
            Q3 = data[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]
            
            outlier_summary[col] = {
                'outlier_count': len(outliers),
                'outlier_percentage': round((len(outliers) / len(data)) * 100, 2),
                'lower_bound': lower_bound,
                'upper_bound': upper_bound,
                'extreme_values': {
                    'min': data[col].min(),
                    'max': data[col].max()
                }
            }
        
        return outlier_summary
    
    def _analyze_duplicates(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ì¤‘ë³µ ë°ì´í„° ë¶„ì„"""
        duplicate_rows = data.duplicated()
        
        return {
            'total_duplicates': duplicate_rows.sum(),
            'duplicate_percentage': round((duplicate_rows.sum() / len(data)) * 100, 2),
            'unique_rows': len(data) - duplicate_rows.sum(),
            'duplicate_subset_analysis': self._analyze_partial_duplicates(data)
        }
    
    def _analyze_partial_duplicates(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ë¶€ë¶„ ì¤‘ë³µ ë¶„ì„"""
        # ì£¼ìš” ì»¬ëŸ¼ë“¤ì— ëŒ€í•œ ë¶€ë¶„ ì¤‘ë³µ ê²€ì‚¬
        important_cols = data.columns[:5].tolist()  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ ë¶„ì„
        
        partial_duplicates = {}
        for col in important_cols:
            duplicate_values = data[col].duplicated()
            partial_duplicates[col] = {
                'duplicate_count': duplicate_values.sum(),
                'unique_values': data[col].nunique(),
                'most_common': data[col].value_counts().head(3).to_dict()
            }
        
        return partial_duplicates
    
    def _identify_data_issues(self, data: pd.DataFrame, missing_analysis: Dict, 
                            outlier_analysis: Dict, duplicate_analysis: Dict) -> List[str]:
        """ì „ë°˜ì ì¸ ë°ì´í„° ì´ìŠˆ ì‹ë³„"""
        issues = []
        
        # ê²°ì¸¡ì¹˜ ê´€ë ¨ ì´ìŠˆ
        if missing_analysis['total_missing'] > 0:
            high_missing_cols = [
                col for col, pct in missing_analysis['missing_percentages'].items() 
                if pct > 20
            ]
            if high_missing_cols:
                issues.append(f"ë†’ì€ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ì»¬ëŸ¼: {', '.join(high_missing_cols)}")
        
        # ì´ìƒì¹˜ ê´€ë ¨ ì´ìŠˆ
        if isinstance(outlier_analysis, dict) and outlier_analysis.get('analysis') != 'No numerical columns for outlier analysis':
            high_outlier_cols = [
                col for col, info in outlier_analysis.items() 
                if info.get('outlier_percentage', 0) > 5
            ]
            if high_outlier_cols:
                issues.append(f"ì´ìƒì¹˜ê°€ ë§ì€ ì»¬ëŸ¼: {', '.join(high_outlier_cols)}")
        
        # ì¤‘ë³µ ë°ì´í„° ì´ìŠˆ
        if duplicate_analysis['duplicate_percentage'] > 5:
            issues.append(f"ì¤‘ë³µ ë°ì´í„° ë¹„ìœ¨ì´ ë†’ìŒ: {duplicate_analysis['duplicate_percentage']}%")
        
        # ë°ì´í„° í¬ê¸° ê´€ë ¨ ì´ìŠˆ
        if len(data) < 30:
            issues.append("í‘œë³¸ í¬ê¸°ê°€ ì‘ìŒ (í†µê³„ì  ê²€ì •ì— ì œí•œì´ ìˆì„ ìˆ˜ ìˆìŒ)")
        
        # ë³€ìˆ˜ ìˆ˜ ê´€ë ¨ ì´ìŠˆ
        if len(data.columns) > len(data):
            issues.append("ë³€ìˆ˜ ìˆ˜ê°€ ê´€ì¸¡ì¹˜ ìˆ˜ë³´ë‹¤ ë§ìŒ (ì°¨ì›ì˜ ì €ì£¼ ê°€ëŠ¥ì„±)")
        
        return issues
    
    def _analyze_variables(self, data: pd.DataFrame, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë³€ìˆ˜ ë¶„ì„"""
        numerical_vars = data.select_dtypes(include=[np.number]).columns.tolist()
        categorical_vars = data.select_dtypes(include=['object', 'category']).columns.tolist()
        
        # ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ë³€ìˆ˜ë“¤ê³¼ ë§¤ì¹­
        request_metadata = input_data.get('request_metadata', {})
        target_variables = request_metadata.get('target_variables', [])
        group_variables = request_metadata.get('group_variables', [])
        
        # ë³€ìˆ˜ ê°„ ê´€ê³„ ë¶„ì„
        relationships = self._analyze_variable_relationships(data, numerical_vars, categorical_vars)
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ê°„ë‹¨í•œ ë²„ì „)
        feature_importance = self._analyze_feature_importance(data, target_variables)
        
        return {
            'numerical_variables': numerical_vars,
            'categorical_variables': categorical_vars,
            'target_variables': target_variables,
            'group_variables': group_variables,
            'variable_relationships': relationships,
            'feature_importance': feature_importance
        }
    
    def _analyze_variable_relationships(self, data: pd.DataFrame, 
                                      numerical_vars: List[str], 
                                      categorical_vars: List[str]) -> Dict[str, Any]:
        """ë³€ìˆ˜ ê°„ ê´€ê³„ ë¶„ì„"""
        relationships = {}
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„
        if len(numerical_vars) > 1:
            corr_matrix = data[numerical_vars].corr()
            high_correlations = []
            
            for i in range(len(numerical_vars)):
                for j in range(i+1, len(numerical_vars)):
                    corr_value = corr_matrix.iloc[i, j]
                    if abs(corr_value) > 0.7:  # ë†’ì€ ìƒê´€ê´€ê³„
                        high_correlations.append({
                            'var1': numerical_vars[i],
                            'var2': numerical_vars[j],
                            'correlation': round(corr_value, 3)
                        })
            
            relationships['high_correlations'] = high_correlations
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ì™€ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ ê´€ê³„ (ê°„ë‹¨í•œ ë¶„ì„)
        cat_num_relationships = []
        for cat_var in categorical_vars[:3]:  # ì²˜ìŒ 3ê°œë§Œ ë¶„ì„
            for num_var in numerical_vars[:3]:  # ì²˜ìŒ 3ê°œë§Œ ë¶„ì„
                try:
                    grouped = data.groupby(cat_var)[num_var].agg(['mean', 'std', 'count'])
                    if len(grouped) > 1:  # ê·¸ë£¹ì´ ì—¬ëŸ¬ ê°œ ìˆëŠ” ê²½ìš°
                        cat_num_relationships.append({
                            'categorical_var': cat_var,
                            'numerical_var': num_var,
                            'group_stats': grouped.to_dict('index')
                        })
                except:
                    continue
        
        relationships['categorical_numerical'] = cat_num_relationships[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€
        
        return relationships
    
    def _analyze_feature_importance(self, data: pd.DataFrame, target_variables: List[str]) -> Dict[str, Any]:
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ê°„ë‹¨í•œ ë²„ì „)"""
        if not target_variables:
            return {'analysis': 'No target variables specified'}
        
        importance_analysis = {}
        
        for target_var in target_variables:
            if target_var in data.columns:
                # ê°„ë‹¨í•œ ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¤‘ìš”ë„
                numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()
                if target_var in numerical_cols and len(numerical_cols) > 1:
                    correlations = data[numerical_cols].corr()[target_var].abs().sort_values(ascending=False)
                    importance_analysis[target_var] = correlations.head(5).to_dict()
        
        return importance_analysis
    
    def _generate_analysis_recommendations(self, data: pd.DataFrame, input_data: Dict[str, Any],
                                         quality_assessment: Dict[str, Any],
                                         variable_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        preprocessing_needed = []
        suitable_analyses = []
        potential_challenges = []
        
        # ì „ì²˜ë¦¬ ì¶”ì²œ
        if quality_assessment['missing_values']['total_missing'] > 0:
            preprocessing_needed.append("ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì œê±° ë˜ëŠ” ëŒ€ì²´)")
        
        if quality_assessment['duplicates']['duplicate_percentage'] > 1:
            preprocessing_needed.append("ì¤‘ë³µ ë°ì´í„° ì œê±°")
        
        # ì´ìƒì¹˜ê°€ ë§ì€ ê²½ìš°
        outlier_analysis = quality_assessment['outliers']
        if isinstance(outlier_analysis, dict):
            high_outlier_cols = [
                col for col, info in outlier_analysis.items() 
                if isinstance(info, dict) and info.get('outlier_percentage', 0) > 10
            ]
            if high_outlier_cols:
                preprocessing_needed.append("ì´ìƒì¹˜ ì²˜ë¦¬ ê³ ë ¤")
        
        # ì í•©í•œ ë¶„ì„ ë°©ë²• ì¶”ì²œ
        request_metadata = input_data.get('request_metadata', {})
        analysis_type = request_metadata.get('analysis_type', 'unknown')
        
        num_vars = len(variable_analysis['numerical_variables'])
        cat_vars = len(variable_analysis['categorical_variables'])
        
        if analysis_type == 'group_comparison':
            if cat_vars > 0 and num_vars > 0:
                suitable_analyses.append("ê·¸ë£¹ ê°„ í‰ê·  ë¹„êµ (t-ê²€ì •, ANOVA)")
            if cat_vars > 1:
                suitable_analyses.append("ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ ì—°ê´€ì„± ë¶„ì„ (ì¹´ì´ì œê³± ê²€ì •)")
        
        elif analysis_type == 'relationship':
            if num_vars > 1:
                suitable_analyses.append("ìƒê´€ê´€ê³„ ë¶„ì„")
                suitable_analyses.append("íšŒê·€ë¶„ì„")
        
        elif analysis_type == 'categorical':
            if cat_vars > 1:
                suitable_analyses.append("ì¹´ì´ì œê³± ë…ë¦½ì„± ê²€ì •")
                suitable_analyses.append("Fisherì˜ ì •í™•ê²€ì •")
        
        # ì ì¬ì  ë„ì „ê³¼ì œ
        if len(data) < 30:
            potential_challenges.append("ì‘ì€ í‘œë³¸ í¬ê¸°ë¡œ ì¸í•œ ê²€ì •ë ¥ ì œí•œ")
        
        if quality_assessment['missing_values']['total_missing'] > len(data) * 0.1:
            potential_challenges.append("ë†’ì€ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ë¡œ ì¸í•œ í¸í–¥ ê°€ëŠ¥ì„±")
        
        if num_vars > 0:
            # ì •ê·œì„± ê°„ë‹¨ ì²´í¬
            numerical_data = data.select_dtypes(include=[np.number])
            for col in numerical_data.columns:
                if len(numerical_data[col].dropna()) > 0:
                    # ê°„ë‹¨í•œ ì •ê·œì„± ì²´í¬ (ì™œë„, ì²¨ë„)
                    skewness = numerical_data[col].skew()
                    if abs(skewness) > 2:
                        potential_challenges.append(f"{col} ë³€ìˆ˜ì˜ ë¹„ì •ê·œì„± (ì™œë„: {round(skewness, 2)})")
                        break
        
        return {
            'preprocessing_needed': preprocessing_needed,
            'suitable_analyses': suitable_analyses,
            'potential_challenges': potential_challenges
        }
    
    def _generate_summary_insights(self, data_overview: Dict, descriptive_stats: Dict,
                                 quality_assessment: Dict, variable_analysis: Dict,
                                 recommendations: Dict) -> Dict[str, Any]:
        """ìš”ì•½ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        key_findings = []
        data_characteristics = []
        
        # ì£¼ìš” ë°œê²¬ì‚¬í•­
        total_rows = data_overview['basic_info']['total_rows']
        total_cols = data_overview['basic_info']['total_columns']
        key_findings.append(f"ë°ì´í„°ì…‹ í¬ê¸°: {total_rows:,}í–‰ Ã— {total_cols}ì—´")
        
        num_vars = len(variable_analysis['numerical_variables'])
        cat_vars = len(variable_analysis['categorical_variables'])
        key_findings.append(f"ë³€ìˆ˜ êµ¬ì„±: ìˆ˜ì¹˜í˜• {num_vars}ê°œ, ë²”ì£¼í˜• {cat_vars}ê°œ")
        
        missing_total = quality_assessment['missing_values']['total_missing']
        if missing_total > 0:
            missing_pct = round((missing_total / (total_rows * total_cols)) * 100, 1)
            key_findings.append(f"ê²°ì¸¡ì¹˜: ì „ì²´ ë°ì´í„°ì˜ {missing_pct}%")
        
        # ë°ì´í„° íŠ¹ì„±
        if total_rows >= 1000:
            data_characteristics.append("ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹")
        elif total_rows < 100:
            data_characteristics.append("ì†Œê·œëª¨ ë°ì´í„°ì…‹")
        else:
            data_characteristics.append("ì¤‘ê°„ ê·œëª¨ ë°ì´í„°ì…‹")
        
        if quality_assessment['duplicates']['duplicate_percentage'] < 1:
            data_characteristics.append("ì¤‘ë³µ ë°ì´í„° ê±°ì˜ ì—†ìŒ")
        
        if missing_total == 0:
            data_characteristics.append("ê²°ì¸¡ì¹˜ ì—†ëŠ” ì™„ì „í•œ ë°ì´í„°")
        
        # ë¶„ì„ ì¤€ë¹„ë„ í‰ê°€
        readiness_score = 0
        
        # ê¸ì •ì  ìš”ì†Œ
        if missing_total == 0:
            readiness_score += 30
        elif missing_total < total_rows * total_cols * 0.05:
            readiness_score += 20
        
        if quality_assessment['duplicates']['duplicate_percentage'] < 5:
            readiness_score += 20
        
        if total_rows >= 30:
            readiness_score += 20
        
        if len(recommendations['preprocessing_needed']) <= 2:
            readiness_score += 15
        
        if num_vars > 0 and cat_vars > 0:
            readiness_score += 15  # ë‹¤ì–‘í•œ ë³€ìˆ˜ íƒ€ì…
        
        # ë¶„ì„ ì¤€ë¹„ë„ ê²°ì •
        if readiness_score >= 80:
            analysis_readiness = "excellent"
        elif readiness_score >= 60:
            analysis_readiness = "good"
        elif readiness_score >= 40:
            analysis_readiness = "fair"
        else:
            analysis_readiness = "poor"
        
        return {
            'key_findings': key_findings,
            'data_characteristics': data_characteristics,
            'analysis_readiness': analysis_readiness,
            'readiness_score': readiness_score
        }
    
    def get_step_info(self) -> Dict[str, Any]:
        """ë‹¨ê³„ ì •ë³´ ë°˜í™˜ (ë¶€ëª¨ í´ë˜ìŠ¤ ë©”ì„œë“œ í™•ì¥)"""
        base_info = super().get_step_info()
        base_info.update({
            'description': 'ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ ë° ìš”ì•½',
            'input_requirements': ['selected_file', 'file_info', 'user_request', 'refined_objectives'],
            'output_provides': [
                'data_overview', 'descriptive_statistics', 'data_quality_assessment',
                'variable_analysis', 'analysis_recommendations', 'summary_insights'
            ],
            'capabilities': [
                'ê¸°ìˆ  í†µê³„ ê³„ì‚°', 'ë°ì´í„° í’ˆì§ˆ í‰ê°€', 'ë³€ìˆ˜ ê´€ê³„ ë¶„ì„',
                'ê²°ì¸¡ì¹˜/ì´ìƒì¹˜ íƒì§€', 'ë¶„ì„ ì¶”ì²œì‚¬í•­ ì œê³µ'
            ]
        })
        return base_info


# ë‹¨ê³„ ë“±ë¡
PipelineStepRegistry.register_step(3, DataSummaryStep) 