"""
User Request Pipeline

2ë‹¨ê³„: LLM Agent ê¸°ë°˜ ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ ë° ëª©í‘œ ì •ì˜
LLM Agentê°€ ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸ê³¼ ë°ì´í„°ë¥¼ í•¨ê»˜ ë¶„ì„í•˜ì—¬
ìœ ì—°í•˜ê³  ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„ ëª©í‘œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
"""

import logging
from typing import Dict, Any, Optional, List
import json
import pandas as pd

from .base_pipeline_step import BasePipelineStep, PipelineStepRegistry
from services.llm.llm_client import LLMClient
from services.llm.prompt_engine import PromptEngine
from core.agent.autonomous_agent import AutonomousAgent
from utils.ui_helpers import get_user_input
from utils.data_loader import DataLoader


class UserRequestStep(BasePipelineStep):
    """2ë‹¨ê³„: LLM Agent ê¸°ë°˜ ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ ë° ëª©í‘œ ì •ì˜"""
    
    def __init__(self):
        """UserRequestStep ì´ˆê¸°í™”"""
        super().__init__("LLM Agent ê¸°ë°˜ ì‚¬ìš©ì ìš”ì²­ ë¶„ì„", 2)
        self.llm_client = LLMClient()
        self.prompt_engine = PromptEngine()
        self.agent = AutonomousAgent(agent_id="request_analyst")
        self.data_loader = DataLoader()
        
    def validate_input(self, input_data: Dict[str, Any]) -> bool:
        """ì…ë ¥ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
        required_fields = ['selected_file', 'file_info']
        return all(field in input_data for field in required_fields)
    
    def get_expected_output_schema(self) -> Dict[str, Any]:
        """ì˜ˆìƒ ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ë°˜í™˜"""
        return {
            'user_request': str,
            'analysis_objectives': dict,
            'agent_analysis': dict,
            'data_understanding': dict,
            'analysis_plan': dict,
            'confidence_level': str
        }
    
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """LLM Agent ê¸°ë°˜ ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ ì‹¤í–‰"""
        self.logger.info("LLM Agent ê¸°ë°˜ ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ ì‹œì‘")
        
        try:
            # ì‚¬ìš©ì ìš”ì²­ ìˆ˜ì§‘
            user_request = self._get_user_request()
            if not user_request:
                return {
                    'error': True,
                    'error_message': 'ì‚¬ìš©ì ìš”ì²­ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
                }
            
            # ë°ì´í„° ë¡œë”© ë° ê¸°ë³¸ ì´í•´
            data_info = self._load_and_understand_data(input_data)
            if data_info.get('error'):
                return data_info
            
            # LLM Agentë¥¼ í†µí•œ í†µí•© ë¶„ì„
            agent_analysis = self._analyze_with_llm_agent(user_request, data_info, input_data)
            
            # ë¶„ì„ ê³„íš ìƒì„±
            analysis_plan = self._generate_analysis_plan(agent_analysis, data_info)
            
            self.logger.info("LLM Agent ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ")
            
            return {
                'success': True,
                'user_request': user_request,
                'analysis_objectives': agent_analysis.get('objectives', {}),
                'agent_analysis': agent_analysis,
                'data_understanding': data_info,
                'analysis_plan': analysis_plan,
                'confidence_level': agent_analysis.get('confidence', 'medium'),
                'step_info': self.get_step_info()
            }
            
        except Exception as e:
            self.logger.error(f"LLM Agent ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': f'ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
                'error_type': 'agent_analysis_error'
            }
    
    def _get_user_request(self) -> Optional[str]:
        """ì‚¬ìš©ì ìš”ì²­ ì…ë ¥ ë°›ê¸°"""
        print("\n" + "="*60)
        print("ğŸ“ ë¶„ì„í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ìì—°ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”")
        print("="*60)
        print("ì˜ˆì‹œ:")
        print("â€¢ ì„±ë³„ì— ë”°ë¥¸ ë§Œì¡±ë„ í‰ê·  ì°¨ì´ë¥¼ ë¶„ì„í•´ì¤˜")
        print("â€¢ ë‚˜ì´ì™€ ì†Œë“ì˜ ìƒê´€ê´€ê³„ë¥¼ ì•Œê³  ì‹¶ì–´")
        print("â€¢ êµìœ¡ìˆ˜ì¤€ë³„ë¡œ ì—°ë´‰ ì°¨ì´ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì¤˜")
        print("â€¢ ì§€ì—­ë³„ ë§¤ì¶œ ë¶„í¬ë¥¼ ë¹„êµ ë¶„ì„í•´ì¤˜")
        print("-"*60)
        
        user_request = get_user_input(
            "ë¶„ì„ ìš”ì²­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”: ",
            input_type="text"
        )
        
        if user_request and len(user_request.strip()) > 5:
            return user_request.strip()
        else:
            print("âŒ ë¶„ì„ ìš”ì²­ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ 5ê¸€ì ì´ìƒ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return None
    
    def _load_and_understand_data(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ë¡œë”© ë° ê¸°ë³¸ ì´í•´"""
        try:
            file_path = input_data['selected_file']
            
            # ë°ì´í„° ë¡œë”©
            data, metadata = self.data_loader.load_file(file_path)
            if data is None:
                return {
                    'error': True,
                    'error_message': f'ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {metadata.get("error", "Unknown error")}'
                }
            
            # ë°ì´í„° ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            data_info = {
                'file_path': file_path,
                'shape': data.shape,
                'columns': list(data.columns),
                'dtypes': {col: str(dtype) for col, dtype in data.dtypes.items()},
                'sample_data': data.head(3).to_dict('records'),
                'missing_info': {col: int(data[col].isnull().sum()) for col in data.columns},
                'numerical_columns': list(data.select_dtypes(include=['number']).columns),
                'categorical_columns': list(data.select_dtypes(include=['object', 'category']).columns),
                'data_object': data  # ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ë°ì´í„° ê°ì²´
            }
            
            return data_info
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ë¡œë”© ë° ì´í•´ ì˜¤ë¥˜: {e}")
            return {
                'error': True,
                'error_message': f'ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }
    
    def _analyze_with_llm_agent(self, user_request: str, data_info: Dict[str, Any], 
                               input_data: Dict[str, Any]) -> Dict[str, Any]:
        """LLM Agentë¥¼ í†µí•œ í†µí•© ë¶„ì„"""
        try:
            # ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            data_context = self._build_data_context(data_info)
            
            # LLM Agent ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
            analysis_prompt = self._create_agent_analysis_prompt(user_request, data_context)
            
            # LLM Agent ì‹¤í–‰
            response = self.llm_client.generate_response(
                analysis_prompt,
                max_tokens=1500,
                temperature=0.3
            )
            
            # ì‘ë‹µ íŒŒì‹±
            agent_analysis = self._parse_agent_response(response.content)
            
            # ì‘ë‹µ ê²€ì¦ ë° ë³´ì™„
            validated_analysis = self._validate_and_enhance_analysis(
                agent_analysis, user_request, data_info
            )
            
            return validated_analysis
            
        except Exception as e:
            self.logger.error(f"LLM Agent ë¶„ì„ ì˜¤ë¥˜: {e}")
            # ë°±ì—… ë¶„ì„ ì‹¤í–‰
            return self._fallback_analysis(user_request, data_info)
    
    def _build_data_context(self, data_info: Dict[str, Any]) -> str:
        """ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        
        # ê¸°ë³¸ ì •ë³´
        context_parts.append(f"ë°ì´í„° í¬ê¸°: {data_info['shape'][0]}í–‰ Ã— {data_info['shape'][1]}ì—´")
        
        # ì»¬ëŸ¼ ì •ë³´
        context_parts.append("ì»¬ëŸ¼ ì •ë³´:")
        for col in data_info['columns']:
            dtype = data_info['dtypes'][col]
            missing = data_info['missing_info'][col]
            missing_pct = round((missing / data_info['shape'][0]) * 100, 1)
            
            context_parts.append(f"  - {col} ({dtype}): ê²°ì¸¡ì¹˜ {missing}ê°œ ({missing_pct}%)")
        
        # ìƒ˜í”Œ ë°ì´í„°
        context_parts.append("\nìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3í–‰):")
        for i, row in enumerate(data_info['sample_data'], 1):
            row_str = ", ".join([f"{k}={v}" for k, v in row.items()][:5])  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ
            context_parts.append(f"  {i}. {row_str}")
        
        # ë³€ìˆ˜ íƒ€ì… ìš”ì•½
        num_cols = len(data_info['numerical_columns'])
        cat_cols = len(data_info['categorical_columns'])
        context_parts.append(f"\në³€ìˆ˜ ìœ í˜•: ìˆ˜ì¹˜í˜• {num_cols}ê°œ, ë²”ì£¼í˜• {cat_cols}ê°œ")
        
        return "\n".join(context_parts)
    
    def _create_agent_analysis_prompt(self, user_request: str, data_context: str) -> str:
        """LLM Agent ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìì—°ì–´ ìš”ì²­ê³¼ ë°ì´í„° ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ í†µê³„ ë¶„ì„ ë°©ë²•ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

## ì‚¬ìš©ì ìš”ì²­
"{user_request}"

## ë°ì´í„° ì •ë³´
{data_context}

## ë¶„ì„ ê³¼ì œ
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ê²°ì •í•´ì£¼ì„¸ìš”:

1. ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë¶„ì„ì˜ í•µì‹¬ ëª©ì 
2. ë¶„ì„ì— í•„ìš”í•œ ë³€ìˆ˜ë“¤ (ì˜ì–´ ì»¬ëŸ¼ëª…ê³¼ í•œê¸€ ìš”ì²­ ê°„ ë§¤ì¹­ í¬í•¨)
3. ì ì ˆí•œ í†µê³„ ë¶„ì„ ë°©ë²•
4. ë¶„ì„ ê³¼ì •ì—ì„œ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ë“¤

## ì‘ë‹µ í˜•ì‹ (JSON)
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ë‹µí•´ì£¼ì„¸ìš”:

```json
{{
    "objectives": {{
        "main_goal": "ë¶„ì„ì˜ ì£¼ìš” ëª©ì ",
        "specific_questions": ["êµ¬ì²´ì ì¸ ë¶„ì„ ì§ˆë¬¸ë“¤"],
        "analysis_type": "group_comparison|correlation|regression|descriptive|categorical"
    }},
    "variables": {{
        "target_variables": ["ì¢…ì†ë³€ìˆ˜/ë¶„ì„ëŒ€ìƒ ì»¬ëŸ¼ëª…ë“¤"],
        "predictor_variables": ["ë…ë¦½ë³€ìˆ˜/ê·¸ë£¹ë³€ìˆ˜ ì»¬ëŸ¼ëª…ë“¤"],
        "variable_matching": {{
            "ì‚¬ìš©ìì–¸ê¸‰ë‹¨ì–´": "ì‹¤ì œì»¬ëŸ¼ëª…"
        }}
    }},
    "analysis_methods": {{
        "primary_method": "ì£¼ìš” ë¶„ì„ ë°©ë²•",
        "alternative_methods": ["ëŒ€ì•ˆ ë¶„ì„ ë°©ë²•ë“¤"],
        "preprocessing_needed": ["í•„ìš”í•œ ì „ì²˜ë¦¬ ë‹¨ê³„ë“¤"]
    }},
    "considerations": {{
        "data_quality_issues": ["ë°ì´í„° í’ˆì§ˆ ì´ìŠˆë“¤"],
        "statistical_assumptions": ["í™•ì¸í•´ì•¼ í•  í†µê³„ì  ê°€ì •ë“¤"],
        "potential_challenges": ["ì˜ˆìƒë˜ëŠ” ë¶„ì„ ì–´ë ¤ì›€ë“¤"]
    }},
    "confidence": "high|medium|low",
    "reasoning": "ë¶„ì„ íŒë‹¨ì˜ ê·¼ê±°"
}}
```

ì¤‘ìš”: ì‚¬ìš©ìê°€ í•œê¸€ë¡œ ì–¸ê¸‰í•œ ê°œë…ë“¤ì„ ë°ì´í„°ì˜ ì‹¤ì œ ì˜ì–´ ì»¬ëŸ¼ëª…ê³¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë§¤ì¹­í•˜ì„¸ìš”.
ì˜ˆ: "ì„±ë³„" â†’ "gender", "ë§Œì¡±ë„" â†’ "satisfaction", "ë‚˜ì´" â†’ "age"
"""
        
        return prompt
    
    def _parse_agent_response(self, response_content: str) -> Dict[str, Any]:
        """LLM Agent ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ë¸”ë¡ ì¶”ì¶œ
            json_start = response_content.find('```json')
            json_end = response_content.find('```', json_start + 7)
            
            if json_start != -1 and json_end != -1:
                json_str = response_content[json_start + 7:json_end].strip()
            else:
                # JSON ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ ì‘ë‹µì—ì„œ JSON ì°¾ê¸°
                json_str = response_content.strip()
            
            # JSON íŒŒì‹±
            parsed_response = json.loads(json_str)
            return parsed_response
            
        except json.JSONDecodeError as e:
            self.logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return {
                "objectives": {
                    "main_goal": "ë°ì´í„° ë¶„ì„",
                    "analysis_type": "descriptive"
                },
                "variables": {
                    "target_variables": [],
                    "predictor_variables": []
                },
                "confidence": "low",
                "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ë¶„ì„ ì ìš©"
            }
    
    def _validate_and_enhance_analysis(self, analysis: Dict[str, Any], 
                                     user_request: str, data_info: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ ê²€ì¦ ë° ë³´ì™„"""
        validated = analysis.copy()
        
        # ê¸°ë³¸ êµ¬ì¡° í™•ì¸
        if 'objectives' not in validated:
            validated['objectives'] = {}
        if 'variables' not in validated:
            validated['variables'] = {}
        if 'analysis_methods' not in validated:
            validated['analysis_methods'] = {}
        
        # ë³€ìˆ˜ëª… ê²€ì¦ ë° ë§¤ì¹­
        available_columns = data_info['columns']
        
        # target_variables ê²€ì¦
        target_vars = validated['variables'].get('target_variables', [])
        validated_targets = [var for var in target_vars if var in available_columns]
        
        # predictor_variables ê²€ì¦
        predictor_vars = validated['variables'].get('predictor_variables', [])
        validated_predictors = [var for var in predictor_vars if var in available_columns]
        
        # ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ì¶”ë¡  ì‹œë„
        if not validated_targets and not validated_predictors:
            inferred_vars = self._infer_variables_from_request(user_request, available_columns)
            validated_targets.extend(inferred_vars.get('targets', []))
            validated_predictors.extend(inferred_vars.get('predictors', []))
        
        validated['variables']['target_variables'] = validated_targets
        validated['variables']['predictor_variables'] = validated_predictors
        
        # ì‹ ë¢°ë„ ì¡°ì •
        if not validated_targets and not validated_predictors:
            validated['confidence'] = 'low'
        elif validated.get('confidence') not in ['high', 'medium', 'low']:
            validated['confidence'] = 'medium'
        
        return validated
    
    def _infer_variables_from_request(self, user_request: str, 
                                    available_columns: List[str]) -> Dict[str, List[str]]:
        """ìš”ì²­ì—ì„œ ë³€ìˆ˜ ì¶”ë¡ """
        request_lower = user_request.lower()
        
        # í•œê¸€-ì˜ì–´ ë§¤ì¹­ ì‚¬ì „
        common_mappings = {
            'ì„±ë³„': ['gender', 'sex'],
            'ë‚˜ì´': ['age'],
            'ë§Œì¡±ë„': ['satisfaction', 'rating', 'score'],
            'ì†Œë“': ['income', 'salary', 'wage'],
            'ì—°ë´‰': ['salary', 'income', 'wage'],
            'êµìœ¡': ['education', 'degree'],
            'ì§€ì—­': ['region', 'area', 'location'],
            'ë§¤ì¶œ': ['sales', 'revenue'],
            'ê°€ê²©': ['price', 'cost'],
            'ìˆ˜ëŸ‰': ['quantity', 'amount']
        }
        
        targets = []
        predictors = []
        
        for korean_term, english_terms in common_mappings.items():
            if korean_term in request_lower:
                for eng_term in english_terms:
                    matching_cols = [col for col in available_columns 
                                   if eng_term.lower() in col.lower()]
                    if matching_cols:
                        # ë¹„êµ/ì°¨ì´ ë¶„ì„ì˜ ê²½ìš°
                        if any(word in request_lower for word in ['ì°¨ì´', 'ë¹„êµ', 'ë”°ë¥¸']):
                            if korean_term in ['ì„±ë³„', 'ì§€ì—­', 'êµìœ¡']:
                                predictors.extend(matching_cols)
                            else:
                                targets.extend(matching_cols)
                        else:
                            targets.extend(matching_cols)
        
        return {'targets': list(set(targets)), 'predictors': list(set(predictors))}
    
    def _fallback_analysis(self, user_request: str, data_info: Dict[str, Any]) -> Dict[str, Any]:
        """ë°±ì—… ë¶„ì„ (LLM ì‹¤íŒ¨ ì‹œ)"""
        self.logger.info("ë°±ì—… ë¶„ì„ ì‹¤í–‰")
        
        # ê¸°ë³¸ êµ¬ì¡° ìƒì„±
        analysis = {
            "objectives": {
                "main_goal": "ë°ì´í„° íƒìƒ‰ì  ë¶„ì„",
                "specific_questions": ["ë°ì´í„°ì˜ ê¸°ë³¸ íŠ¹ì„± íŒŒì•…"],
                "analysis_type": "descriptive"
            },
            "variables": {
                "target_variables": data_info['numerical_columns'][:2],  # ì²˜ìŒ 2ê°œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜
                "predictor_variables": data_info['categorical_columns'][:2],  # ì²˜ìŒ 2ê°œ ë²”ì£¼í˜• ë³€ìˆ˜
            },
            "analysis_methods": {
                "primary_method": "ê¸°ìˆ í†µê³„ë¶„ì„",
                "alternative_methods": ["ë°ì´í„° ì‹œê°í™”"],
                "preprocessing_needed": ["ê²°ì¸¡ì¹˜ í™•ì¸"]
            },
            "confidence": "low",
            "reasoning": "LLM ë¶„ì„ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ íƒìƒ‰ì  ë¶„ì„ ì ìš©"
        }
        
        return analysis
    
    def _generate_analysis_plan(self, agent_analysis: Dict[str, Any], 
                              data_info: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ê³„íš ìƒì„±"""
        plan = {
            'analysis_steps': [],
            'expected_outputs': [],
            'estimated_duration': 'medium',
            'complexity_level': agent_analysis.get('confidence', 'medium')
        }
        
        # ë¶„ì„ ìœ í˜•ì— ë”°ë¥¸ ê³„íš ìƒì„±
        analysis_type = agent_analysis.get('objectives', {}).get('analysis_type', 'descriptive')
        
        if analysis_type == 'group_comparison':
            plan['analysis_steps'] = [
                'ê¸°ìˆ í†µê³„ ê³„ì‚°',
                'ì •ê·œì„± ê²€ì •',
                'ê·¸ë£¹ê°„ ë¹„êµ ê²€ì • (t-test/ANOVA)',
                'ê²°ê³¼ í•´ì„ ë° ì‹œê°í™”'
            ]
        elif analysis_type == 'correlation':
            plan['analysis_steps'] = [
                'ê¸°ìˆ í†µê³„ ê³„ì‚°',
                'ìƒê´€ê´€ê³„ ë¶„ì„',
                'ì‚°ì ë„ ì‹œê°í™”',
                'ê²°ê³¼ í•´ì„'
            ]
        elif analysis_type == 'regression':
            plan['analysis_steps'] = [
                'ê¸°ìˆ í†µê³„ ê³„ì‚°',
                'íšŒê·€ë¶„ì„ ê°€ì • ê²€í† ',
                'íšŒê·€ëª¨ë¸ ìˆ˜í–‰',
                'ëª¨ë¸ í‰ê°€ ë° í•´ì„'
            ]
        else:  # descriptive
            plan['analysis_steps'] = [
                'ê¸°ìˆ í†µê³„ ê³„ì‚°',
                'ë°ì´í„° ë¶„í¬ í™•ì¸',
                'ì‹œê°í™” ìƒì„±',
                'ê¸°ë³¸ íŒ¨í„´ ë¶„ì„'
            ]
        
        plan['expected_outputs'] = [
            'í†µê³„ ê²€ì • ê²°ê³¼',
            'ì‹œê°í™” ì°¨íŠ¸',
            'ë¶„ì„ í•´ì„ ë³´ê³ ì„œ'
        ]
        
        return plan
    
    def get_step_info(self) -> Dict[str, Any]:
        """ë‹¨ê³„ ì •ë³´ ë°˜í™˜"""
        base_info = super().get_step_info()
        base_info.update({
            'description': 'LLM Agent ê¸°ë°˜ ì‚¬ìš©ì ìš”ì²­ ë¶„ì„',
            'input_requirements': ['selected_file', 'file_info'],
            'output_provides': [
                'analysis_objectives', 'agent_analysis', 'data_understanding', 'analysis_plan'
            ],
            'capabilities': [
                'LLM ê¸°ë°˜ ìì—°ì–´ ì´í•´', 'ìë™ ë³€ìˆ˜ ë§¤ì¹­', 'ë¶„ì„ ë°©ë²• ì¶”ì²œ', 'ë¶„ì„ ê³„íš ìˆ˜ë¦½'
            ]
        })
        return base_info


